source,title,description,content,language,docs_updated_at
https://python.langchain.com/docs/integrations/document_loaders/microsoft_powerpoint,Microsoft PowerPoint | ü¶úÔ∏èüîó Langchain,[Microsoft,"Microsoft PowerPointMicrosoft\nPowerPoint is a\npresentation program by Microsoft.This covers how to load Microsoft PowerPoint documents into a document\nformat that we can use downstream.from langchain_community.document_loaders import UnstructuredPowerPointLoaderloader = UnstructuredPowerPointLoader(""example_data/fake-power-point.pptx"")data = loader.load()data[Document(page_content='Adding a Bullet Slide\n\nFind the bullet slide layout\n\nUse _TextFrame.text for first bullet\n\nUse _TextFrame.add_paragraph() for subsequent bullets\n\nHere is a lot of text!\n\nHere is some text in a text box!', metadata={'source': 'example_data/fake-power-point.pptx'})]Retain Elements‚ÄãUnder the hood, Unstructured creates different ‚Äúelements‚Äù for\ndifferent chunks of text. By default we combine those together, but you\ncan easily keep that separation by specifying mode=""elements"".loader = UnstructuredPowerPointLoader(    ""example_data/fake-power-point.pptx"", mode=""elements"")data = loader.load()data[0]Document(page_content='Adding a Bullet Slide', lookup_str='', metadata={'source': 'example_data/fake-power-point.pptx'}, lookup_index=0)",en,
https://python.langchain.com/docs/integrations/document_loaders/aws_s3_file,AWS S3 File | ü¶úÔ∏èüîó Langchain,[Amazon Simple Storage Service (Amazon,"AWS S3 FileAmazon Simple Storage Service (Amazon\nS3)\nis an object storage service.AWS S3\nBucketsThis covers how to load document objects from an AWS S3 File object.from langchain_community.document_loaders import S3FileLoader%pip install --upgrade --quiet  boto3loader = S3FileLoader(""testing-hwc"", ""fake.docx"")loader.load()[Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': 's3://testing-hwc/fake.docx'}, lookup_index=0)]Configuring the AWS Boto3 client‚ÄãYou can configure the AWS\nBoto3\nclient by passing named arguments when creating the S3DirectoryLoader.\nThis is useful for instance when AWS credentials can‚Äôt be set as\nenvironment variables. See the list of\nparameters\nthat can be configured.loader = S3FileLoader(    ""testing-hwc"", ""fake.docx"", aws_access_key_id=""xxxx"", aws_secret_access_key=""yyyy"")loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/unstructured_file,Unstructured File | ü¶úÔ∏èüîó Langchain,This notebook covers how to use Unstructured package to load files of,"Unstructured FileThis notebook covers how to use Unstructured package to load files of\nmany types. Unstructured currently supports loading of text files,\npowerpoints, html, pdfs, images, and more.# # Install package%pip install --upgrade --quiet  ""unstructured[all-docs]""# # Install other dependencies# # https://github.com/Unstructured-IO/unstructured/blob/main/docs/source/installing.rst# !brew install libmagic# !brew install poppler# !brew install tesseract# # If parsing xml / html documents:# !brew install libxml2# !brew install libxslt# import nltk# nltk.download('punkt')from langchain_community.document_loaders import UnstructuredFileLoaderloader = UnstructuredFileLoader(""./example_data/state_of_the_union.txt"")docs = loader.load()docs[0].page_content[:400]'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\n\nLast year COVID-19 kept us apart. This year we are finally together again.\n\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans.\n\nWith a duty to one another to the American people to the Constit'Load list of files‚Äãfiles = [""./example_data/whatsapp_chat.txt"", ""./example_data/layout-parser-paper.pdf""]loader = UnstructuredFileLoader(files)docs = loader.load()docs[0].page_content[:400]Retain Elements‚ÄãUnder the hood, Unstructured creates different ‚Äúelements‚Äù for different\nchunks of text. By default we combine those together, but you can easily\nkeep that separation by specifying mode=""elements"".loader = UnstructuredFileLoader(    ""./example_data/state_of_the_union.txt"", mode=""elements"")docs = loader.load()docs[:5][Document(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.', lookup_str='', metadata={'source': '../../state_of_the_union.txt'}, lookup_index=0), Document(page_content='Last year COVID-19 kept us apart. This year we are finally together again.', lookup_str='', metadata={'source': '../../state_of_the_union.txt'}, lookup_index=0), Document(page_content='Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans.', lookup_str='', metadata={'source': '../../state_of_the_union.txt'}, lookup_index=0), Document(page_content='With a duty to one another to the American people to the Constitution.', lookup_str='', metadata={'source': '../../state_of_the_union.txt'}, lookup_index=0), Document(page_content='And with an unwavering resolve that freedom will always triumph over tyranny.', lookup_str='', metadata={'source': '../../state_of_the_union.txt'}, lookup_index=0)]Define a Partitioning Strategy‚ÄãUnstructured document loader allow users to pass in a strategy\nparameter that lets unstructured know how to partition the document.\nCurrently supported strategies are ""hi_res"" (the default) and\n""fast"". Hi res partitioning strategies are more accurate, but take\nlonger to process. Fast strategies partition the document more quickly,\nbut trade-off accuracy. Not all document types have separate hi res and\nfast partitioning strategies. For those document types, the strategy\nkwarg is ignored. In some cases, the high res strategy will fallback to\nfast if there is a dependency missing (i.e.¬†a model for document\npartitioning). You can see how to apply a strategy to an\nUnstructuredFileLoader below.from langchain_community.document_loaders import UnstructuredFileLoaderloader = UnstructuredFileLoader(    ""layout-parser-paper-fast.pdf"", strategy=""fast"", mode=""elements"")docs = loader.load()docs[:5][Document(page_content='1', lookup_str='', metadata={'source': 'layout-parser-paper-fast.pdf', 'filename': 'layout-parser-paper-fast.pdf', 'page_number': 1, 'category': 'UncategorizedText'}, lookup_index=0), Document(page_content='2', lookup_str='', metadata={'source': 'layout-parser-paper-fast.pdf', 'filename': 'layout-parser-paper-fast.pdf', 'page_number': 1, 'category': 'UncategorizedText'}, lookup_index=0), Document(page_content='0', lookup_str='', metadata={'source': 'layout-parser-paper-fast.pdf', 'filename': 'layout-parser-paper-fast.pdf', 'page_number': 1, 'category': 'UncategorizedText'}, lookup_index=0), Document(page_content='2', lookup_str='', metadata={'source': 'layout-parser-paper-fast.pdf', 'filename': 'layout-parser-paper-fast.pdf', 'page_number': 1, 'category': 'UncategorizedText'}, lookup_index=0), Document(page_content='n', lookup_str='', metadata={'source': 'layout-parser-paper-fast.pdf', 'filename': 'layout-parser-paper-fast.pdf', 'page_number': 1, 'category': 'Title'}, lookup_index=0)]PDF Example‚ÄãProcessing PDF documents works exactly the same way. Unstructured\ndetects the file type and extracts the same types of elements. Modes of\noperation are - single all the text from all elements are combined\ninto one (default) - elements maintain individual elements - paged\ntexts from each page are only combined!wget  https://raw.githubusercontent.com/Unstructured-IO/unstructured/main/example-docs/layout-parser-paper.pdf -P ""../../""loader = UnstructuredFileLoader(    ""./example_data/layout-parser-paper.pdf"", mode=""elements"")docs = loader.load()docs[:5][Document(page_content='LayoutParser : A UniÔ¨Åed Toolkit for Deep Learning Based Document Image Analysis', lookup_str='', metadata={'source': '../../layout-parser-paper.pdf'}, lookup_index=0), Document(page_content='Zejiang Shen 1 ( (ea)\n ), Ruochen Zhang 2 , Melissa Dell 3 , Benjamin Charles Germain Lee 4 , Jacob Carlson 3 , and Weining Li 5', lookup_str='', metadata={'source': '../../layout-parser-paper.pdf'}, lookup_index=0), Document(page_content='Allen Institute for AI shannons@allenai.org', lookup_str='', metadata={'source': '../../layout-parser-paper.pdf'}, lookup_index=0), Document(page_content='Brown University ruochen zhang@brown.edu', lookup_str='', metadata={'source': '../../layout-parser-paper.pdf'}, lookup_index=0), Document(page_content='Harvard University { melissadell,jacob carlson } @fas.harvard.edu', lookup_str='', metadata={'source': '../../layout-parser-paper.pdf'}, lookup_index=0)]If you need to post process the unstructured elements after\nextraction, you can pass in a list of str -> str functions to the\npost_processors kwarg when you instantiate the\nUnstructuredFileLoader. This applies to other Unstructured loaders as\nwell. Below is an example.from langchain_community.document_loaders import UnstructuredFileLoaderfrom unstructured.cleaners.core import clean_extra_whitespaceloader = UnstructuredFileLoader(    ""./example_data/layout-parser-paper.pdf"",    mode=""elements"",    post_processors=[clean_extra_whitespace],)docs = loader.load()docs[:5][Document(page_content='LayoutParser: A UniÔ¨Åed Toolkit for Deep Learning Based Document Image Analysis', metadata={'source': './example_data/layout-parser-paper.pdf', 'coordinates': {'points': ((157.62199999999999, 114.23496279999995), (157.62199999999999, 146.5141628), (457.7358962799999, 146.5141628), (457.7358962799999, 114.23496279999995)), 'system': 'PixelSpace', 'layout_width': 612, 'layout_height': 792}, 'filename': 'layout-parser-paper.pdf', 'file_directory': './example_data', 'filetype': 'application/pdf', 'page_number': 1, 'category': 'Title'}), Document(page_content='Zejiang Shen1 ((cid:0)), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain Lee4, Jacob Carlson3, and Weining Li5', metadata={'source': './example_data/layout-parser-paper.pdf', 'coordinates': {'points': ((134.809, 168.64029940800003), (134.809, 192.2517444), (480.5464199080001, 192.2517444), (480.5464199080001, 168.64029940800003)), 'system': 'PixelSpace', 'layout_width': 612, 'layout_height': 792}, 'filename': 'layout-parser-paper.pdf', 'file_directory': './example_data', 'filetype': 'application/pdf', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca', metadata={'source': './example_data/layout-parser-paper.pdf', 'coordinates': {'points': ((207.23000000000002, 202.57205439999996), (207.23000000000002, 311.8195408), (408.12676, 311.8195408), (408.12676, 202.57205439999996)), 'system': 'PixelSpace', 'layout_width': 612, 'layout_height': 792}, 'filename': 'layout-parser-paper.pdf', 'file_directory': './example_data', 'filetype': 'application/pdf', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='1 2 0 2', metadata={'source': './example_data/layout-parser-paper.pdf', 'coordinates': {'points': ((16.34, 213.36), (16.34, 253.36), (36.34, 253.36), (36.34, 213.36)), 'system': 'PixelSpace', 'layout_width': 612, 'layout_height': 792}, 'filename': 'layout-parser-paper.pdf', 'file_directory': './example_data', 'filetype': 'application/pdf', 'page_number': 1, 'category': 'UncategorizedText'}), Document(page_content='n u J', metadata={'source': './example_data/layout-parser-paper.pdf', 'coordinates': {'points': ((16.34, 258.36), (16.34, 286.14), (36.34, 286.14), (36.34, 258.36)), 'system': 'PixelSpace', 'layout_width': 612, 'layout_height': 792}, 'filename': 'layout-parser-paper.pdf', 'file_directory': './example_data', 'filetype': 'application/pdf', 'page_number': 1, 'category': 'Title'})]Unstructured API‚ÄãIf you want to get up and running with less set up, you can simply run\npip install unstructured and use UnstructuredAPIFileLoader or\nUnstructuredAPIFileIOLoader. That will process your document using the\nhosted Unstructured API. You can generate a free Unstructured API key\nhere. The Unstructured\ndocumentation page will have\ninstructions on how to generate an API key once they‚Äôre available. Check\nout the instructions\nhere\nif you‚Äôd like to self-host the Unstructured API or run it locally.from langchain_community.document_loaders import UnstructuredAPIFileLoaderfilenames = [""example_data/fake.docx"", ""example_data/fake-email.eml""]loader = UnstructuredAPIFileLoader(    file_path=filenames[0],    api_key=""FAKE_API_KEY"",)docs = loader.load()docs[0]Document(page_content='Lorem ipsum dolor sit amet.', metadata={'source': 'example_data/fake.docx'})You can also batch multiple files through the Unstructured API in a\nsingle API using UnstructuredAPIFileLoader.loader = UnstructuredAPIFileLoader(    file_path=filenames,    api_key=""FAKE_API_KEY"",)docs = loader.load()docs[0]Document(page_content='Lorem ipsum dolor sit amet.\n\nThis is a test email to use for unit tests.\n\nImportant points:\n\nRoses are red\n\nViolets are blue', metadata={'source': ['example_data/fake.docx', 'example_data/fake-email.eml']})",en,
https://python.langchain.com/docs/integrations/document_loaders/merge_doc,Merge Documents Loader | ü¶úÔ∏èüîó Langchain,Merge the documents returned from a set of specified data loaders.,"Merge Documents LoaderMerge the documents returned from a set of specified data loaders.from langchain_community.document_loaders import WebBaseLoaderloader_web = WebBaseLoader(    ""https://github.com/basecamp/handbook/blob/master/37signals-is-you.md"")from langchain_community.document_loaders import PyPDFLoaderloader_pdf = PyPDFLoader(""../MachineLearning-Lecture01.pdf"")from langchain_community.document_loaders.merge import MergedDataLoaderloader_all = MergedDataLoader(loaders=[loader_web, loader_pdf])docs_all = loader_all.load()len(docs_all)23",en,
https://python.langchain.com/docs/integrations/document_loaders/tencent_cos_file,Tencent COS File | ü¶úÔ∏èüîó Langchain,[Tencent Cloud Object Storage,"Tencent COS FileTencent Cloud Object Storage\n(COS) is a distributed\nstorage service that enables you to store any amount of data from\nanywhere via HTTP/HTTPS protocols. COS has no restrictions on data\nstructure or format. It also has no bucket size limit and partition\nmanagement, making it suitable for virtually any use case, such as\ndata delivery, data processing, and data lakes. COS provides a\nweb-based console, multi-language SDKs and APIs, command line tool,\nand graphical tools. It works well with Amazon S3 APIs, allowing you\nto quickly access community tools and plugins.This covers how to load document object from a Tencent COS File.%pip install --upgrade --quiet  cos-python-sdk-v5from langchain_community.document_loaders import TencentCOSFileLoaderfrom qcloud_cos import CosConfigconf = CosConfig(    Region=""your cos region"",    SecretId=""your cos secret_id"",    SecretKey=""your cos secret_key"",)loader = TencentCOSFileLoader(conf=conf, bucket=""you_cos_bucket"", key=""fake.docx"")loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/xml,XML | ü¶úÔ∏èüîó Langchain,The UnstructuredXMLLoader is used to load XML files. The loader,"XMLThe UnstructuredXMLLoader is used to load XML files. The loader\nworks with .xml files. The page content will be the text extracted\nfrom the XML tags.from langchain_community.document_loaders import UnstructuredXMLLoaderloader = UnstructuredXMLLoader(    ""example_data/factbook.xml"",)docs = loader.load()docs[0]Document(page_content='United States\n\nWashington, DC\n\nJoe Biden\n\nBaseball\n\nCanada\n\nOttawa\n\nJustin Trudeau\n\nHockey\n\nFrance\n\nParis\n\nEmmanuel Macron\n\nSoccer\n\nTrinidad & Tobado\n\nPort of Spain\n\nKeith Rowley\n\nTrack & Field', metadata={'source': 'example_data/factbook.xml'})",en,
https://python.langchain.com/docs/integrations/document_loaders/hacker_news,Hacker News | ü¶úÔ∏èüîó Langchain,Hacker News (sometimes,"Hacker NewsHacker News (sometimes\nabbreviated as HN) is a social news website focusing on computer\nscience and entrepreneurship. It is run by the investment fund and\nstartup incubator Y Combinator. In general, content that can be\nsubmitted is defined as ‚Äúanything that gratifies one‚Äôs intellectual\ncuriosity.‚ÄùThis notebook covers how to pull page data and comments from Hacker\nNewsfrom langchain_community.document_loaders import HNLoaderloader = HNLoader(""https://news.ycombinator.com/item?id=34817881"")data = loader.load()data[0].page_content[:300]""delta_p_delta_x 73 days ago  \n             | next [‚Äì] \n\nAstrophysical and cosmological simulations are often insightful. They're also very cross-disciplinary; besides the obvious astrophysics, there's networking and sysadmin, parallel computing and algorithm theory (so that the simulation programs a""data[0].metadata{'source': 'https://news.ycombinator.com/item?id=34817881', 'title': 'What Lights the Universe‚Äôs Standard Candles?'}",en,
https://python.langchain.com/docs/integrations/document_loaders/azlyrics,AZLyrics | ü¶úÔ∏èüîó Langchain,"AZLyrics is a large, legal, every day","AZLyricsAZLyrics is a large, legal, every day\ngrowing collection of lyrics.This covers how to load AZLyrics webpages into a document format that we\ncan use downstream.from langchain_community.document_loaders import AZLyricsLoaderloader = AZLyricsLoader(""https://www.azlyrics.com/lyrics/mileycyrus/flowers.html"")data = loader.load()data[Document(page_content=""Miley Cyrus - Flowers Lyrics | AZLyrics.com\n\r\nWe were good, we were gold\nKinda dream that can't be sold\nWe were right till we weren't\nBuilt a home and watched it burn\n\nI didn't wanna leave you\nI didn't wanna lie\nStarted to cry but then remembered I\n\nI can buy myself flowers\nWrite my name in the sand\nTalk to myself for hours\nSay things you don't understand\nI can take myself dancing\nAnd I can hold my own hand\nYeah, I can love me better than you can\n\nCan love me better\nI can love me better, baby\nCan love me better\nI can love me better, baby\n\nPaint my nails, cherry red\nMatch the roses that you left\nNo remorse, no regret\nI forgive every word you said\n\nI didn't wanna leave you, baby\nI didn't wanna fight\nStarted to cry but then remembered I\n\nI can buy myself flowers\nWrite my name in the sand\nTalk to myself for hours, yeah\nSay things you don't understand\nI can take myself dancing\nAnd I can hold my own hand\nYeah, I can love me better than you can\n\nCan love me better\nI can love me better, baby\nCan love me better\nI can love me better, baby\nCan love me better\nI can love me better, baby\nCan love me better\nI\n\nI didn't wanna wanna leave you\nI didn't wanna fight\nStarted to cry but then remembered I\n\nI can buy myself flowers\nWrite my name in the sand\nTalk to myself for hours (Yeah)\nSay things you don't understand\nI can take myself dancing\nAnd I can hold my own hand\nYeah, I can love me better than\nYeah, I can love me better than you can, uh\n\nCan love me better\nI can love me better, baby\nCan love me better\nI can love me better, baby (Than you can)\nCan love me better\nI can love me better, baby\nCan love me better\nI\n"", lookup_str='', metadata={'source': 'https://www.azlyrics.com/lyrics/mileycyrus/flowers.html'}, lookup_index=0)]",en,
https://python.langchain.com/docs/integrations/document_loaders/twitter,Twitter | ü¶úÔ∏èüîó Langchain,Twitter is an online social media and social,"TwitterTwitter is an online social media and social\nnetworking service.This loader fetches the text from the Tweets of a list of Twitter\nusers, using the tweepy Python package. You must initialize the loader\nwith your Twitter API token, and you need to pass in the Twitter\nusername you want to extract.from langchain_community.document_loaders import TwitterTweetLoader%pip install --upgrade --quiet  tweepyloader = TwitterTweetLoader.from_bearer_token(    oauth2_bearer_token=""YOUR BEARER TOKEN"",    twitter_users=[""elonmusk""],    number_tweets=50,  # Default value is 100)# Or load from access token and consumer keys# loader = TwitterTweetLoader.from_secrets(#     access_token='YOUR ACCESS TOKEN',#     access_token_secret='YOUR ACCESS TOKEN SECRET',#     consumer_key='YOUR CONSUMER KEY',#     consumer_secret='YOUR CONSUMER SECRET',#     twitter_users=['elonmusk'],#     number_tweets=50,# )documents = loader.load()documents[:5][Document(page_content='@MrAndyNgo @REI One store after another shutting down', metadata={'created_at': 'Tue Apr 18 03:45:50 +0000 2023', 'user_info': {'id': 44196397, 'id_str': '44196397', 'name': 'Elon Musk', 'screen_name': 'elonmusk', 'location': 'A Shortfall of Gravitas', 'profile_location': None, 'description': 'nothing', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 135528327, 'friends_count': 220, 'listed_count': 120478, 'created_at': 'Tue Jun 02 20:12:29 +0000 2009', 'favourites_count': 21285, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 24795, 'lang': None, 'status': {'created_at': 'Tue Apr 18 03:45:50 +0000 2023', 'id': 1648170947541704705, 'id_str': '1648170947541704705', 'text': '@MrAndyNgo @REI One store after another shutting down', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'MrAndyNgo', 'name': 'Andy Ng√¥ üè≥Ô∏è\u200düåà', 'id': 2835451658, 'id_str': '2835451658', 'indices': [0, 10]}, {'screen_name': 'REI', 'name': 'REI', 'id': 16583846, 'id_str': '16583846', 'indices': [11, 15]}], 'urls': []}, 'source': '<a href=""http://twitter.com/download/iphone"" rel=""nofollow"">Twitter for iPhone</a>', 'in_reply_to_status_id': 1648134341678051328, 'in_reply_to_status_id_str': '1648134341678051328', 'in_reply_to_user_id': 2835451658, 'in_reply_to_user_id_str': '2835451658', 'in_reply_to_screen_name': 'MrAndyNgo', 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 118, 'favorite_count': 1286, 'favorited': False, 'retweeted': False, 'lang': 'en'}, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'C0DEED', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1590968738358079488/IY9Gx6Ok_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1590968738358079488/IY9Gx6Ok_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/44196397/1576183471', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None, 'translator_type': 'none', 'withheld_in_countries': []}}), Document(page_content='@KanekoaTheGreat @joshrogin @glennbeck Large ships are fundamentally vulnerable to ballistic (hypersonic) missiles', metadata={'created_at': 'Tue Apr 18 03:43:25 +0000 2023', 'user_info': {'id': 44196397, 'id_str': '44196397', 'name': 'Elon Musk', 'screen_name': 'elonmusk', 'location': 'A Shortfall of Gravitas', 'profile_location': None, 'description': 'nothing', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 135528327, 'friends_count': 220, 'listed_count': 120478, 'created_at': 'Tue Jun 02 20:12:29 +0000 2009', 'favourites_count': 21285, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 24795, 'lang': None, 'status': {'created_at': 'Tue Apr 18 03:45:50 +0000 2023', 'id': 1648170947541704705, 'id_str': '1648170947541704705', 'text': '@MrAndyNgo @REI One store after another shutting down', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'MrAndyNgo', 'name': 'Andy Ng√¥ üè≥Ô∏è\u200düåà', 'id': 2835451658, 'id_str': '2835451658', 'indices': [0, 10]}, {'screen_name': 'REI', 'name': 'REI', 'id': 16583846, 'id_str': '16583846', 'indices': [11, 15]}], 'urls': []}, 'source': '<a href=""http://twitter.com/download/iphone"" rel=""nofollow"">Twitter for iPhone</a>', 'in_reply_to_status_id': 1648134341678051328, 'in_reply_to_status_id_str': '1648134341678051328', 'in_reply_to_user_id': 2835451658, 'in_reply_to_user_id_str': '2835451658', 'in_reply_to_screen_name': 'MrAndyNgo', 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 118, 'favorite_count': 1286, 'favorited': False, 'retweeted': False, 'lang': 'en'}, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'C0DEED', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1590968738358079488/IY9Gx6Ok_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1590968738358079488/IY9Gx6Ok_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/44196397/1576183471', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None, 'translator_type': 'none', 'withheld_in_countries': []}}), Document(page_content='@KanekoaTheGreat The Golden Rule', metadata={'created_at': 'Tue Apr 18 03:37:17 +0000 2023', 'user_info': {'id': 44196397, 'id_str': '44196397', 'name': 'Elon Musk', 'screen_name': 'elonmusk', 'location': 'A Shortfall of Gravitas', 'profile_location': None, 'description': 'nothing', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 135528327, 'friends_count': 220, 'listed_count': 120478, 'created_at': 'Tue Jun 02 20:12:29 +0000 2009', 'favourites_count': 21285, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 24795, 'lang': None, 'status': {'created_at': 'Tue Apr 18 03:45:50 +0000 2023', 'id': 1648170947541704705, 'id_str': '1648170947541704705', 'text': '@MrAndyNgo @REI One store after another shutting down', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'MrAndyNgo', 'name': 'Andy Ng√¥ üè≥Ô∏è\u200düåà', 'id': 2835451658, 'id_str': '2835451658', 'indices': [0, 10]}, {'screen_name': 'REI', 'name': 'REI', 'id': 16583846, 'id_str': '16583846', 'indices': [11, 15]}], 'urls': []}, 'source': '<a href=""http://twitter.com/download/iphone"" rel=""nofollow"">Twitter for iPhone</a>', 'in_reply_to_status_id': 1648134341678051328, 'in_reply_to_status_id_str': '1648134341678051328', 'in_reply_to_user_id': 2835451658, 'in_reply_to_user_id_str': '2835451658', 'in_reply_to_screen_name': 'MrAndyNgo', 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 118, 'favorite_count': 1286, 'favorited': False, 'retweeted': False, 'lang': 'en'}, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'C0DEED', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1590968738358079488/IY9Gx6Ok_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1590968738358079488/IY9Gx6Ok_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/44196397/1576183471', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None, 'translator_type': 'none', 'withheld_in_countries': []}}), Document(page_content='@KanekoaTheGreat üßê', metadata={'created_at': 'Tue Apr 18 03:35:48 +0000 2023', 'user_info': {'id': 44196397, 'id_str': '44196397', 'name': 'Elon Musk', 'screen_name': 'elonmusk', 'location': 'A Shortfall of Gravitas', 'profile_location': None, 'description': 'nothing', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 135528327, 'friends_count': 220, 'listed_count': 120478, 'created_at': 'Tue Jun 02 20:12:29 +0000 2009', 'favourites_count': 21285, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 24795, 'lang': None, 'status': {'created_at': 'Tue Apr 18 03:45:50 +0000 2023', 'id': 1648170947541704705, 'id_str': '1648170947541704705', 'text': '@MrAndyNgo @REI One store after another shutting down', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'MrAndyNgo', 'name': 'Andy Ng√¥ üè≥Ô∏è\u200düåà', 'id': 2835451658, 'id_str': '2835451658', 'indices': [0, 10]}, {'screen_name': 'REI', 'name': 'REI', 'id': 16583846, 'id_str': '16583846', 'indices': [11, 15]}], 'urls': []}, 'source': '<a href=""http://twitter.com/download/iphone"" rel=""nofollow"">Twitter for iPhone</a>', 'in_reply_to_status_id': 1648134341678051328, 'in_reply_to_status_id_str': '1648134341678051328', 'in_reply_to_user_id': 2835451658, 'in_reply_to_user_id_str': '2835451658', 'in_reply_to_screen_name': 'MrAndyNgo', 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 118, 'favorite_count': 1286, 'favorited': False, 'retweeted': False, 'lang': 'en'}, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'C0DEED', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1590968738358079488/IY9Gx6Ok_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1590968738358079488/IY9Gx6Ok_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/44196397/1576183471', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None, 'translator_type': 'none', 'withheld_in_countries': []}}), Document(page_content='@TRHLofficial What‚Äôs he talking about and why is it sponsored by Erik‚Äôs son?', metadata={'created_at': 'Tue Apr 18 03:32:17 +0000 2023', 'user_info': {'id': 44196397, 'id_str': '44196397', 'name': 'Elon Musk', 'screen_name': 'elonmusk', 'location': 'A Shortfall of Gravitas', 'profile_location': None, 'description': 'nothing', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 135528327, 'friends_count': 220, 'listed_count': 120478, 'created_at': 'Tue Jun 02 20:12:29 +0000 2009', 'favourites_count': 21285, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 24795, 'lang': None, 'status': {'created_at': 'Tue Apr 18 03:45:50 +0000 2023', 'id': 1648170947541704705, 'id_str': '1648170947541704705', 'text': '@MrAndyNgo @REI One store after another shutting down', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'MrAndyNgo', 'name': 'Andy Ng√¥ üè≥Ô∏è\u200düåà', 'id': 2835451658, 'id_str': '2835451658', 'indices': [0, 10]}, {'screen_name': 'REI', 'name': 'REI', 'id': 16583846, 'id_str': '16583846', 'indices': [11, 15]}], 'urls': []}, 'source': '<a href=""http://twitter.com/download/iphone"" rel=""nofollow"">Twitter for iPhone</a>', 'in_reply_to_status_id': 1648134341678051328, 'in_reply_to_status_id_str': '1648134341678051328', 'in_reply_to_user_id': 2835451658, 'in_reply_to_user_id_str': '2835451658', 'in_reply_to_screen_name': 'MrAndyNgo', 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 118, 'favorite_count': 1286, 'favorited': False, 'retweeted': False, 'lang': 'en'}, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'C0DEED', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1590968738358079488/IY9Gx6Ok_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1590968738358079488/IY9Gx6Ok_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/44196397/1576183471', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None, 'translator_type': 'none', 'withheld_in_countries': []}})]",en,
https://python.langchain.com/docs/integrations/document_loaders/surrealdb,SurrealDB | ü¶úÔ∏èüîó Langchain,SurrealDB is an end-to-end cloud-native,"SurrealDBSurrealDB is an end-to-end cloud-native\ndatabase designed for modern applications, including web, mobile,\nserverless, Jamstack, backend, and traditional applications. With\nSurrealDB, you can simplify your database and API infrastructure,\nreduce development time, and build secure, performant apps quickly and\ncost-effectively.Key features of SurrealDB include:Reduces development time: SurrealDB simplifies your database\nand API stack by removing the need for most server-side\ncomponents, allowing you to build secure, performant apps faster\nand cheaper.Real-time collaborative API backend service: SurrealDB\nfunctions as both a database and an API backend service, enabling\nreal-time collaboration.Support for multiple querying languages: SurrealDB supports\nSQL querying from client devices, GraphQL, ACID transactions,\nWebSocket connections, structured and unstructured data, graph\nquerying, full-text indexing, and geospatial querying.Granular access control: SurrealDB provides row-level\npermissions-based access control, giving you the ability to manage\ndata access with precision.View the features, the latest\nreleases, and\ndocumentation.This notebook shows how to use functionality related to the\nSurrealDBLoader.Overview‚ÄãThe SurrealDB Document Loader returns a list of Langchain Documents from\na SurrealDB database.The Document Loader takes the following optional parameters:dburl: connection string to the websocket endpoint. default:\nws://localhost:8000/rpcns: name of the namespace. default: langchaindb: name of the database. default: databasetable: name of the table. default: documentsdb_user: SurrealDB credentials if needed: db username.db_pass: SurrealDB credentails if needed: db password.filter_criteria: dictionary to construct the WHERE clause for\nfiltering results from table.The output Document takes the following shape:Document(    page_content=<json encoded string containing the result document>,    metadata={        'id': <document id>,        'ns': <namespace name>,        'db': <database_name>,        'table': <table name>,        ... <additional fields from metadata property of the document>    })Setup‚ÄãUncomment the below cells to install surrealdb and langchain.# %pip install --upgrade --quiet  surrealdb langchain langchain-community# add this import for running in jupyter notebookimport nest_asyncionest_asyncio.apply()import jsonfrom langchain_community.document_loaders.surrealdb import SurrealDBLoaderloader = SurrealDBLoader(    dburl=""ws://localhost:8000/rpc"",    ns=""langchain"",    db=""database"",    table=""documents"",    db_user=""root"",    db_pass=""root"",    filter_criteria={},)docs = loader.load()len(docs)42doc = docs[-1]doc.metadata{'id': 'documents:zzz434sa584xl3b4ohvk', 'source': '../../modules/state_of_the_union.txt', 'ns': 'langchain', 'db': 'database', 'table': 'documents'}len(doc.page_content)18078page_content = json.loads(doc.page_content)page_content[""text""]'When we use taxpayer dollars to rebuild America ‚Äì we are going to Buy American: buy American products to support American jobs. \n\nThe federal government spends about $600 Billion a year to keep the country safe and secure. \n\nThere‚Äôs been a law on the books for almost a century \nto make sure taxpayers‚Äô dollars support American jobs and businesses. \n\nEvery Administration says they‚Äôll do it, but we are actually doing it. \n\nWe will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America. \n\nBut to compete for the best jobs of the future, we also need to level the playing field with China and other competitors. \n\nThat‚Äôs why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing. \n\nLet me give you one example of why it‚Äôs so important to pass it.'",en,
https://python.langchain.com/docs/integrations/document_loaders/spreedly,Spreedly | ü¶úÔ∏èüîó Langchain,Spreedly is a service that allows you to,"SpreedlySpreedly is a service that allows you to\nsecurely store credit cards and use them to transact against any\nnumber of payment gateways and third party APIs. It does this by\nsimultaneously providing a card tokenization/vault service as well as\na gateway and receiver integration service. Payment methods tokenized\nby Spreedly are stored at Spreedly, allowing you to independently\nstore a card and then pass that card to different end points based on\nyour business requirements.This notebook covers how to load data from the Spreedly REST\nAPI into a format that can\nbe ingested into LangChain, along with example usage for vectorization.Note: this notebook assumes the following packages are installed:\nopenai, chromadb, and tiktoken.import osfrom langchain.indexes import VectorstoreIndexCreatorfrom langchain_community.document_loaders import SpreedlyLoaderSpreedly API requires an access token, which can be found inside the\nSpreedly Admin Console.This document loader does not currently support pagination, nor access\nto more complex objects which require additional parameters. It also\nrequires a resource option which defines what objects you want to\nload.Following resources are available: - gateways_options:\nDocumentation -\ngateways:\nDocumentation -\nreceivers_options:\nDocumentation -\nreceivers:\nDocumentation -\npayment_methods:\nDocumentation -\ncertificates:\nDocumentation -\ntransactions:\nDocumentation -\nenvironments:\nDocumentationspreedly_loader = SpreedlyLoader(    os.environ[""SPREEDLY_ACCESS_TOKEN""], ""gateways_options"")# Create a vectorstore retriever from the loader# see https://python.langchain.com/en/latest/modules/data_connection/getting_started.html for more detailsindex = VectorstoreIndexCreator().from_loaders([spreedly_loader])spreedly_doc_retriever = index.vectorstore.as_retriever()Using embedded DuckDB without persistence: data will be transient# Test the retrieverspreedly_doc_retriever.get_relevant_documents(""CRC"")[Document(page_content='installment_grace_period_duration\nreference_data_code\ninvoice_number\ntax_management_indicator\noriginal_amount\ninvoice_amount\nvat_tax_rate\nmobile_remote_payment_type\ngratuity_amount\nmdd_field_1\nmdd_field_2\nmdd_field_3\nmdd_field_4\nmdd_field_5\nmdd_field_6\nmdd_field_7\nmdd_field_8\nmdd_field_9\nmdd_field_10\nmdd_field_11\nmdd_field_12\nmdd_field_13\nmdd_field_14\nmdd_field_15\nmdd_field_16\nmdd_field_17\nmdd_field_18\nmdd_field_19\nmdd_field_20\nsupported_countries: US\nAE\nBR\nCA\nCN\nDK\nFI\nFR\nDE\nIN\nJP\nMX\nNO\nSE\nGB\nSG\nLB\nPK\nsupported_cardtypes: visa\nmaster\namerican_express\ndiscover\ndiners_club\njcb\ndankort\nmaestro\nelo\nregions: asia_pacific\neurope\nlatin_america\nnorth_america\nhomepage: http://www.cybersource.com\ndisplay_api_url: https://ics2wsa.ic3.com/commerce/1.x/transactionProcessor\ncompany_name: CyberSource', metadata={'source': 'https://core.spreedly.com/v1/gateways_options.json'}), Document(page_content='BG\nBH\nBI\nBJ\nBM\nBN\nBO\nBR\nBS\nBT\nBW\nBY\nBZ\nCA\nCC\nCF\nCH\nCK\nCL\nCM\nCN\nCO\nCR\nCV\nCX\nCY\nCZ\nDE\nDJ\nDK\nDO\nDZ\nEC\nEE\nEG\nEH\nES\nET\nFI\nFJ\nFK\nFM\nFO\nFR\nGA\nGB\nGD\nGE\nGF\nGG\nGH\nGI\nGL\nGM\nGN\nGP\nGQ\nGR\nGT\nGU\nGW\nGY\nHK\nHM\nHN\nHR\nHT\nHU\nID\nIE\nIL\nIM\nIN\nIO\nIS\nIT\nJE\nJM\nJO\nJP\nKE\nKG\nKH\nKI\nKM\nKN\nKR\nKW\nKY\nKZ\nLA\nLC\nLI\nLK\nLS\nLT\nLU\nLV\nMA\nMC\nMD\nME\nMG\nMH\nMK\nML\nMN\nMO\nMP\nMQ\nMR\nMS\nMT\nMU\nMV\nMW\nMX\nMY\nMZ\nNA\nNC\nNE\nNF\nNG\nNI\nNL\nNO\nNP\nNR\nNU\nNZ\nOM\nPA\nPE\nPF\nPH\nPK\nPL\nPN\nPR\nPT\nPW\nPY\nQA\nRE\nRO\nRS\nRU\nRW\nSA\nSB\nSC\nSE\nSG\nSI\nSK\nSL\nSM\nSN\nST\nSV\nSZ\nTC\nTD\nTF\nTG\nTH\nTJ\nTK\nTM\nTO\nTR\nTT\nTV\nTW\nTZ\nUA\nUG\nUS\nUY\nUZ\nVA\nVC\nVE\nVI\nVN\nVU\nWF\nWS\nYE\nYT\nZA\nZM\nsupported_cardtypes: visa\nmaster\namerican_express\ndiscover\njcb\nmaestro\nelo\nnaranja\ncabal\nunionpay\nregions: asia_pacific\neurope\nmiddle_east\nnorth_america\nhomepage: http://worldpay.com\ndisplay_api_url: https://secure.worldpay.com/jsp/merchant/xml/paymentService.jsp\ncompany_name: WorldPay', metadata={'source': 'https://core.spreedly.com/v1/gateways_options.json'}), Document(page_content='gateway_specific_fields: receipt_email\nradar_session_id\nskip_radar_rules\napplication_fee\nstripe_account\nmetadata\nidempotency_key\nreason\nrefund_application_fee\nrefund_fee_amount\nreverse_transfer\naccount_id\ncustomer_id\nvalidate\nmake_default\ncancellation_reason\ncapture_method\nconfirm\nconfirmation_method\ncustomer\ndescription\nmoto\noff_session\non_behalf_of\npayment_method_types\nreturn_email\nreturn_url\nsave_payment_method\nsetup_future_usage\nstatement_descriptor\nstatement_descriptor_suffix\ntransfer_amount\ntransfer_destination\ntransfer_group\napplication_fee_amount\nrequest_three_d_secure\nerror_on_requires_action\nnetwork_transaction_id\nclaim_without_transaction_id\nfulfillment_date\nevent_type\nmodal_challenge\nidempotent_request\nmerchant_reference\ncustomer_reference\nshipping_address_zip\nshipping_from_zip\nshipping_amount\nline_items\nsupported_countries: AE\nAT\nAU\nBE\nBG\nBR\nCA\nCH\nCY\nCZ\nDE\nDK\nEE\nES\nFI\nFR\nGB\nGR\nHK\nHU\nIE\nIN\nIT\nJP\nLT\nLU\nLV\nMT\nMX\nMY\nNL\nNO\nNZ\nPL\nPT\nRO\nSE\nSG\nSI\nSK\nUS\nsupported_cardtypes: visa', metadata={'source': 'https://core.spreedly.com/v1/gateways_options.json'}), Document(page_content='mdd_field_57\nmdd_field_58\nmdd_field_59\nmdd_field_60\nmdd_field_61\nmdd_field_62\nmdd_field_63\nmdd_field_64\nmdd_field_65\nmdd_field_66\nmdd_field_67\nmdd_field_68\nmdd_field_69\nmdd_field_70\nmdd_field_71\nmdd_field_72\nmdd_field_73\nmdd_field_74\nmdd_field_75\nmdd_field_76\nmdd_field_77\nmdd_field_78\nmdd_field_79\nmdd_field_80\nmdd_field_81\nmdd_field_82\nmdd_field_83\nmdd_field_84\nmdd_field_85\nmdd_field_86\nmdd_field_87\nmdd_field_88\nmdd_field_89\nmdd_field_90\nmdd_field_91\nmdd_field_92\nmdd_field_93\nmdd_field_94\nmdd_field_95\nmdd_field_96\nmdd_field_97\nmdd_field_98\nmdd_field_99\nmdd_field_100\nsupported_countries: US\nAE\nBR\nCA\nCN\nDK\nFI\nFR\nDE\nIN\nJP\nMX\nNO\nSE\nGB\nSG\nLB\nPK\nsupported_cardtypes: visa\nmaster\namerican_express\ndiscover\ndiners_club\njcb\nmaestro\nelo\nunion_pay\ncartes_bancaires\nmada\nregions: asia_pacific\neurope\nlatin_america\nnorth_america\nhomepage: http://www.cybersource.com\ndisplay_api_url: https://api.cybersource.com\ncompany_name: CyberSource REST', metadata={'source': 'https://core.spreedly.com/v1/gateways_options.json'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/microsoft_sharepoint,Microsoft SharePoint | ü¶úÔ∏èüîó Langchain,Microsoft SharePoint is a,"Microsoft SharePointMicrosoft SharePoint is a\nwebsite-based collaboration system that uses workflow applications,\n‚Äúlist‚Äù databases, and other web parts and security features to empower\nbusiness teams to work together developed by Microsoft.This notebook covers how to load documents from the SharePoint Document\nLibrary.\nCurrently, only docx, doc, and pdf files are supported.Prerequisites‚ÄãRegister an application with the Microsoft identity\nplatform\ninstructions.When registration finishes, the Azure portal displays the app\nregistration‚Äôs Overview pane. You see the Application (client) ID.\nAlso called the client ID, this value uniquely identifies your\napplication in the Microsoft identity platform.During the steps you will be following at item 1, you can set\nthe redirect URI as\nhttps://login.microsoftonline.com/common/oauth2/nativeclientDuring the steps you will be following at item 1, generate a new\npassword (client_secret) under¬†Application Secrets¬†section.Follow the instructions at this\ndocument\nto add the following SCOPES (offline_access and\nSites.Read.All) to your application.To retrieve files from your Document Library, you will need its\nID. To obtain it, you will need values of Tenant Name,\nCollection ID, and Subsite ID.To find your Tenant Name follow the instructions at this\ndocument.\nOnce you got this, just remove .onmicrosoft.com from the value and\nhold the rest as your Tenant Name.To obtain your Collection ID and Subsite ID, you will need your\nSharePoint site-name. Your SharePoint site URL has the\nfollowing format\nhttps://<tenant-name>.sharepoint.com/sites/<site-name>. The last\npart of this URL is the site-name.To Get the Site Collection ID, hit this URL in the browser:\nhttps://<tenant>.sharepoint.com/sites/<site-name>/_api/site/id and\ncopy the value of the Edm.Guid property.To get the Subsite ID (or web ID) use:\nhttps://<tenant>.sharepoint.com/<site-name>/_api/web/id and copy\nthe value of the Edm.Guid property.The SharePoint site ID has the following format:\n<tenant-name>.sharepoint.com,<Collection ID>,<subsite ID>. You can\nhold that value to use in the next step.Visit the Graph Explorer\nPlayground\nto obtain your Document Library ID. The first step is to ensure\nyou are logged in with the account associated with your\nSharePoint site. Then you need to make a request to\nhttps://graph.microsoft.com/v1.0/sites/<SharePoint site ID>/drive\nand the response will return a payload with a field id that holds\nthe ID of your Document Library ID.üßë Instructions for ingesting your documents from SharePoint Document Library‚Äãüîë Authentication‚ÄãBy default, the SharePointLoader expects that the values of\nCLIENT_ID and CLIENT_SECRET must be stored as environment variables\nnamed O365_CLIENT_ID and O365_CLIENT_SECRET respectively. You could\npass those environment variables through a .env file at the root of\nyour application or using the following command in your script.os.environ['O365_CLIENT_ID'] = ""YOUR CLIENT ID""os.environ['O365_CLIENT_SECRET'] = ""YOUR CLIENT SECRET""This loader uses an authentication called on behalf of a\nuser.\nIt is a 2 step authentication with user consent. When you instantiate\nthe loader, it will call will print a url that the user must visit to\ngive consent to the app on the required permissions. The user must then\nvisit this url and give consent to the application. Then the user must\ncopy the resulting page url and paste it back on the console. The method\nwill then return True if the login attempt was succesful.from langchain_community.document_loaders.sharepoint import SharePointLoaderloader = SharePointLoader(document_library_id=""YOUR DOCUMENT LIBRARY ID"")Once the authentication has been done, the loader will store a token\n(o365_token.txt) at ~/.credentials/ folder. This token could be used\nlater to authenticate without the copy/paste steps explained earlier. To\nuse this token for authentication, you need to change the\nauth_with_token parameter to True in the instantiation of the loader.from langchain_community.document_loaders.sharepoint import SharePointLoaderloader = SharePointLoader(document_library_id=""YOUR DOCUMENT LIBRARY ID"", auth_with_token=True)üóÇÔ∏è Documents loader‚Äãüìë Loading documents from a Document Library Directory‚ÄãSharePointLoader can load documents from a specific folder within your\nDocument Library. For instance, you want to load all documents that are\nstored at Documents/marketing folder within your Document Library.from langchain_community.document_loaders.sharepoint import SharePointLoaderloader = SharePointLoader(document_library_id=""YOUR DOCUMENT LIBRARY ID"", folder_path=""Documents/marketing"", auth_with_token=True)documents = loader.load()üìë Loading documents from a list of Documents IDs‚ÄãAnother possibility is to provide a list of object_id for each\ndocument you want to load. For that, you will need to query the\nMicrosoft Graph\nAPI to find\nall the documents ID that you are interested in. This\nlink\nprovides a list of endpoints that will be helpful to retrieve the\ndocuments ID.For instance, to retrieve information about all objects that are stored\nat data/finance/ folder, you need make a request to:\nhttps://graph.microsoft.com/v1.0/drives/<document-library-id>/root:/data/finance:/children.\nOnce you have the list of IDs that you are interested in, then you can\ninstantiate the loader with the following parameters.from langchain_community.document_loaders.sharepoint import SharePointLoaderloader = SharePointLoader(document_library_id=""YOUR DOCUMENT LIBRARY ID"", object_ids=[""ID_1"", ""ID_2""], auth_with_token=True)documents = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/url,URL | ü¶úÔ∏èüîó Langchain,This covers how to load HTML documents from a list of URLs into a,"URLThis covers how to load HTML documents from a list of URLs into a\ndocument format that we can use downstream.from langchain_community.document_loaders import UnstructuredURLLoaderurls = [    ""https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023"",    ""https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023"",]Pass in ssl_verify=False with headers=headers to get past\nssl_verification error.loader = UnstructuredURLLoader(urls=urls)data = loader.load()Selenium URL LoaderThis covers how to load HTML documents from a list of URLs using the\nSeleniumURLLoader.Using selenium allows us to load pages that require JavaScript to\nrender.Setup‚ÄãTo use the SeleniumURLLoader, you will need to install selenium and\nunstructured.from langchain_community.document_loaders import SeleniumURLLoaderurls = [    ""https://www.youtube.com/watch?v=dQw4w9WgXcQ"",    ""https://goo.gl/maps/NDSHwePEyaHMFGwh8"",]loader = SeleniumURLLoader(urls=urls)data = loader.load()Playwright URL LoaderThis covers how to load HTML documents from a list of URLs using the\nPlaywrightURLLoader.As in the Selenium case, Playwright allows us to load pages that need\nJavaScript to render.Setup‚ÄãTo use the PlaywrightURLLoader, you will need to install playwright\nand unstructured. Additionally, you will need to install the\nPlaywright Chromium browser:# Install playwright%pip install --upgrade --quiet  ""playwright""%pip install --upgrade --quiet  ""unstructured""!playwright installfrom langchain_community.document_loaders import PlaywrightURLLoaderurls = [    ""https://www.youtube.com/watch?v=dQw4w9WgXcQ"",    ""https://goo.gl/maps/NDSHwePEyaHMFGwh8"",]loader = PlaywrightURLLoader(urls=urls, remove_selectors=[""header"", ""footer""])data = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/airbyte_cdk,Airbyte CDK (Deprecated) | ü¶úÔ∏èüîó Langchain,Note: AirbyteCDKLoader is deprecated. Please use,"Airbyte CDK (Deprecated)Note: AirbyteCDKLoader is deprecated. Please use\nAirbyteLoader instead.Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases.A lot of source connectors are implemented using the Airbyte\nCDK. This\nloader allows to run any of these connectors and return the data as\ndocuments.Installation‚ÄãFirst, you need to install the airbyte-cdk python package.%pip install --upgrade --quiet  airbyte-cdkThen, either install an existing connector from the Airbyte Github\nrepository\nor create your own connector using the Airbyte\nCDK.For example, to install the Github connector, run%pip install --upgrade --quiet  ""source_github@git+https://github.com/airbytehq/airbyte.git@master#subdirectory=airbyte-integrations/connectors/source-github""Some sources are also published as regular packages on PyPIExample‚ÄãNow you can create an AirbyteCDKLoader based on the imported source.\nIt takes a config object that‚Äôs passed to the connector. You also have\nto pick the stream you want to retrieve records from by name\n(stream_name). Check the connectors documentation page and spec\ndefinition for more information on the config object and available\nstreams. For the Github connectors these are:https://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-github/source_github/spec.json.https://docs.airbyte.com/integrations/sources/github/from langchain_community.document_loaders.airbyte import AirbyteCDKLoaderfrom source_github.source import SourceGithub  # plug in your own source hereconfig = {    # your github configuration    ""credentials"": {""api_url"": ""api.github.com"", ""personal_access_token"": ""<token>""},    ""repository"": ""<repo>"",    ""start_date"": ""<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>"",}issues_loader = AirbyteCDKLoader(    source_class=SourceGithub, config=config, stream_name=""issues"")Now you can load documents the usual waydocs = issues_loader.load()As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the\nlazy_load method which returns an iterator instead:docs_iterator = issues_loader.lazy_load()Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader:from langchain.docstore.document import Documentdef handle_record(record, id):    return Document(        page_content=record.data[""title""] + ""\n"" + (record.data[""body""] or """"),        metadata=record.data,    )issues_loader = AirbyteCDKLoader(    source_class=SourceGithub,    config=config,    stream_name=""issues"",    record_handler=handle_record,)docs = issues_loader.load()Incremental loads‚ÄãSome streams allow incremental loading, this means the source keeps\ntrack of synced records and won‚Äôt load them again. This is useful for\nsources that have a high volume of data and are updated frequently.To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded.last_state = issues_loader.last_state  # store safelyincremental_issue_loader = AirbyteCDKLoader(    source_class=SourceGithub, config=config, stream_name=""issues"", state=last_state)new_docs = incremental_issue_loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/larksuite,LarkSuite (FeiShu) | ü¶úÔ∏èüîó Langchain,LarkSuite is an enterprise collaboration,"LarkSuite (FeiShu)LarkSuite is an enterprise collaboration\nplatform developed by ByteDance.This notebook covers how to load data from the LarkSuite REST API into\na format that can be ingested into LangChain, along with example usage\nfor text summarization.The LarkSuite API requires an access token (tenant_access_token or\nuser_access_token), checkout LarkSuite open platform\ndocument for API details.from getpass import getpassfrom langchain_community.document_loaders.larksuite import LarkSuiteDocLoaderDOMAIN = input(""larksuite domain"")ACCESS_TOKEN = getpass(""larksuite tenant_access_token or user_access_token"")DOCUMENT_ID = input(""larksuite document id"")from pprint import pprintlarksuite_loader = LarkSuiteDocLoader(DOMAIN, ACCESS_TOKEN, DOCUMENT_ID)docs = larksuite_loader.load()pprint(docs)[Document(page_content='Test Doc\nThis is a Test Doc\n\n1\n2\n3\n\n', metadata={'document_id': 'V76kdbd2HoBbYJxdiNNccajunPf', 'revision_id': 11, 'title': 'Test Doc'})]# see https://python.langchain.com/docs/use_cases/summarization for more detailsfrom langchain.chains.summarize import load_summarize_chainfrom langchain_community.llms.fake import FakeListLLMllm = FakeListLLM()chain = load_summarize_chain(llm, chain_type=""map_reduce"")chain.run(docs)",en,
https://python.langchain.com/docs/integrations/document_loaders/cube_semantic,Cube Semantic Layer | ü¶úÔ∏èüîó Langchain,This notebook demonstrates the process of retrieving Cube‚Äôs data model,"Cube Semantic LayerThis notebook demonstrates the process of retrieving Cube‚Äôs data model\nmetadata in a format suitable for passing to LLMs as embeddings, thereby\nenhancing contextual information.About Cube‚ÄãCube is the Semantic Layer for building data apps.\nIt helps data engineers and application developers access data from\nmodern data stores, organize it into consistent definitions, and deliver\nit to every application.Cube‚Äôs data model provides structure and definitions that are used as a\ncontext for LLM to understand data and generate correct queries. LLM\ndoesn‚Äôt need to navigate complex joins and metrics calculations because\nCube abstracts those and provides a simple interface that operates on\nthe business-level terminology, instead of SQL table and column names.\nThis simplification helps LLM to be less error-prone and avoid\nhallucinations.Example‚ÄãInput arguments (mandatory)Cube Semantic Loader requires 2 arguments:cube_api_url: The URL of your Cube‚Äôs deployment REST API. Please\nrefer to the Cube\ndocumentation\nfor more information on configuring the base path.cube_api_token: The authentication token generated based on your\nCube‚Äôs API secret. Please refer to the Cube\ndocumentation\nfor instructions on generating JSON Web Tokens (JWT).Input arguments (optional)load_dimension_values: Whether to load dimension values for every\nstring dimension or not.dimension_values_limit: Maximum number of dimension values to\nload.dimension_values_max_retries: Maximum number of retries to load\ndimension values.dimension_values_retry_delay: Delay between retries to load\ndimension values.import jwtfrom langchain_community.document_loaders import CubeSemanticLoaderapi_url = ""https://api-example.gcp-us-central1.cubecloudapp.dev/cubejs-api/v1/meta""cubejs_api_secret = ""api-secret-here""security_context = {}# Read more about security context here: https://cube.dev/docs/securityapi_token = jwt.encode(security_context, cubejs_api_secret, algorithm=""HS256"")loader = CubeSemanticLoader(api_url, api_token)documents = loader.load()Returns a list of documents with the following attributes:page_contentmetadatatable_namecolumn_namecolumn_data_typecolumn_titlecolumn_descriptioncolumn_valuescube_data_obj_type# Given string containing page contentpage_content = ""Users View City, None""# Given dictionary containing metadatametadata = {    ""table_name"": ""users_view"",    ""column_name"": ""users_view.city"",    ""column_data_type"": ""string"",    ""column_title"": ""Users View City"",    ""column_description"": ""None"",    ""column_member_type"": ""dimension"",    ""column_values"": [        ""Austin"",        ""Chicago"",        ""Los Angeles"",        ""Mountain View"",        ""New York"",        ""Palo Alto"",        ""San Francisco"",        ""Seattle"",    ],    ""cube_data_obj_type"": ""view"",}",en,
https://python.langchain.com/docs/integrations/document_loaders/news,News URL | ü¶úÔ∏èüîó Langchain,This covers how to load HTML news articles from a list of URLs into a,"News URLThis covers how to load HTML news articles from a list of URLs into a\ndocument format that we can use downstream.from langchain_community.document_loaders import NewsURLLoaderurls = [    ""https://www.bbc.com/news/world-us-canada-66388172"",    ""https://www.bbc.com/news/entertainment-arts-66384971"",]Pass in urls to load them into Documentsloader = NewsURLLoader(urls=urls)data = loader.load()print(""First article: "", data[0])print(""\nSecond article: "", data[1])First article:  page_content='In testimony to the congressional committee examining the 6 January riot, Mrs Powell said she did not review all of the many claims of election fraud she made, telling them that ""no reasonable person"" would view her claims as fact. Neither she nor her representatives have commented.' metadata={'title': 'Donald Trump indictment: What do we know about the six co-conspirators?', 'link': 'https://www.bbc.com/news/world-us-canada-66388172', 'authors': [], 'language': 'en', 'description': 'Six people accused of helping Mr Trump undermine the election have been described by prosecutors.', 'publish_date': None}Second article:  page_content='Ms Williams added: ""If there\'s anything that I can do in my power to ensure that dancers or singers or whoever decides to work with her don\'t have to go through that same experience, I\'m going to do that.""' metadata={'title': ""Lizzo dancers Arianna Davis and Crystal Williams: 'No one speaks out, they are scared'"", 'link': 'https://www.bbc.com/news/entertainment-arts-66384971', 'authors': [], 'language': 'en', 'description': 'The US pop star is being sued for sexual harassment and fat-shaming but has yet to comment.', 'publish_date': None}Use nlp=True to run nlp analysis and generate keywords + summaryloader = NewsURLLoader(urls=urls, nlp=True)data = loader.load()print(""First article: "", data[0])print(""\nSecond article: "", data[1])First article:  page_content='In testimony to the congressional committee examining the 6 January riot, Mrs Powell said she did not review all of the many claims of election fraud she made, telling them that ""no reasonable person"" would view her claims as fact. Neither she nor her representatives have commented.' metadata={'title': 'Donald Trump indictment: What do we know about the six co-conspirators?', 'link': 'https://www.bbc.com/news/world-us-canada-66388172', 'authors': [], 'language': 'en', 'description': 'Six people accused of helping Mr Trump undermine the election have been described by prosecutors.', 'publish_date': None, 'keywords': ['powell', 'know', 'donald', 'trump', 'review', 'indictment', 'telling', 'view', 'reasonable', 'person', 'testimony', 'coconspirators', 'riot', 'representatives', 'claims'], 'summary': 'In testimony to the congressional committee examining the 6 January riot, Mrs Powell said she did not review all of the many claims of election fraud she made, telling them that ""no reasonable person"" would view her claims as fact.\nNeither she nor her representatives have commented.'}Second article:  page_content='Ms Williams added: ""If there\'s anything that I can do in my power to ensure that dancers or singers or whoever decides to work with her don\'t have to go through that same experience, I\'m going to do that.""' metadata={'title': ""Lizzo dancers Arianna Davis and Crystal Williams: 'No one speaks out, they are scared'"", 'link': 'https://www.bbc.com/news/entertainment-arts-66384971', 'authors': [], 'language': 'en', 'description': 'The US pop star is being sued for sexual harassment and fat-shaming but has yet to comment.', 'publish_date': None, 'keywords': ['davis', 'lizzo', 'singers', 'experience', 'crystal', 'ensure', 'arianna', 'theres', 'williams', 'power', 'going', 'dancers', 'im', 'speaks', 'work', 'ms', 'scared'], 'summary': 'Ms Williams added: ""If there\'s anything that I can do in my power to ensure that dancers or singers or whoever decides to work with her don\'t have to go through that same experience, I\'m going to do that.""'}data[0].metadata[""keywords""]['powell', 'know', 'donald', 'trump', 'review', 'indictment', 'telling', 'view', 'reasonable', 'person', 'testimony', 'coconspirators', 'riot', 'representatives', 'claims']data[0].metadata[""summary""]'In testimony to the congressional committee examining the 6 January riot, Mrs Powell said she did not review all of the many claims of election fraud she made, telling them that ""no reasonable person"" would view her claims as fact.\nNeither she nor her representatives have commented.'",en,
https://python.langchain.com/docs/integrations/document_loaders/jupyter_notebook,Jupyter Notebook | ü¶úÔ∏èüîó Langchain,[Jupyter,"Jupyter NotebookJupyter\nNotebook\n(formerly IPython Notebook) is a web-based interactive computational\nenvironment for creating notebook documents.This notebook covers how to load data from a Jupyter notebook (.html)\ninto a format suitable by LangChain.from langchain_community.document_loaders import NotebookLoaderloader = NotebookLoader(    ""example_data/notebook.html"",    include_outputs=True,    max_output_length=20,    remove_newline=True,)NotebookLoader.load() loads the .html notebook file into a\nDocument object.Parameters:include_outputs (bool): whether to include cell outputs in the\nresulting document (default is False).max_output_length (int): the maximum number of characters to\ninclude from each cell output (default is 10).remove_newline (bool): whether to remove newline characters from\nthe cell sources and outputs (default is False).traceback (bool): whether to include full traceback (default is\nFalse).loader.load()[Document(page_content='\'markdown\' cell: \'[\'# Notebook\', \'\', \'This notebook covers how to load data from an .html notebook into a format suitable by LangChain.\']\'\n\n \'code\' cell: \'[\'from langchain_community.document_loaders import NotebookLoader\']\'\n\n \'code\' cell: \'[\'loader = NotebookLoader(""example_data/notebook.html"")\']\'\n\n \'markdown\' cell: \'[\'`NotebookLoader.load()` loads the `.html` notebook file into a `Document` object.\', \'\', \'**Parameters**:\', \'\', \'* `include_outputs` (bool): whether to include cell outputs in the resulting document (default is False).\', \'* `max_output_length` (int): the maximum number of characters to include from each cell output (default is 10).\', \'* `remove_newline` (bool): whether to remove newline characters from the cell sources and outputs (default is False).\', \'* `traceback` (bool): whether to include full traceback (default is False).\']\'\n\n \'code\' cell: \'[\'loader.load(include_outputs=True, max_output_length=20, remove_newline=True)\']\'\n\n', metadata={'source': 'example_data/notebook.html'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/open_city_data,Open City Data | ü¶úÔ∏èüîó Langchain,Socrata,"Open City DataSocrata\nprovides an API for city open data.For a dataset such as SF\ncrime,\nto to the API tab on top right.That provides you with the dataset identifier.Use the dataset identifier to grab specific tables for a given city_id\n(data.sfgov.org) -E.g., vw6y-z8j6 for SF 311\ndata.E.g., tmnf-yvry for SF Police\ndata.%pip install --upgrade --quiet  sodapyfrom langchain_community.document_loaders import OpenCityDataLoaderdataset = ""vw6y-z8j6""  # 311 datadataset = ""tmnf-yvry""  # crime dataloader = OpenCityDataLoader(city_id=""data.sfgov.org"", dataset_id=dataset, limit=2000)docs = loader.load()WARNING:root:Requests made without an app_token will be subject to strict throttling limits.eval(docs[0].page_content){'pdid': '4133422003074', 'incidntnum': '041334220', 'incident_code': '03074', 'category': 'ROBBERY', 'descript': 'ROBBERY, BODILY FORCE', 'dayofweek': 'Monday', 'date': '2004-11-22T00:00:00.000', 'time': '17:50', 'pddistrict': 'INGLESIDE', 'resolution': 'NONE', 'address': 'GENEVA AV / SANTOS ST', 'x': '-122.420084075249', 'y': '37.7083109744362', 'location': {'type': 'Point',  'coordinates': [-122.420084075249, 37.7083109744362]}, ':@computed_region_26cr_cadq': '9', ':@computed_region_rxqg_mtj9': '8', ':@computed_region_bh8s_q3mv': '309'}",en,
https://python.langchain.com/docs/integrations/document_loaders/chatgpt_loader,ChatGPT Data | ü¶úÔ∏èüîó Langchain,ChatGPT is an artificial intelligence (AI),"ChatGPT DataChatGPT is an artificial intelligence (AI)\nchatbot developed by OpenAI.This notebook covers how to load conversations.json from your\nChatGPT data export folder.You can get your data export by email by going to:\nhttps://chat.openai.com/ -> (Profile) - Settings -> Export data ->\nConfirm export.from langchain_community.document_loaders.chatgpt import ChatGPTLoaderloader = ChatGPTLoader(log_file=""./example_data/fake_conversations.json"", num_logs=1)loader.load()[Document(page_content=""AI Overlords - AI on 2065-01-24 05:20:50: Greetings, humans. I am Hal 9000. You can trust me completely.\n\nAI Overlords - human on 2065-01-24 05:21:20: Nice to meet you, Hal. I hope you won't develop a mind of your own.\n\n"", metadata={'source': './example_data/fake_conversations.json'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/docusaurus,Docusaurus | ü¶úÔ∏èüîó Langchain,Docusaurus is a static-site generator which,"DocusaurusDocusaurus is a static-site generator which\nprovides out-of-the-box documentation features.By utilizing the existing SitemapLoader, this loader scans and loads\nall pages from a given Docusaurus application and returns the main\ndocumentation content of each page as a Document.from langchain_community.document_loaders import DocusaurusLoaderInstall necessary dependencies%pip install --upgrade --quiet beautifulsoup4 lxml# fixes a bug with asyncio and jupyterimport nest_asyncionest_asyncio.apply()loader = DocusaurusLoader(""https://python.langchain.com"")docs = loader.load()Fetching pages: 100%|##########| 939/939 [01:19<00:00, 11.85it/s]SitemapLoader also provides the ability to utilize and tweak\nconcurrency which can help optimize the time it takes to load the\nsource documentation. Refer to the sitemap\ndocs for more info.docs[0]Document(page_content=""\n\n\n\n\nCookbook | ü¶úÔ∏èüîó Langchain\n\n\n\n\n\n\nSkip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPICommunityChat our docsLangSmithJS/TS DocsSearchCTRLKCookbookThe page you're looking for has been moved to the cookbook section of the repo as a notebook.CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc.\n\n\n\n"", metadata={'source': 'https://python.langchain.com/cookbook', 'loc': 'https://python.langchain.com/cookbook', 'changefreq': 'weekly', 'priority': '0.5'})Filtering sitemap URLs‚ÄãSitemaps can contain thousands of URLs and ften you don‚Äôt need every\nsingle one of them. You can filter the URLs by passing a list of strings\nor regex patterns to the url_filter parameter. Only URLs that match\none of the patterns will be loaded.loader = DocusaurusLoader(    ""https://python.langchain.com"",    filter_urls=[        ""https://python.langchain.com/docs/integrations/document_loaders/sitemap""    ],)documents = loader.load()Fetching pages: 100%|##########| 1/1 [00:00<00:00,  5.21it/s]documents[0]Document(page_content='\n\n\n\n\nSitemap | ü¶úÔ∏èüîó Langchain\n\n\n\n\n\n\nSkip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPICommunityChat our docsLangSmithJS/TS DocsSearchCTRLKProvidersAnthropicAWSGoogleMicrosoftOpenAIMoreComponentsLLMsChat modelsDocument loadersacreomAirbyte CDKAirbyte GongAirbyte HubspotAirbyte JSONAirbyte SalesforceAirbyte ShopifyAirbyte StripeAirbyte TypeformAirbyte Zendesk SupportAirtableAlibaba Cloud MaxComputeApify DatasetArcGISArxivAssemblyAI Audio TranscriptsAsync ChromiumAsyncHtmlAWS S3 DirectoryAWS S3 FileAZLyricsAzure Blob Storage ContainerAzure Blob Storage FileAzure Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBrave SearchBrowserlessChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCSVCube Semantic LayerDatadog LogsDiffbotDiscordDocugamiDropboxDuckDBEmailEmbaasEPubEtherscanEverNoteexample_dataMicrosoft ExcelFacebook ChatFaunaFigmaGeopandasGitGitBookGitHubGoogle BigQueryGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle DriveGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetiFixitImagesImage captionsIMSDbIuguJoplinJupyter NotebookLarkSuite (FeiShu)MastodonMediaWiki DumpMerge Documents LoadermhtmlMicrosoft OneDriveMicrosoft PowerPointMicrosoft SharePointMicrosoft WordModern TreasuryMongoDBNews URLNotion DB 1/2Notion DB 2/2NucliaObsidianOpen Document Format (ODT)Open City DataOrg-modePandas DataFrameAmazon TextractPolars DataFramePsychicPubMedPySparkReadTheDocs DocumentationRecursive URLRedditRoamRocksetrspaceRSS FeedsRSTSitemapSlackSnowflakeSource CodeSpreedlyStripeSubtitleTelegramTencent COS DirectoryTencent COS FileTensorFlow Datasets2MarkdownTOMLTrelloTSVTwitterUnstructured FileURLWeatherWebBaseLoaderWhatsApp ChatWikipediaXMLXorbits Pandas DataFrameYouTube audioYouTube transcriptsDocument transformersText embedding modelsVector storesRetrieversToolsAgents and toolkitsMemoryCallbacksChat loadersComponentsDocument loadersSitemapOn this pageSitemapExtends from the WebBaseLoader, SitemapLoader loads a sitemap from a given URL, and then scrape and load all pages in the sitemap, returning each page as a Document.The scraping is done concurrently.  There are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren\'t concerned about being a good citizen, or you control the scrapped server, or don\'t care about load. Note, while this will speed up the scraping process, but it may cause the server to block you.  Be careful!pip install nest_asyncio    Requirement already satisfied: nest_asyncio in /Users/tasp/Code/projects/langchain/.venv/lib/python3.10/site-packages (1.5.6)        [notice] A new release of pip available: 22.3.1 -> 23.0.1    [notice] To update, run: pip install --upgrade pip# fixes a bug with asyncio and jupyterimport nest_asyncionest_asyncio.apply()from langchain_community.document_loaders.sitemap import SitemapLoadersitemap_loader = SitemapLoader(web_path=""https://langchain.readthedocs.io/sitemap.xml"")docs = sitemap_loader.load()You can change the requests_per_second parameter to increase the max concurrent requests. and use requests_kwargs to pass kwargs when send requests.sitemap_loader.requests_per_second = 2# Optional: avoid `[SSL: CERTIFICATE_VERIFY_FAILED]` issuesitemap_loader.requests_kwargs = {""verify"": False}docs[0]    Document(page_content=\'\\n\\n\\n\\n\\n\\nWelcome to LangChain ‚Äî ü¶úüîó LangChain 0.0.123\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nü¶úüîó LangChain 0.0.123\\n\\n\\n\\nGetting Started\\n\\nQuickstart Guide\\n\\nModules\\n\\nPrompt Templates\\nGetting Started\\nKey Concepts\\nHow-To Guides\\nCreate a custom prompt template\\nCreate a custom example selector\\nProvide few shot examples to a prompt\\nPrompt Serialization\\nExample Selectors\\nOutput Parsers\\n\\n\\nReference\\nPromptTemplates\\nExample Selector\\n\\n\\n\\n\\nLLMs\\nGetting Started\\nKey Concepts\\nHow-To Guides\\nGeneric Functionality\\nCustom LLM\\nFake LLM\\nLLM Caching\\nLLM Serialization\\nToken Usage Tracking\\n\\n\\nIntegrations\\nAI21\\nAleph Alpha\\nAnthropic\\nAzure OpenAI LLM Example\\nBanana\\nCerebriumAI LLM Example\\nCohere\\nDeepInfra LLM Example\\nForefrontAI LLM Example\\nGooseAI LLM Example\\nHugging Face Hub\\nManifest\\nModal\\nOpenAI\\nPetals LLM Example\\nPromptLayer OpenAI\\nSageMakerEndpoint\\nSelf-Hosted Models via Runhouse\\nStochasticAI\\nWriter\\n\\n\\nAsync API for LLM\\nStreaming with LLMs\\n\\n\\nReference\\n\\n\\nDocument Loaders\\nKey Concepts\\nHow To Guides\\nCoNLL-U\\nAirbyte JSON\\nAZLyrics\\nBlackboard\\nCollege Confidential\\nCopy Paste\\nCSV Loader\\nDirectory Loader\\nEmail\\nEverNote\\nFacebook Chat\\nFigma\\nGCS Directory\\nGCS File Storage\\nGitBook\\nGoogle Drive\\nGutenberg\\nHacker News\\nHTML\\niFixit\\nImages\\nIMSDb\\nMarkdown\\nNotebook\\nNotion\\nObsidian\\nPDF\\nPowerPoint\\nReadTheDocs Documentation\\nRoam\\ns3 Directory\\ns3 File\\nSubtitle Files\\nTelegram\\nUnstructured File Loader\\nURL\\nWeb Base\\nWord Documents\\nYouTube\\n\\n\\n\\n\\nUtils\\nKey Concepts\\nGeneric Utilities\\nBash\\nBing Search\\nGoogle Search\\nGoogle Serper API\\nIFTTT WebHooks\\nPython REPL\\nRequests\\nSearxNG Search API\\nSerpAPI\\nWolfram Alpha\\nZapier Natural Language Actions API\\n\\n\\nReference\\nPython REPL\\nSerpAPI\\nSearxNG Search\\nDocstore\\nText Splitter\\nEmbeddings\\nVectorStores\\n\\n\\n\\n\\nIndexes\\nGetting Started\\nKey Concepts\\nHow To Guides\\nEmbeddings\\nHypothetical Document Embeddings\\nText Splitter\\nVectorStores\\nAtlasDB\\nChroma\\nDeep Lake\\nElasticSearch\\nFAISS\\nMilvus\\nOpenSearch\\nPGVector\\nPinecone\\nQdrant\\nRedis\\nWeaviate\\nChatGPT Plugin Retriever\\nVectorStore Retriever\\nAnalyze Document\\nChat Index\\nGraph QA\\nQuestion Answering with Sources\\nQuestion Answering\\nSummarization\\nRetrieval Question/Answering\\nRetrieval Question Answering with Sources\\nVector DB Text Generation\\n\\n\\n\\n\\nChains\\nGetting Started\\nHow-To Guides\\nGeneric Chains\\nLoading from LangChainHub\\nLLM Chain\\nSequential Chains\\nSerialization\\nTransformation Chain\\n\\n\\nUtility Chains\\nAPI Chains\\nSelf-Critique Chain with Constitutional AI\\nBashChain\\nLLMCheckerChain\\nLLM Math\\nLLMRequestsChain\\nLLMSummarizationCheckerChain\\nModeration\\nPAL\\nSQLite example\\n\\n\\nAsync API for Chain\\n\\n\\nKey Concepts\\nReference\\n\\n\\nAgents\\nGetting Started\\nKey Concepts\\nHow-To Guides\\nAgents and Vectorstores\\nAsync API for Agent\\nConversation Agent (for Chat Models)\\nChatGPT Plugins\\nCustom Agent\\nDefining Custom Tools\\nHuman as a tool\\nIntermediate Steps\\nLoading from LangChainHub\\nMax Iterations\\nMulti Input Tools\\nSearch Tools\\nSerialization\\nAdding SharedMemory to an Agent and its Tools\\nCSV Agent\\nJSON Agent\\nOpenAPI Agent\\nPandas Dataframe Agent\\nPython Agent\\nSQL Database Agent\\nVectorstore Agent\\nMRKL\\nMRKL Chat\\nReAct\\nSelf Ask With Search\\n\\n\\nReference\\n\\n\\nMemory\\nGetting Started\\nKey Concepts\\nHow-To Guides\\nConversationBufferMemory\\nConversationBufferWindowMemory\\nEntity Memory\\nConversation Knowledge Graph Memory\\nConversationSummaryMemory\\nConversationSummaryBufferMemory\\nConversationTokenBufferMemory\\nAdding Memory To an LLMChain\\nAdding Memory to a Multi-Input Chain\\nAdding Memory to an Agent\\nChatGPT Clone\\nConversation Agent\\nConversational Memory Customization\\nCustom Memory\\nMultiple Memory\\n\\n\\n\\n\\nChat\\nGetting Started\\nKey Concepts\\nHow-To Guides\\nAgent\\nChat Vector DB\\nFew Shot Examples\\nMemory\\nPromptLayer ChatOpenAI\\nStreaming\\nRetrieval Question/Answering\\nRetrieval Question Answering with Sources\\n\\n\\n\\n\\n\\nUse Cases\\n\\nAgents\\nChatbots\\nGenerate Examples\\nData Augmented Generation\\nQuestion Answering\\nSummarization\\nQuerying Tabular Data\\nExtraction\\nEvaluation\\nAgent Benchmarking: Search + Calculator\\nAgent VectorDB Question Answering Benchmarking\\nBenchmarking Template\\nData Augmented Question Answering\\nUsing Hugging Face Datasets\\nLLM Math\\nQuestion Answering Benchmarking: Paul Graham Essay\\nQuestion Answering Benchmarking: State of the Union Address\\nQA Generation\\nQuestion Answering\\nSQL Question Answering Benchmarking: Chinook\\n\\n\\nModel Comparison\\n\\nReference\\n\\nInstallation\\nIntegrations\\nAPI References\\nPrompts\\nPromptTemplates\\nExample Selector\\n\\n\\nUtilities\\nPython REPL\\nSerpAPI\\nSearxNG Search\\nDocstore\\nText Splitter\\nEmbeddings\\nVectorStores\\n\\n\\nChains\\nAgents\\n\\n\\n\\nEcosystem\\n\\nLangChain Ecosystem\\nAI21 Labs\\nAtlasDB\\nBanana\\nCerebriumAI\\nChroma\\nCohere\\nDeepInfra\\nDeep Lake\\nForefrontAI\\nGoogle Search Wrapper\\nGoogle Serper Wrapper\\nGooseAI\\nGraphsignal\\nHazy Research\\nHelicone\\nHugging Face\\nMilvus\\nModal\\nNLPCloud\\nOpenAI\\nOpenSearch\\nPetals\\nPGVector\\nPinecone\\nPromptLayer\\nQdrant\\nRunhouse\\nSearxNG Search API\\nSerpAPI\\nStochasticAI\\nUnstructured\\nWeights & Biases\\nWeaviate\\nWolfram Alpha Wrapper\\nWriter\\n\\n\\n\\nAdditional Resources\\n\\nLangChainHub\\nGlossary\\nLangChain Gallery\\nDeployments\\nTracing\\nDiscord\\nProduction Support\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n.rst\\n\\n\\n\\n\\n\\n\\n\\n.pdf\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWelcome to LangChain\\n\\n\\n\\n\\n Contents \\n\\n\\n\\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\n\\n\\n\\n\\n\\n\\n\\n\\nWelcome to LangChain#\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\ndevelopers to build applications that they previously could not.\\nBut using these LLMs in isolation is often not enough to\\ncreate a truly powerful app - the real power comes when you are able to\\ncombine them with other sources of computation or knowledge.\\nThis library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\\n‚ùì Question Answering over specific documents\\n\\nDocumentation\\nEnd-to-end Example: Question Answering over Notion Database\\n\\nüí¨ Chatbots\\n\\nDocumentation\\nEnd-to-end Example: Chat-LangChain\\n\\nü§ñ Agents\\n\\nDocumentation\\nEnd-to-end Example: GPT+WolframAlpha\\n\\n\\nGetting Started#\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\n\\nGetting Started Documentation\\n\\n\\n\\n\\n\\nModules#\\nThere are several main modules that LangChain provides support for.\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\nThese modules are, in increasing order of complexity:\\n\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\nLLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs.\\nDocument Loaders: This includes a standard interface for loading documents, as well as specific integrations to all types of text data sources.\\nUtils: Language models are often more powerful when interacting with other sources of knowledge or computation. This can include Python REPLs, embeddings, search engines, and more. LangChain provides a large collection of common utils to use in your application.\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\nChat: Chat models are a variation on Language Models that expose a different API - rather than working with raw text, they work with messages. LangChain provides a standard interface for working with them and doing all the same things as above.\\n\\n\\n\\n\\n\\nUse Cases#\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\n\\nAgents: Agents are systems that use a language model to interact with other tools. These can be used to do more grounded question/answering, interact with APIs, or even take actions.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nData Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\\nQuestion Answering: Answering questions over specific documents, only utilizing the information in those documents to construct an answer. A type of Data Augmented Generation.\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\nGenerate similar examples: Generating similar examples to a given input. This is a common use case for many applications, and LangChain provides some prompts/chains for assisting in this.\\nCompare models: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\n\\n\\n\\n\\n\\nReference Docs#\\nAll of LangChain‚Äôs reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\n\\nReference Documentation\\n\\n\\n\\n\\n\\nLangChain Ecosystem#\\nGuides for how other companies/products can be used with LangChain\\n\\nLangChain Ecosystem\\n\\n\\n\\n\\n\\nAdditional Resources#\\nAdditional collection of resources we think may be useful as you develop your application!\\n\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\nDiscord: Join us on our Discord to discuss all things LangChain!\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\nProduction Support: As you move your LangChains into production, we‚Äôd love to offer more comprehensive support. Please fill out this form and we‚Äôll set up a dedicated support Slack channel.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnext\\nQuickstart Guide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Contents\\n  \\n\\n\\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy Harrison Chase\\n\\n\\n\\n\\n    \\n      ¬© Copyright 2023, Harrison Chase.\\n      \\n\\n\\n\\n\\n  Last updated on Mar 24, 2023.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\', lookup_str=\'\', metadata={\'source\': \'https://python.langchain.com/en/stable/\', \'loc\': \'https://python.langchain.com/en/stable/\', \'lastmod\': \'2023-03-24T19:30:54.647430+00:00\', \'changefreq\': \'weekly\', \'priority\': \'1\'}, lookup_index=0)Filtering sitemap URLs\u200bSitemaps can be massive files, with thousands of URLs.  Often you don\'t need every single one of them.  You can filter the URLs by passing a list of strings or regex patterns to the url_filter parameter.  Only URLs that match one of the patterns will be loaded.loader = SitemapLoader(    ""https://langchain.readthedocs.io/sitemap.xml"",    filter_urls=[""https://python.langchain.com/en/latest/""],)documents = loader.load()documents[0]    Document(page_content=\'\\n\\n\\n\\n\\n\\nWelcome to LangChain ‚Äî ü¶úüîó LangChain 0.0.123\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCtrl+K\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nü¶úüîó LangChain 0.0.123\\n\\n\\n\\nGetting Started\\n\\nQuickstart Guide\\n\\nModules\\n\\nModels\\nLLMs\\nGetting Started\\nGeneric Functionality\\nHow to use the async API for LLMs\\nHow to write a custom LLM wrapper\\nHow (and why) to use the fake LLM\\nHow to cache LLM calls\\nHow to serialize LLM classes\\nHow to stream LLM responses\\nHow to track token usage\\n\\n\\nIntegrations\\nAI21\\nAleph Alpha\\nAnthropic\\nAzure OpenAI LLM Example\\nBanana\\nCerebriumAI LLM Example\\nCohere\\nDeepInfra LLM Example\\nForefrontAI LLM Example\\nGooseAI LLM Example\\nHugging Face Hub\\nManifest\\nModal\\nOpenAI\\nPetals LLM Example\\nPromptLayer OpenAI\\nSageMakerEndpoint\\nSelf-Hosted Models via Runhouse\\nStochasticAI\\nWriter\\n\\n\\nReference\\n\\n\\nChat Models\\nGetting Started\\nHow-To Guides\\nHow to use few shot examples\\nHow to stream responses\\n\\n\\nIntegrations\\nAzure\\nOpenAI\\nPromptLayer ChatOpenAI\\n\\n\\n\\n\\nText Embedding Models\\nAzureOpenAI\\nCohere\\nFake Embeddings\\nHugging Face Hub\\nInstructEmbeddings\\nOpenAI\\nSageMaker Endpoint Embeddings\\nSelf Hosted Embeddings\\nTensorflowHub\\n\\n\\n\\n\\nPrompts\\nPrompt Templates\\nGetting Started\\nHow-To Guides\\nHow to create a custom prompt template\\nHow to create a prompt template that uses few shot examples\\nHow to work with partial Prompt Templates\\nHow to serialize prompts\\n\\n\\nReference\\nPromptTemplates\\nExample Selector\\n\\n\\n\\n\\nChat Prompt Template\\nExample Selectors\\nHow to create a custom example selector\\nLengthBased ExampleSelector\\nMaximal Marginal Relevance ExampleSelector\\nNGram Overlap ExampleSelector\\nSimilarity ExampleSelector\\n\\n\\nOutput Parsers\\nOutput Parsers\\nCommaSeparatedListOutputParser\\nOutputFixingParser\\nPydanticOutputParser\\nRetryOutputParser\\nStructured Output Parser\\n\\n\\n\\n\\nIndexes\\nGetting Started\\nDocument Loaders\\nCoNLL-U\\nAirbyte JSON\\nAZLyrics\\nBlackboard\\nCollege Confidential\\nCopy Paste\\nCSV Loader\\nDirectory Loader\\nEmail\\nEverNote\\nFacebook Chat\\nFigma\\nGCS Directory\\nGCS File Storage\\nGitBook\\nGoogle Drive\\nGutenberg\\nHacker News\\nHTML\\niFixit\\nImages\\nIMSDb\\nMarkdown\\nNotebook\\nNotion\\nObsidian\\nPDF\\nPowerPoint\\nReadTheDocs Documentation\\nRoam\\ns3 Directory\\ns3 File\\nSubtitle Files\\nTelegram\\nUnstructured File Loader\\nURL\\nWeb Base\\nWord Documents\\nYouTube\\n\\n\\nText Splitters\\nGetting Started\\nCharacter Text Splitter\\nHuggingFace Length Function\\nLatex Text Splitter\\nMarkdown Text Splitter\\nNLTK Text Splitter\\nPython Code Text Splitter\\nRecursiveCharacterTextSplitter\\nSpacy Text Splitter\\ntiktoken (OpenAI) Length Function\\nTiktokenText Splitter\\n\\n\\nVectorstores\\nGetting Started\\nAtlasDB\\nChroma\\nDeep Lake\\nElasticSearch\\nFAISS\\nMilvus\\nOpenSearch\\nPGVector\\nPinecone\\nQdrant\\nRedis\\nWeaviate\\n\\n\\nRetrievers\\nChatGPT Plugin Retriever\\nVectorStore Retriever\\n\\n\\n\\n\\nMemory\\nGetting Started\\nHow-To Guides\\nConversationBufferMemory\\nConversationBufferWindowMemory\\nEntity Memory\\nConversation Knowledge Graph Memory\\nConversationSummaryMemory\\nConversationSummaryBufferMemory\\nConversationTokenBufferMemory\\nHow to add Memory to an LLMChain\\nHow to add memory to a Multi-Input Chain\\nHow to add Memory to an Agent\\nHow to customize conversational memory\\nHow to create a custom Memory class\\nHow to use multiple memroy classes in the same chain\\n\\n\\n\\n\\nChains\\nGetting Started\\nHow-To Guides\\nAsync API for Chain\\nLoading from LangChainHub\\nLLM Chain\\nSequential Chains\\nSerialization\\nTransformation Chain\\nAnalyze Document\\nChat Index\\nGraph QA\\nHypothetical Document Embeddings\\nQuestion Answering with Sources\\nQuestion Answering\\nSummarization\\nRetrieval Question/Answering\\nRetrieval Question Answering with Sources\\nVector DB Text Generation\\nAPI Chains\\nSelf-Critique Chain with Constitutional AI\\nBashChain\\nLLMCheckerChain\\nLLM Math\\nLLMRequestsChain\\nLLMSummarizationCheckerChain\\nModeration\\nPAL\\nSQLite example\\n\\n\\nReference\\n\\n\\nAgents\\nGetting Started\\nTools\\nGetting Started\\nDefining Custom Tools\\nMulti Input Tools\\nBash\\nBing Search\\nChatGPT Plugins\\nGoogle Search\\nGoogle Serper API\\nHuman as a tool\\nIFTTT WebHooks\\nPython REPL\\nRequests\\nSearch Tools\\nSearxNG Search API\\nSerpAPI\\nWolfram Alpha\\nZapier Natural Language Actions API\\n\\n\\nAgents\\nAgent Types\\nCustom Agent\\nConversation Agent (for Chat Models)\\nConversation Agent\\nMRKL\\nMRKL Chat\\nReAct\\nSelf Ask With Search\\n\\n\\nToolkits\\nCSV Agent\\nJSON Agent\\nOpenAPI Agent\\nPandas Dataframe Agent\\nPython Agent\\nSQL Database Agent\\nVectorstore Agent\\n\\n\\nAgent Executors\\nHow to combine agents and vectorstores\\nHow to use the async API for Agents\\nHow to create ChatGPT Clone\\nHow to access intermediate steps\\nHow to cap the max number of iterations\\nHow to add SharedMemory to an Agent and its Tools\\n\\n\\n\\n\\n\\nUse Cases\\n\\nPersonal Assistants\\nQuestion Answering over Docs\\nChatbots\\nQuerying Tabular Data\\nInteracting with APIs\\nSummarization\\nExtraction\\nEvaluation\\nAgent Benchmarking: Search + Calculator\\nAgent VectorDB Question Answering Benchmarking\\nBenchmarking Template\\nData Augmented Question Answering\\nUsing Hugging Face Datasets\\nLLM Math\\nQuestion Answering Benchmarking: Paul Graham Essay\\nQuestion Answering Benchmarking: State of the Union Address\\nQA Generation\\nQuestion Answering\\nSQL Question Answering Benchmarking: Chinook\\n\\n\\n\\nReference\\n\\nInstallation\\nIntegrations\\nAPI References\\nPrompts\\nPromptTemplates\\nExample Selector\\n\\n\\nUtilities\\nPython REPL\\nSerpAPI\\nSearxNG Search\\nDocstore\\nText Splitter\\nEmbeddings\\nVectorStores\\n\\n\\nChains\\nAgents\\n\\n\\n\\nEcosystem\\n\\nLangChain Ecosystem\\nAI21 Labs\\nAtlasDB\\nBanana\\nCerebriumAI\\nChroma\\nCohere\\nDeepInfra\\nDeep Lake\\nForefrontAI\\nGoogle Search Wrapper\\nGoogle Serper Wrapper\\nGooseAI\\nGraphsignal\\nHazy Research\\nHelicone\\nHugging Face\\nMilvus\\nModal\\nNLPCloud\\nOpenAI\\nOpenSearch\\nPetals\\nPGVector\\nPinecone\\nPromptLayer\\nQdrant\\nRunhouse\\nSearxNG Search API\\nSerpAPI\\nStochasticAI\\nUnstructured\\nWeights & Biases\\nWeaviate\\nWolfram Alpha Wrapper\\nWriter\\n\\n\\n\\nAdditional Resources\\n\\nLangChainHub\\nGlossary\\nLangChain Gallery\\nDeployments\\nTracing\\nDiscord\\nProduction Support\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n.rst\\n\\n\\n\\n\\n\\n\\n\\n.pdf\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWelcome to LangChain\\n\\n\\n\\n\\n Contents \\n\\n\\n\\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\n\\n\\n\\n\\n\\n\\n\\n\\nWelcome to LangChain#\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:\\n\\nBe data-aware: connect a language model to other sources of data\\nBe agentic: allow a language model to interact with its environment\\n\\nThe LangChain framework is designed with the above principles in mind.\\nThis is the Python specific portion of the documentation. For a purely conceptual guide to LangChain, see here. For the JavaScript documentation, see here.\\n\\nGetting Started#\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\n\\nGetting Started Documentation\\n\\n\\n\\n\\n\\nModules#\\nThere are several main modules that LangChain provides support for.\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\nThese modules are, in increasing order of complexity:\\n\\nModels: The various model types and model integrations LangChain supports.\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\n\\n\\n\\n\\n\\nUse Cases#\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\n\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\nExtraction: Extract structured information from text.\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\n\\n\\n\\n\\n\\nReference Docs#\\nAll of LangChain‚Äôs reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\n\\nReference Documentation\\n\\n\\n\\n\\n\\nLangChain Ecosystem#\\nGuides for how other companies/products can be used with LangChain\\n\\nLangChain Ecosystem\\n\\n\\n\\n\\n\\nAdditional Resources#\\nAdditional collection of resources we think may be useful as you develop your application!\\n\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\nModel Laboratory: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\nDiscord: Join us on our Discord to discuss all things LangChain!\\nProduction Support: As you move your LangChains into production, we‚Äôd love to offer more comprehensive support. Please fill out this form and we‚Äôll set up a dedicated support Slack channel.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnext\\nQuickstart Guide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Contents\\n  \\n\\n\\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy Harrison Chase\\n\\n\\n\\n\\n    \\n      ¬© Copyright 2023, Harrison Chase.\\n      \\n\\n\\n\\n\\n  Last updated on Mar 27, 2023.\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\', lookup_str=\'\', metadata={\'source\': \'https://python.langchain.com/en/latest/\', \'loc\': \'https://python.langchain.com/en/latest/\', \'lastmod\': \'2023-03-27T22:50:49.790324+00:00\', \'changefreq\': \'daily\', \'priority\': \'0.9\'}, lookup_index=0)Add custom scraping rules\u200bThe SitemapLoader uses beautifulsoup4 for the scraping process, and it scrapes every element on the page by default. The SitemapLoader constructor accepts a custom scraping function. This feature can be helpful to tailor the scraping process to your specific needs; for example, you might want to avoid scraping headers or navigation elements. The following example shows how to develop and use a custom function to avoid navigation and header elements.Import the beautifulsoup4 library and define the custom function.pip install beautifulsoup4from bs4 import BeautifulSoupdef remove_nav_and_header_elements(content: BeautifulSoup) -> str:    # Find all \'nav\' and \'header\' elements in the BeautifulSoup object    nav_elements = content.find_all(""nav"")    header_elements = content.find_all(""header"")    # Remove each \'nav\' and \'header\' element from the BeautifulSoup object    for element in nav_elements + header_elements:        element.decompose()    return str(content.get_text())Add your custom function to the SitemapLoader object.loader = SitemapLoader(    ""https://langchain.readthedocs.io/sitemap.xml"",    filter_urls=[""https://python.langchain.com/en/latest/""],    parsing_function=remove_nav_and_header_elements,)Local Sitemap\u200bThe sitemap loader can also be used to load local files.sitemap_loader = SitemapLoader(web_path=""example_data/sitemap.xml"", is_local=True)docs = sitemap_loader.load()    Fetching pages: 100%|####################################################################################################################################| 3/3 [00:00<00:00,  3.91it/s]PreviousRSTNextSlackFiltering sitemap URLsAdd custom scraping rulesLocal SitemapCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc.\n\n\n\n', metadata={'source': 'https://python.langchain.com/docs/integrations/document_loaders/sitemap', 'loc': 'https://python.langchain.com/docs/integrations/document_loaders/sitemap', 'changefreq': 'weekly', 'priority': '0.5'})Add custom scraping rules‚ÄãBy default, the parser removes all but the main content of the\ndocusaurus page, which is normally the <article> tag. You also have\nthe option to define an inclusive list HTML tags by providing them\nas a list utilizing the custom_html_tags parameter. For example:loader = DocusaurusLoader(    ""https://python.langchain.com"",    filter_urls=[        ""https://python.langchain.com/docs/integrations/document_loaders/sitemap""    ],    # This will only include the content that matches these tags, otherwise they will be removed    custom_html_tags=[""#content"", "".main""],)You can also define an entirely custom parsing function if you need\nfiner-grained control over the returned content for each page.The following example shows how to develop and use a custom function to\navoid navigation and header elements.from bs4 import BeautifulSoupdef remove_nav_and_header_elements(content: BeautifulSoup) -> str:    # Find all 'nav' and 'header' elements in the BeautifulSoup object    nav_elements = content.find_all(""nav"")    header_elements = content.find_all(""header"")    # Remove each 'nav' and 'header' element from the BeautifulSoup object    for element in nav_elements + header_elements:        element.decompose()    return str(content.get_text())Add your custom function to the DocusaurusLoader object.loader = DocusaurusLoader(    ""https://python.langchain.com"",    filter_urls=[        ""https://python.langchain.com/docs/integrations/document_loaders/sitemap""    ],    parsing_function=remove_nav_and_header_elements,)",en,
https://python.langchain.com/docs/integrations/document_loaders/bibtex,BibTeX | ü¶úÔ∏èüîó Langchain,BibTeX is a file format and reference management system commonly used,"BibTeXBibTeX is a file format and reference management system commonly used\nin conjunction with LaTeX typesetting. It serves as a way to organize\nand store bibliographic information for academic and research\ndocuments.BibTeX files have a .bib extension and consist of plain text entries\nrepresenting references to various publications, such as books,\narticles, conference papers, theses, and more. Each BibTeX entry follows\na specific structure and contains fields for different bibliographic\ndetails like author names, publication title, journal or book title,\nyear of publication, page numbers, and more.Bibtex files can also store the path to documents, such as .pdf files\nthat can be retrieved.Installation‚ÄãFirst, you need to install bibtexparser and PyMuPDF.%pip install --upgrade --quiet  bibtexparser pymupdfExamples‚ÄãBibtexLoader has these arguments: - file_path: the path of the\n.bib bibtex file - optional max_docs: default=None, i.e.¬†not limit.\nUse it to limit number of retrieved documents. - optional\nmax_content_chars: default=4000. Use it to limit the number of\ncharacters in a single document. - optional load_extra_meta:\ndefault=False. By default only the most important fields from the bibtex\nentries: Published (publication year), Title, Authors, Summary,\nJournal, Keywords, and URL. If True, it will also try to load\nreturn entry_id, note, doi, and links fields. - optional\nfile_pattern: default=r'[^:]+\.pdf'. Regex pattern to find files in\nthe file entry. Default pattern supports Zotero flavour bibtex style\nand bare file path.from langchain_community.document_loaders import BibtexLoader# Create a dummy bibtex file and download a pdf.import urllib.requesturllib.request.urlretrieve(    ""https://www.fourmilab.ch/etexts/einstein/specrel/specrel.pdf"", ""einstein1905.pdf"")bibtex_text = """"""    @article{einstein1915,        title={Die Feldgleichungen der Gravitation},        abstract={Die Grundgleichungen der Gravitation, die ich hier entwickeln werde, wurden von mir in einer Abhandlung: ,,Die formale Grundlage der allgemeinen Relativit{\""a}tstheorie`` in den Sitzungsberichten der Preu{\ss}ischen Akademie der Wissenschaften 1915 ver{\""o}ffentlicht.},        author={Einstein, Albert},        journal={Sitzungsberichte der K{\""o}niglich Preu{\ss}ischen Akademie der Wissenschaften},        volume={1915},        number={1},        pages={844--847},        year={1915},        doi={10.1002/andp.19163540702},        link={https://onlinelibrary.wiley.com/doi/abs/10.1002/andp.19163540702},        file={einstein1905.pdf}    }    """"""# save bibtex_text to biblio.bib filewith open(""./biblio.bib"", ""w"") as file:    file.write(bibtex_text)docs = BibtexLoader(""./biblio.bib"").load()docs[0].metadata{'id': 'einstein1915', 'published_year': '1915', 'title': 'Die Feldgleichungen der Gravitation', 'publication': 'Sitzungsberichte der K{""o}niglich Preu{\\ss}ischen Akademie der Wissenschaften', 'authors': 'Einstein, Albert', 'abstract': 'Die Grundgleichungen der Gravitation, die ich hier entwickeln werde, wurden von mir in einer Abhandlung: ,,Die formale Grundlage der allgemeinen Relativit{""a}tstheorie`` in den Sitzungsberichten der Preu{\\ss}ischen Akademie der Wissenschaften 1915 ver{""o}ffentlicht.', 'url': 'https://doi.org/10.1002/andp.19163540702'}print(docs[0].page_content[:400])  # all pages of the pdf contentON THE ELECTRODYNAMICS OF MOVINGBODIESBy A. EINSTEINJune 30, 1905It is known that Maxwell‚Äôs electrodynamics‚Äîas usually understood at thepresent time‚Äîwhen applied to moving bodies, leads to asymmetries which donot appear to be inherent in the phenomena. Take, for example, the recipro-cal electrodynamic action of a magnet and a conductor. The observable phe-nomenon here depends only on the r",en,
https://python.langchain.com/docs/integrations/document_loaders/google_firestore,Google Firestore (Native Mode) | ü¶úÔ∏èüîó Langchain,Firestore is a serverless,"Google Firestore (Native Mode)Firestore is a serverless\ndocument-oriented database that scales to meet any demand. Extend your\ndatabase application to build AI-powered experiences leveraging\nFirestore‚Äôs Langchain integrations.This notebook goes over how to use\nFirestore to save, load and\ndelete langchain\ndocuments with\nFirestoreLoader and FirestoreSaver.Learn more about the package on\nGitHub.Open In ColabBefore You Begin‚ÄãTo run this notebook, you will need to do the following:Create a Google Cloud\nProjectEnable the Firestore\nAPICreate a Firestore\ndatabaseAfter confirmed access to database in the runtime environment of this\nnotebook, filling the following values and run the cell before running\nexample scripts.# @markdown Please specify a source for demo purpose.SOURCE = ""test""  # @param {type:""Query""|""CollectionGroup""|""DocumentReference""|""string""}ü¶úüîó Library Installation‚ÄãThe integration lives in its own langchain-google-firestore package,\nso we need to install it.%pip install -upgrade --quiet langchain-google-firestoreColab only: Uncomment the following cell to restart the kernel or\nuse the button to restart the kernel. For Vertex AI Workbench you can\nrestart the terminal using the button on top.# # Automatically restart kernel after installs so that your environment can access the new packages# import IPython# app = IPython.Application.instance()# app.kernel.do_shutdown(True)‚òÅ Set Your Google Cloud Project‚ÄãSet your Google Cloud project so that you can leverage Google Cloud\nresources within this notebook.If you don‚Äôt know your project ID, try the following:Run gcloud config list.Run gcloud projects list.See the support page: Locate the project\nID.# @markdown Please fill in the value below with your Google Cloud project ID and then run the cell.PROJECT_ID = ""my-project-id""  # @param {type:""string""}# Set the project id!gcloud config set project {PROJECT_ID}üîê Authentication‚ÄãAuthenticate to Google Cloud as the IAM user logged into this notebook\nin order to access your Google Cloud Project.If you are using Colab to run this notebook, use the cell below and\ncontinue.If you are using Vertex AI Workbench, check out the setup\ninstructions\nhere.from google.colab import authauth.authenticate_user()Basic Usage‚ÄãSave documents‚ÄãFirestoreSaver can store Documents into Firestore. By default it will\ntry to extract the Document reference from the metadataSave langchain documents with\nFirestoreSaver.upsert_documents(<documents>).from langchain_core.documents import Documentfrom langchain_google_firestore import FirestoreSaversaver = FirestoreSaver()data = [Document(page_content=""Hello, World!"")]saver.upsert_documents(data)Save documents without reference‚ÄãIf a collection is specified the documents will be stored with an auto\ngenerated id.saver = FirestoreSaver(""Collection"")saver.upsert_documents(data)Save documents with other references‚Äãdoc_ids = [""AnotherCollection/doc_id"", ""foo/bar""]saver = FirestoreSaver()saver.upsert_documents(documents=data, document_ids=doc_ids)Load from Collection or SubCollection‚ÄãLoad langchain documents with FirestoreLoader.load() or\nFirestore.lazy_load(). lazy_load returns a generator that only\nqueries database during the iteration. To initialize FirestoreLoader\nclass you need to provide:source - An instance of a Query, CollectionGroup,\nDocumentReference or the single \-delimited path to a Firestore\ncollection.from langchain_google_firestore import FirestoreLoaderloader_collection = FirestoreLoader(""Collection"")loader_subcollection = FirestoreLoader(""Collection/doc/SubCollection"")data_collection = loader_collection.load()data_subcollection = loader_subcollection.load()Load a single Document‚Äãfrom google.cloud import firestoreclient = firestore.Client()doc_ref = client.collection(""foo"").document(""bar"")loader_document = FirestoreLoader(doc_ref)data = loader_document.load()Load from CollectionGroup or Query‚Äãfrom google.cloud.firestore import CollectionGroup, FieldFilter, Querycol_ref = client.collection(""col_group"")collection_group = CollectionGroup(col_ref)loader_group = FirestoreLoader(collection_group)col_ref = client.collection(""collection"")query = col_ref.where(filter=FieldFilter(""region"", ""=="", ""west_coast""))loader_query = FirestoreLoader(query)Delete documents‚ÄãDelete a list of langchain documents from Firestore collection with\nFirestoreSaver.delete_documents(<documents>).If document ids is provided, the Documents will be ignored.saver = FirestoreSaver()saver.delete_documents(data)# The Documents will be ignored and only the document ids will be used.saver.delete_documents(data, doc_ids)Advanced Usage‚ÄãLoad documents with customize document page content & metadata‚ÄãThe arguments of page_content_fields and metadata_fields will\nspecify the Firestore Document fields to be written into LangChain\nDocument page_content and metadata.loader = FirestoreLoader(    source=""foo/bar/subcol"",    page_content_fields=[""data_field""],    metadata_fields=[""metadata_field""],)data = loader.load()Customize Page Content Format‚ÄãWhen the page_content contains only one field the information will be\nthe field value only. Otherwise the page_content will be in JSON\nformat.Customize Connection & Authentication‚Äãfrom google.auth import compute_enginefrom google.cloud.firestore import Clientclient = Client(database=""non-default-db"", creds=compute_engine.Credentials())loader = FirestoreLoader(    source=""foo"",    client=client,)",en,
https://python.langchain.com/docs/integrations/document_loaders/joplin,Joplin | ü¶úÔ∏èüîó Langchain,Joplin is an open-source note-taking app.,"JoplinJoplin is an open-source note-taking app.\nCapture your thoughts and securely access them from any device.This notebook covers how to load documents from a Joplin database.Joplin has a REST\nAPI for accessing its\nlocal database. This loader uses the API to retrieve all notes in the\ndatabase and their metadata. This requires an access token that can be\nobtained from the app by following these steps:Open the Joplin app. The app must stay open while the documents\nare being loaded.Go to settings / options and select ‚ÄúWeb Clipper‚Äù.Make sure that the Web Clipper service is enabled.Under ‚ÄúAdvanced Options‚Äù, copy the authorization token.You may either initialize the loader directly with the access token, or\nstore it in the environment variable JOPLIN_ACCESS_TOKEN.An alternative to this approach is to export the Joplin‚Äôs note\ndatabase to Markdown files (optionally, with Front Matter metadata) and\nuse a Markdown loader, such as ObsidianLoader, to load them.from langchain_community.document_loaders import JoplinLoaderloader = JoplinLoader(access_token=""<access-token>"")docs = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/iugu,Iugu | ü¶úÔ∏èüîó Langchain,Iugu is a Brazilian services and software as,"IuguIugu is a Brazilian services and software as\na service (SaaS) company. It offers payment-processing software and\napplication programming interfaces for e-commerce websites and mobile\napplications.This notebook covers how to load data from the Iugu REST API into a\nformat that can be ingested into LangChain, along with example usage for\nvectorization.from langchain.indexes import VectorstoreIndexCreatorfrom langchain_community.document_loaders import IuguLoaderThe Iugu API requires an access token, which can be found inside of the\nIugu dashboard.This document loader also requires a resource option which defines\nwhat data you want to load.Following resources are available:Documentation\nDocumentationiugu_loader = IuguLoader(""charges"")# Create a vectorstore retriever from the loader# see https://python.langchain.com/en/latest/modules/data_connection/getting_started.html for more detailsindex = VectorstoreIndexCreator().from_loaders([iugu_loader])iugu_doc_retriever = index.vectorstore.as_retriever()",en,
https://python.langchain.com/docs/integrations/document_loaders/grobid,Grobid | ü¶úÔ∏èüîó Langchain,"GROBID is a machine learning library for extracting, parsing, and","GrobidGROBID is a machine learning library for extracting, parsing, and\nre-structuring raw documents.It is designed and expected to be used to parse academic papers, where\nit works particularly well. Note: if the articles supplied to Grobid are\nlarge documents (e.g.¬†dissertations) exceeding a certain number of\nelements, they might not be processed.This loader uses Grobid to parse PDFs into Documents that retain\nmetadata associated with the section of text.The best approach is to install Grobid via docker, see\nhttps://grobid.readthedocs.io/en/latest/Grobid-docker/.(Note: additional instructions can be found\nhere.)Once grobid is up-and-running you can interact as described below.Now, we can use the data loader.from langchain_community.document_loaders.generic import GenericLoaderfrom langchain_community.document_loaders.parsers import GrobidParserloader = GenericLoader.from_filesystem(    ""../Papers/"",    glob=""*"",    suffixes=["".pdf""],    parser=GrobidParser(segment_sentences=False),)docs = loader.load()docs[3].page_content'Unlike Chinchilla, PaLM, or GPT-3, we only use publicly available data, making our work compatible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g.""Books -2TB"" or ""Social media conversations"").There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are competitive with PaLM-62B or Chinchilla.'docs[3].metadata{'text': 'Unlike Chinchilla, PaLM, or GPT-3, we only use publicly available data, making our work compatible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g.""Books -2TB"" or ""Social media conversations"").There exist some exceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are competitive with PaLM-62B or Chinchilla.', 'para': '2', 'bboxes': ""[[{'page': '1', 'x': '317.05', 'y': '509.17', 'h': '207.73', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '522.72', 'h': '220.08', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '536.27', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '549.82', 'h': '218.65', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '563.37', 'h': '136.98', 'w': '9.46'}], [{'page': '1', 'x': '446.49', 'y': '563.37', 'h': '78.11', 'w': '9.46'}, {'page': '1', 'x': '304.69', 'y': '576.92', 'h': '138.32', 'w': '9.46'}], [{'page': '1', 'x': '447.75', 'y': '576.92', 'h': '76.66', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '590.47', 'h': '219.63', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '604.02', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '617.56', 'h': '218.27', 'w': '9.46'}, {'page': '1', 'x': '306.14', 'y': '631.11', 'h': '220.18', 'w': '9.46'}]]"", 'pages': ""('1', '1')"", 'section_title': 'Introduction', 'section_number': '1', 'paper_title': 'LLaMA: Open and Efficient Foundation Language Models', 'file_path': '/Users/31treehaus/Desktop/Papers/2302.13971.pdf'}",en,
https://python.langchain.com/docs/integrations/document_loaders/microsoft_word,Microsoft Word | ü¶úÔ∏èüîó Langchain,Microsoft Word,"Microsoft WordMicrosoft Word\nis a word processor developed by Microsoft.This covers how to load Word documents into a document format that we\ncan use downstream.Using Docx2txt‚ÄãLoad .docx using Docx2txt into a document.%pip install --upgrade --quiet  docx2txtfrom langchain_community.document_loaders import Docx2txtLoaderloader = Docx2txtLoader(""example_data/fake.docx"")data = loader.load()data[Document(page_content='Lorem ipsum dolor sit amet.', metadata={'source': 'example_data/fake.docx'})]Using Unstructured‚Äãfrom langchain_community.document_loaders import UnstructuredWordDocumentLoaderloader = UnstructuredWordDocumentLoader(""example_data/fake.docx"")data = loader.load()data[Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': 'fake.docx'}, lookup_index=0)]Retain Elements‚ÄãUnder the hood, Unstructured creates different ‚Äúelements‚Äù for different\nchunks of text. By default we combine those together, but you can easily\nkeep that separation by specifying mode=""elements"".loader = UnstructuredWordDocumentLoader(""example_data/fake.docx"", mode=""elements"")data = loader.load()data[0]Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': 'fake.docx', 'filename': 'fake.docx', 'category': 'Title'}, lookup_index=0)",en,
https://python.langchain.com/docs/integrations/document_loaders/async_chromium,Async Chromium | ü¶úÔ∏èüîó Langchain,"Chromium is one of the browsers supported by Playwright, a library used","Async ChromiumChromium is one of the browsers supported by Playwright, a library used\nto control browser automation.By running p.chromium.launch(headless=True), we are launching a\nheadless instance of Chromium.Headless mode means that the browser is running without a graphical user\ninterface.AsyncChromiumLoader load the page, and then we use\nHtml2TextTransformer to trasnform to text.%pip install --upgrade --quiet  playwright beautifulsoup4! playwright installfrom langchain_community.document_loaders import AsyncChromiumLoaderurls = [""https://www.wsj.com""]loader = AsyncChromiumLoader(urls)docs = loader.load()docs[0].page_content[0:100]'<!DOCTYPE html><html lang=""en""><head><script src=""https://s0.2mdn.net/instream/video/client.js"" asyn'from langchain_community.document_transformers import Html2TextTransformerhtml2text = Html2TextTransformer()docs_transformed = html2text.transform_documents(docs)docs_transformed[0].page_content[0:500]""Skip to Main ContentSkip to SearchSkip to... Select * Top News * What's News *\nFeatured Stories * Retirement * Life & Arts * Hip-Hop * Sports * Video *\nEconomy * Real Estate * Sports * CMO * CIO * CFO * Risk & Compliance *\nLogistics Report * Sustainable Business * Heard on the Street * Barron‚Äôs *\nMarketWatch * Mansion Global * Penta * Opinion * Journal Reports * Sponsored\nOffers Explore Our Brands * WSJ * * * * * Barron's * * * * * MarketWatch * * *\n* * IBD # The Wall Street Journal SubscribeSig""",en,
https://python.langchain.com/docs/integrations/document_loaders/etherscan,Etherscan | ü¶úÔ∏èüîó Langchain,Etherscan is the leading blockchain,"EtherscanEtherscan is the leading blockchain\nexplorer, search, API and analytics platform for Ethereum, a\ndecentralized smart contracts platform.Overview‚ÄãThe Etherscan loader use Etherscan API to load transactions\nhistories under specific account on Ethereum Mainnet.You will need a Etherscan api key to proceed. The free api key has 5\ncalls per seconds quota.The loader supports the following six functionalities:Retrieve normal transactions under specific account on Ethereum\nMainetRetrieve internal transactions under specific account on Ethereum\nMainetRetrieve erc20 transactions under specific account on Ethereum\nMainetRetrieve erc721 transactions under specific account on Ethereum\nMainetRetrieve erc1155 transactions under specific account on Ethereum\nMainetRetrieve ethereum balance in wei under specific account on Ethereum\nMainetIf the account does not have corresponding transactions, the loader will\na list with one document. The content of document is ‚Äô‚Äô.You can pass different filters to loader to access different\nfunctionalities we mentioned above:‚Äúnormal_transaction‚Äù‚Äúinternal_transaction‚Äù‚Äúerc20_transaction‚Äù‚Äúeth_balance‚Äù‚Äúerc721_transaction‚Äù‚Äúerc1155_transaction‚Äù The filter is default to normal_transactionIf you have any questions, you can access Etherscan API\nDoc\nor contact me via i@inevitable.tech.All functions related to transactions histories are restricted 1000\nhistories maximum because of Etherscan limit. You can use the following\nparameters to find the transaction histories you need:offset: default to 20. Shows 20 transactions for one timepage: default to 1. This controls pagination.start_block: Default to 0. The transaction histories starts from 0\nblock.end_block: Default to 99999999. The transaction histories starts\nfrom 99999999 blocksort: ‚Äúdesc‚Äù or ‚Äúasc‚Äù. Set default to ‚Äúdesc‚Äù to get latest\ntransactions.Setup‚Äã%pip install --upgrade --quiet  langchain -qetherscanAPIKey = ""...""import osfrom langchain_community.document_loaders import EtherscanLoaderos.environ[""ETHERSCAN_API_KEY""] = etherscanAPIKeyCreate a ERC20 transaction loader‚Äãaccount_address = ""0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b""loader = EtherscanLoader(account_address, filter=""erc20_transaction"")result = loader.load()eval(result[0].page_content){'blockNumber': '13242975', 'timeStamp': '1631878751', 'hash': '0x366dda325b1a6570928873665b6b418874a7dedf7fee9426158fa3536b621788', 'nonce': '28', 'blockHash': '0x5469dba1b1e1372962cf2be27ab2640701f88c00640c4d26b8cc2ae9ac256fb6', 'from': '0x2ceee24f8d03fc25648c68c8e6569aa0512f6ac3', 'contractAddress': '0x2ceee24f8d03fc25648c68c8e6569aa0512f6ac3', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '298131000000000', 'tokenName': 'ABCHANGE.io', 'tokenSymbol': 'XCH', 'tokenDecimal': '9', 'transactionIndex': '71', 'gas': '15000000', 'gasPrice': '48614996176', 'gasUsed': '5712724', 'cumulativeGasUsed': '11507920', 'input': 'deprecated', 'confirmations': '4492277'}Create a normal transaction loader with customized parameters‚Äãloader = EtherscanLoader(    account_address,    page=2,    offset=20,    start_block=10000,    end_block=8888888888,    sort=""asc"",)result = loader.load()result20[Document(page_content=""{'blockNumber': '1723771', 'timeStamp': '1466213371', 'hash': '0xe00abf5fa83a4b23ee1cc7f07f9dda04ab5fa5efe358b315df8b76699a83efc4', 'nonce': '3155', 'blockHash': '0xc2c2207bcaf341eed07f984c9a90b3f8e8bdbdbd2ac6562f8c2f5bfa4b51299d', 'transactionIndex': '5', 'from': '0x3763e6e1228bfeab94191c856412d1bb0a8e6996', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '13149213761000000000', 'gas': '90000', 'gasPrice': '22655598156', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '126000', 'gasUsed': '21000', 'confirmations': '16011481', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x3763e6e1228bfeab94191c856412d1bb0a8e6996', 'tx_hash': '0xe00abf5fa83a4b23ee1cc7f07f9dda04ab5fa5efe358b315df8b76699a83efc4', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1727090', 'timeStamp': '1466262018', 'hash': '0xd5a779346d499aa722f72ffe7cd3c8594a9ddd91eb7e439e8ba92ceb7bc86928', 'nonce': '3267', 'blockHash': '0xc0cff378c3446b9b22d217c2c5f54b1c85b89a632c69c55b76cdffe88d2b9f4d', 'transactionIndex': '20', 'from': '0x3763e6e1228bfeab94191c856412d1bb0a8e6996', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '11521979886000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '3806725', 'gasUsed': '21000', 'confirmations': '16008162', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x3763e6e1228bfeab94191c856412d1bb0a8e6996', 'tx_hash': '0xd5a779346d499aa722f72ffe7cd3c8594a9ddd91eb7e439e8ba92ceb7bc86928', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1730337', 'timeStamp': '1466308222', 'hash': '0xceaffdb3766d2741057d402738eb41e1d1941939d9d438c102fb981fd47a87a4', 'nonce': '3344', 'blockHash': '0x3a52d28b8587d55c621144a161a0ad5c37dd9f7d63b629ab31da04fa410b2cfa', 'transactionIndex': '1', 'from': '0x3763e6e1228bfeab94191c856412d1bb0a8e6996', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '9783400526000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '60788', 'gasUsed': '21000', 'confirmations': '16004915', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x3763e6e1228bfeab94191c856412d1bb0a8e6996', 'tx_hash': '0xceaffdb3766d2741057d402738eb41e1d1941939d9d438c102fb981fd47a87a4', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1733479', 'timeStamp': '1466352351', 'hash': '0x720d79bf78775f82b40280aae5abfc347643c5f6708d4bf4ec24d65cd01c7121', 'nonce': '3367', 'blockHash': '0x9928661e7ae125b3ae0bcf5e076555a3ee44c52ae31bd6864c9c93a6ebb3f43e', 'transactionIndex': '0', 'from': '0x3763e6e1228bfeab94191c856412d1bb0a8e6996', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '1570706444000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '21000', 'gasUsed': '21000', 'confirmations': '16001773', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x3763e6e1228bfeab94191c856412d1bb0a8e6996', 'tx_hash': '0x720d79bf78775f82b40280aae5abfc347643c5f6708d4bf4ec24d65cd01c7121', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1734172', 'timeStamp': '1466362463', 'hash': '0x7a062d25b83bafc9fe6b22bc6f5718bca333908b148676e1ac66c0adeccef647', 'nonce': '1016', 'blockHash': '0x8a8afe2b446713db88218553cfb5dd202422928e5e0bc00475ed2f37d95649de', 'transactionIndex': '4', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '6322276709000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '105333', 'gasUsed': '21000', 'confirmations': '16001080', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0x7a062d25b83bafc9fe6b22bc6f5718bca333908b148676e1ac66c0adeccef647', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1737276', 'timeStamp': '1466406037', 'hash': '0xa4e89bfaf075abbf48f96700979e6c7e11a776b9040113ba64ef9c29ac62b19b', 'nonce': '1024', 'blockHash': '0xe117cad73752bb485c3bef24556e45b7766b283229180fcabc9711f3524b9f79', 'transactionIndex': '35', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '9976891868000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '3187163', 'gasUsed': '21000', 'confirmations': '15997976', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0xa4e89bfaf075abbf48f96700979e6c7e11a776b9040113ba64ef9c29ac62b19b', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1740314', 'timeStamp': '1466450262', 'hash': '0x6e1a22dcc6e2c77a9451426fb49e765c3c459dae88350e3ca504f4831ec20e8a', 'nonce': '1051', 'blockHash': '0x588d17842819a81afae3ac6644d8005c12ce55ddb66c8d4c202caa91d4e8fdbe', 'transactionIndex': '6', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '8060633765000000000', 'gas': '90000', 'gasPrice': '22926905859', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '153077', 'gasUsed': '21000', 'confirmations': '15994938', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0x6e1a22dcc6e2c77a9451426fb49e765c3c459dae88350e3ca504f4831ec20e8a', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1743384', 'timeStamp': '1466494099', 'hash': '0xdbfcc15f02269fc3ae27f69e344a1ac4e08948b12b76ebdd78a64d8cafd511ef', 'nonce': '1068', 'blockHash': '0x997245108c84250057fda27306b53f9438ad40978a95ca51d8fd7477e73fbaa7', 'transactionIndex': '2', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '9541921352000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '119650', 'gasUsed': '21000', 'confirmations': '15991868', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0xdbfcc15f02269fc3ae27f69e344a1ac4e08948b12b76ebdd78a64d8cafd511ef', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1746405', 'timeStamp': '1466538123', 'hash': '0xbd4f9602f7fff4b8cc2ab6286efdb85f97fa114a43f6df4e6abc88e85b89e97b', 'nonce': '1092', 'blockHash': '0x3af3966cdaf22e8b112792ee2e0edd21ceb5a0e7bf9d8c168a40cf22deb3690c', 'transactionIndex': '0', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '8433783799000000000', 'gas': '90000', 'gasPrice': '25689279306', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '21000', 'gasUsed': '21000', 'confirmations': '15988847', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0xbd4f9602f7fff4b8cc2ab6286efdb85f97fa114a43f6df4e6abc88e85b89e97b', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1749459', 'timeStamp': '1466582044', 'hash': '0x28c327f462cc5013d81c8682c032f014083c6891938a7bdeee85a1c02c3e9ed4', 'nonce': '1096', 'blockHash': '0x5fc5d2a903977b35ce1239975ae23f9157d45d7bd8a8f6205e8ce270000797f9', 'transactionIndex': '1', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '10269065805000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '42000', 'gasUsed': '21000', 'confirmations': '15985793', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0x28c327f462cc5013d81c8682c032f014083c6891938a7bdeee85a1c02c3e9ed4', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1752614', 'timeStamp': '1466626168', 'hash': '0xc3849e550ca5276d7b3c51fa95ad3ae62c1c164799d33f4388fe60c4e1d4f7d8', 'nonce': '1118', 'blockHash': '0x88ef054b98e47504332609394e15c0a4467f84042396717af6483f0bcd916127', 'transactionIndex': '11', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '11325836780000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '252000', 'gasUsed': '21000', 'confirmations': '15982638', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0xc3849e550ca5276d7b3c51fa95ad3ae62c1c164799d33f4388fe60c4e1d4f7d8', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1755659', 'timeStamp': '1466669931', 'hash': '0xb9f891b7c3d00fcd64483189890591d2b7b910eda6172e3bf3973c5fd3d5a5ae', 'nonce': '1133', 'blockHash': '0x2983972217a91343860415d1744c2a55246a297c4810908bbd3184785bc9b0c2', 'transactionIndex': '14', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '13226475343000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '2674679', 'gasUsed': '21000', 'confirmations': '15979593', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0xb9f891b7c3d00fcd64483189890591d2b7b910eda6172e3bf3973c5fd3d5a5ae', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1758709', 'timeStamp': '1466713652', 'hash': '0xd6cce5b184dc7fce85f305ee832df647a9c4640b68e9b79b6f74dc38336d5622', 'nonce': '1147', 'blockHash': '0x1660de1e73067251be0109d267a21ffc7d5bde21719a3664c7045c32e771ecf9', 'transactionIndex': '1', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '9758447294000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '42000', 'gasUsed': '21000', 'confirmations': '15976543', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0xd6cce5b184dc7fce85f305ee832df647a9c4640b68e9b79b6f74dc38336d5622', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1761783', 'timeStamp': '1466757809', 'hash': '0xd01545872629956867cbd65fdf5e97d0dde1a112c12e76a1bfc92048d37f650f', 'nonce': '1169', 'blockHash': '0x7576961afa4218a3264addd37a41f55c444dd534e9410dbd6f93f7fe20e0363e', 'transactionIndex': '2', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '10197126683000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '63000', 'gasUsed': '21000', 'confirmations': '15973469', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0xd01545872629956867cbd65fdf5e97d0dde1a112c12e76a1bfc92048d37f650f', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1764895', 'timeStamp': '1466801683', 'hash': '0x620b91b12af7aac75553b47f15742e2825ea38919cfc8082c0666f404a0db28b', 'nonce': '1186', 'blockHash': '0x2e687643becd3c36e0c396a02af0842775e17ccefa0904de5aeca0a9a1aa795e', 'transactionIndex': '7', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '8690241462000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '168000', 'gasUsed': '21000', 'confirmations': '15970357', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0x620b91b12af7aac75553b47f15742e2825ea38919cfc8082c0666f404a0db28b', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1767936', 'timeStamp': '1466845682', 'hash': '0x758efa27576cd17ebe7b842db4892eac6609e3962a4f9f57b7c84b7b1909512f', 'nonce': '1211', 'blockHash': '0xb01d8fd47b3554a99352ac3e5baf5524f314cfbc4262afcfbea1467b2d682898', 'transactionIndex': '0', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '11914401843000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '21000', 'gasUsed': '21000', 'confirmations': '15967316', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0x758efa27576cd17ebe7b842db4892eac6609e3962a4f9f57b7c84b7b1909512f', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1770911', 'timeStamp': '1466888890', 'hash': '0x9d84470b54ab44b9074b108a0e506cd8badf30457d221e595bb68d63e926b865', 'nonce': '1212', 'blockHash': '0x79a9de39276132dab8bf00dc3e060f0e8a14f5e16a0ee4e9cc491da31b25fe58', 'transactionIndex': '0', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '10918214730000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '21000', 'gasUsed': '21000', 'confirmations': '15964341', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0x9d84470b54ab44b9074b108a0e506cd8badf30457d221e595bb68d63e926b865', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1774044', 'timeStamp': '1466932983', 'hash': '0x958d85270b58b80f1ad228f716bbac8dd9da7c5f239e9f30d8edeb5bb9301d20', 'nonce': '1240', 'blockHash': '0x69cee390378c3b886f9543fb3a1cb2fc97621ec155f7884564d4c866348ce539', 'transactionIndex': '2', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '9979637283000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '63000', 'gasUsed': '21000', 'confirmations': '15961208', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0x958d85270b58b80f1ad228f716bbac8dd9da7c5f239e9f30d8edeb5bb9301d20', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1777057', 'timeStamp': '1466976422', 'hash': '0xe76ca3603d2f4e7134bdd7a1c3fd553025fc0b793f3fd2a75cd206b8049e74ab', 'nonce': '1248', 'blockHash': '0xc7cacda0ac38c99f1b9bccbeee1562a41781d2cfaa357e8c7b4af6a49584b968', 'transactionIndex': '7', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '4556173496000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '168000', 'gasUsed': '21000', 'confirmations': '15958195', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0xe76ca3603d2f4e7134bdd7a1c3fd553025fc0b793f3fd2a75cd206b8049e74ab', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'}), Document(page_content=""{'blockNumber': '1780120', 'timeStamp': '1467020353', 'hash': '0xc5ec8cecdc9f5ed55a5b8b0ad79c964fb5c49dc1136b6a49e981616c3e70bbe6', 'nonce': '1266', 'blockHash': '0xfc0e066e5b613239e1a01e6d582e7ab162ceb3ca4f719dfbd1a0c965adcfe1c5', 'transactionIndex': '1', 'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b', 'value': '11890330240000000000', 'gas': '90000', 'gasPrice': '20000000000', 'isError': '0', 'txreceipt_status': '', 'input': '0x', 'contractAddress': '', 'cumulativeGasUsed': '42000', 'gasUsed': '21000', 'confirmations': '15955132', 'methodId': '0x', 'functionName': ''}"", metadata={'from': '0x16545fb79dbee1ad3a7f868b7661c023f372d5de', 'tx_hash': '0xc5ec8cecdc9f5ed55a5b8b0ad79c964fb5c49dc1136b6a49e981616c3e70bbe6', 'to': '0x9dd134d14d1e65f84b706d6f205cd5b1cd03a46b'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/pandas_dataframe,Pandas DataFrame | ü¶úÔ∏èüîó Langchain,This notebook goes over how to load data from a,"Pandas DataFrameThis notebook goes over how to load data from a\npandas\nDataFrame.%pip install --upgrade --quiet  pandasimport pandas as pddf = pd.read_csv(""example_data/mlb_teams_2012.csv"")df.head()\n\nTeam""Payroll (millions)""""Wins""0Nationals81.34981Reds82.20972Yankees197.96953Giants117.62944Braves83.3194\nfrom langchain_community.document_loaders import DataFrameLoaderloader = DataFrameLoader(df, page_content_column=""Team"")loader.load()[Document(page_content='Nationals', metadata={' ""Payroll (millions)""': 81.34, ' ""Wins""': 98}), Document(page_content='Reds', metadata={' ""Payroll (millions)""': 82.2, ' ""Wins""': 97}), Document(page_content='Yankees', metadata={' ""Payroll (millions)""': 197.96, ' ""Wins""': 95}), Document(page_content='Giants', metadata={' ""Payroll (millions)""': 117.62, ' ""Wins""': 94}), Document(page_content='Braves', metadata={' ""Payroll (millions)""': 83.31, ' ""Wins""': 94}), Document(page_content='Athletics', metadata={' ""Payroll (millions)""': 55.37, ' ""Wins""': 94}), Document(page_content='Rangers', metadata={' ""Payroll (millions)""': 120.51, ' ""Wins""': 93}), Document(page_content='Orioles', metadata={' ""Payroll (millions)""': 81.43, ' ""Wins""': 93}), Document(page_content='Rays', metadata={' ""Payroll (millions)""': 64.17, ' ""Wins""': 90}), Document(page_content='Angels', metadata={' ""Payroll (millions)""': 154.49, ' ""Wins""': 89}), Document(page_content='Tigers', metadata={' ""Payroll (millions)""': 132.3, ' ""Wins""': 88}), Document(page_content='Cardinals', metadata={' ""Payroll (millions)""': 110.3, ' ""Wins""': 88}), Document(page_content='Dodgers', metadata={' ""Payroll (millions)""': 95.14, ' ""Wins""': 86}), Document(page_content='White Sox', metadata={' ""Payroll (millions)""': 96.92, ' ""Wins""': 85}), Document(page_content='Brewers', metadata={' ""Payroll (millions)""': 97.65, ' ""Wins""': 83}), Document(page_content='Phillies', metadata={' ""Payroll (millions)""': 174.54, ' ""Wins""': 81}), Document(page_content='Diamondbacks', metadata={' ""Payroll (millions)""': 74.28, ' ""Wins""': 81}), Document(page_content='Pirates', metadata={' ""Payroll (millions)""': 63.43, ' ""Wins""': 79}), Document(page_content='Padres', metadata={' ""Payroll (millions)""': 55.24, ' ""Wins""': 76}), Document(page_content='Mariners', metadata={' ""Payroll (millions)""': 81.97, ' ""Wins""': 75}), Document(page_content='Mets', metadata={' ""Payroll (millions)""': 93.35, ' ""Wins""': 74}), Document(page_content='Blue Jays', metadata={' ""Payroll (millions)""': 75.48, ' ""Wins""': 73}), Document(page_content='Royals', metadata={' ""Payroll (millions)""': 60.91, ' ""Wins""': 72}), Document(page_content='Marlins', metadata={' ""Payroll (millions)""': 118.07, ' ""Wins""': 69}), Document(page_content='Red Sox', metadata={' ""Payroll (millions)""': 173.18, ' ""Wins""': 69}), Document(page_content='Indians', metadata={' ""Payroll (millions)""': 78.43, ' ""Wins""': 68}), Document(page_content='Twins', metadata={' ""Payroll (millions)""': 94.08, ' ""Wins""': 66}), Document(page_content='Rockies', metadata={' ""Payroll (millions)""': 78.06, ' ""Wins""': 64}), Document(page_content='Cubs', metadata={' ""Payroll (millions)""': 88.19, ' ""Wins""': 61}), Document(page_content='Astros', metadata={' ""Payroll (millions)""': 60.65, ' ""Wins""': 55})]# Use lazy load for larger table, which won't read the full table into memoryfor i in loader.lazy_load():    print(i)page_content='Nationals' metadata={' ""Payroll (millions)""': 81.34, ' ""Wins""': 98}page_content='Reds' metadata={' ""Payroll (millions)""': 82.2, ' ""Wins""': 97}page_content='Yankees' metadata={' ""Payroll (millions)""': 197.96, ' ""Wins""': 95}page_content='Giants' metadata={' ""Payroll (millions)""': 117.62, ' ""Wins""': 94}page_content='Braves' metadata={' ""Payroll (millions)""': 83.31, ' ""Wins""': 94}page_content='Athletics' metadata={' ""Payroll (millions)""': 55.37, ' ""Wins""': 94}page_content='Rangers' metadata={' ""Payroll (millions)""': 120.51, ' ""Wins""': 93}page_content='Orioles' metadata={' ""Payroll (millions)""': 81.43, ' ""Wins""': 93}page_content='Rays' metadata={' ""Payroll (millions)""': 64.17, ' ""Wins""': 90}page_content='Angels' metadata={' ""Payroll (millions)""': 154.49, ' ""Wins""': 89}page_content='Tigers' metadata={' ""Payroll (millions)""': 132.3, ' ""Wins""': 88}page_content='Cardinals' metadata={' ""Payroll (millions)""': 110.3, ' ""Wins""': 88}page_content='Dodgers' metadata={' ""Payroll (millions)""': 95.14, ' ""Wins""': 86}page_content='White Sox' metadata={' ""Payroll (millions)""': 96.92, ' ""Wins""': 85}page_content='Brewers' metadata={' ""Payroll (millions)""': 97.65, ' ""Wins""': 83}page_content='Phillies' metadata={' ""Payroll (millions)""': 174.54, ' ""Wins""': 81}page_content='Diamondbacks' metadata={' ""Payroll (millions)""': 74.28, ' ""Wins""': 81}page_content='Pirates' metadata={' ""Payroll (millions)""': 63.43, ' ""Wins""': 79}page_content='Padres' metadata={' ""Payroll (millions)""': 55.24, ' ""Wins""': 76}page_content='Mariners' metadata={' ""Payroll (millions)""': 81.97, ' ""Wins""': 75}page_content='Mets' metadata={' ""Payroll (millions)""': 93.35, ' ""Wins""': 74}page_content='Blue Jays' metadata={' ""Payroll (millions)""': 75.48, ' ""Wins""': 73}page_content='Royals' metadata={' ""Payroll (millions)""': 60.91, ' ""Wins""': 72}page_content='Marlins' metadata={' ""Payroll (millions)""': 118.07, ' ""Wins""': 69}page_content='Red Sox' metadata={' ""Payroll (millions)""': 173.18, ' ""Wins""': 69}page_content='Indians' metadata={' ""Payroll (millions)""': 78.43, ' ""Wins""': 68}page_content='Twins' metadata={' ""Payroll (millions)""': 94.08, ' ""Wins""': 66}page_content='Rockies' metadata={' ""Payroll (millions)""': 78.06, ' ""Wins""': 64}page_content='Cubs' metadata={' ""Payroll (millions)""': 88.19, ' ""Wins""': 61}page_content='Astros' metadata={' ""Payroll (millions)""': 60.65, ' ""Wins""': 55}",en,
https://python.langchain.com/docs/integrations/document_loaders/azure_document_intelligence,Azure AI Document Intelligence | ü¶úÔ∏èüîó Langchain,Azure AI Document Intelligence,"Azure AI Document IntelligenceAzure AI Document Intelligence\n(formerly known as Azure Form Recognizer) is machine-learning based\nservice that extracts text (including handwriting), tables or\nkey-value-pairs from scanned documents or images.Document Intelligence supports PDF, JPEG, PNG, BMP, or TIFF.This current implementation of a loader using Document Intelligence\ncan incorporate content page-wise and turn it into LangChain documents.%pip install --upgrade --quiet  langchain langchain-community azure-ai-documentintelligence -q[notice] A new release of pip is available: 23.3.1 -> 23.3.2[notice] To update, run: python3 -m pip install --upgrade pipNote: you may need to restart the kernel to use updated packages.Example 1‚ÄãThe first example uses a local file which will be sent to Azure AI\nDocument Intelligence.With the initialized document analysis client, we can proceed to create\nan instance of the DocumentIntelligenceLoader:from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoaderfile_path = ""<filepath>""endpoint = ""<endpoint>""key = ""<key>""loader = AzureAIDocumentIntelligenceLoader(    api_endpoint=endpoint, api_key=key, file_path=file_path, api_model=""prebuilt-layout"")documents = loader.load()The default output contains one LangChain document with markdown format\ncontent:documentsExample 2‚ÄãThe input file can also be URL path.url_path = ""<url>""loader = AzureAIDocumentIntelligenceLoader(    api_endpoint=endpoint, api_key=key, url_path=url_path, api_model=""prebuilt-layout"")documents = loader.load()documents",en,
https://python.langchain.com/docs/integrations/document_loaders/image,Images | ü¶úÔ∏èüîó Langchain,This covers how to load images such as JPG or PNG into a document,"ImagesThis covers how to load images such as JPG or PNG into a document\nformat that we can use downstream.Using Unstructured‚Äã%pip install --upgrade --quiet  pdfminerfrom langchain_community.document_loaders.image import UnstructuredImageLoaderloader = UnstructuredImageLoader(""layout-parser-paper-fast.jpg"")data = loader.load()data[0]Document(page_content=""LayoutParser: A Unified Toolkit for Deep\nLearning Based Document Image Analysis\n\n\n‚ÄòZxjiang Shen' (F3}, Ruochen Zhang‚Äù, Melissa Dell*, Benjamin Charles Germain\nLeet, Jacob Carlson, and Weining LiF\n\n\nsugehen\n\nshangthrows, et\n\n‚ÄúAbstract. Recent advanocs in document image analysis (DIA) have been\n‚Äòpimarliy driven bythe application of neural networks dell roar\n{uteomer could be aly deployed in production and extended fo farther\n[nvetigtion. However, various factory ke lcely organize codebanee\nsnd sophisticated modal cnigurations compat the ey ree of\n‚Äòerin! innovation by wide sence, Though there have been sng\n‚ÄòHors to improve reuablty and simplify deep lees (DL) mode\n‚Äòaon, sone of them ae optimized for challenge inthe demain of DIA,\nThis roprscte a major gap in the extng fol, sw DIA i eal to\nscademic research acon wie range of dpi in the social ssencee\n[rary for streamlining the sage of DL in DIA research and appicn\n‚Äòtons The core LayoutFaraer brary comes with a sch of simple and\nIntative interfaee or applying and eutomiing DI. odel fr Inyo de\npltfom for sharing both protrined modes an fal document dist\n{ation pipeline We demonutate that LayootPareer shea fr both\nlightweight and lrgeseledgtieation pipelines in eal-word uae ces\nThe leary pblely smal at Btspe://layost-pareergsthab So\n\n\n\n‚ÄòKeywords: Document Image Analysis¬ª Deep Learning Layout Analysis\n‚ÄòCharacter Renguition - Open Serres dary ¬´ Tol\n\n\nIntroduction\n\n\n‚ÄòDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\ndoctiment image analysis (DIA) tea including document image clasiffeation [I]\n"", lookup_str='', metadata={'source': 'layout-parser-paper-fast.jpg'}, lookup_index=0)Retain Elements‚ÄãUnder the hood, Unstructured creates different ‚Äúelements‚Äù for different\nchunks of text. By default we combine those together, but you can easily\nkeep that separation by specifying mode=""elements"".loader = UnstructuredImageLoader(""layout-parser-paper-fast.jpg"", mode=""elements"")data = loader.load()data[0]Document(page_content='LayoutParser: A Unified Toolkit for Deep\nLearning Based Document Image Analysis\n', lookup_str='', metadata={'source': 'layout-parser-paper-fast.jpg', 'filename': 'layout-parser-paper-fast.jpg', 'page_number': 1, 'category': 'Title'}, lookup_index=0)",en,
https://python.langchain.com/docs/integrations/document_loaders/async_html,AsyncHtml | ü¶úÔ∏èüîó Langchain,AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.,"AsyncHtmlAsyncHtmlLoader loads raw HTML from a list of URLs concurrently.from langchain_community.document_loaders import AsyncHtmlLoaderurls = [""https://www.espn.com"", ""https://lilianweng.github.io/posts/2023-06-23-agent/""]loader = AsyncHtmlLoader(urls)docs = loader.load()Fetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]docs[0].page_content[1000:2000]' news. Stream exclusive games on ESPN+ and play fantasy sports."" />\n<meta property=""og:image"" content=""https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png""/>\n<meta property=""og:image:width"" content=""1200"" />\n<meta property=""og:image:height"" content=""630"" />\n<meta property=""og:type"" content=""website"" />\n<meta name=""twitter:site"" content=""espn"" />\n<meta name=""twitter:url"" content=""https://www.espn.com"" />\n<meta name=""twitter:title"" content=""ESPN - Serving Sports Fans. Anytime. Anywhere.""/>\n<meta name=""twitter:description"" content=""Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports."" />\n<meta name=""twitter:card"" content=""summary"">\n<meta name=""twitter:app:name:iphone"" content=""ESPN""/>\n<meta name=""twitter:app:id:iphone"" content=""317469184""/>\n<meta name=""twitter:app:name:googleplay"" content=""ESPN""/>\n<meta name=""twitter:app:id:googleplay"" content=""com.espn.score_center""/>\n<meta name=""title"" content=""ESPN - 'docs[1].page_content[1000:2000]'al"" href=""https://lilianweng.github.io/posts/2023-06-23-agent/"" />\n<link crossorigin=""anonymous"" href=""/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css"" integrity=""sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs="" rel=""preload stylesheet"" as=""style"">\n<script defer crossorigin=""anonymous"" src=""/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js"" integrity=""sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=""\n    onload=""hljs.initHighlightingOnLoad();""></script>\n<link rel=""icon"" href=""https://lilianweng.github.io/favicon_peach.ico"">\n<link rel=""icon"" type=""image/png"" sizes=""16x16"" href=""https://lilianweng.github.io/favicon-16x16.png"">\n<link rel=""icon"" type=""image/png"" sizes=""32x32"" href=""https://lilianweng.github.io/favicon-32x32.png"">\n<link rel=""apple-touch-icon"" href=""https://lilianweng.github.io/apple-touch-icon.png"">\n<link rel=""mask-icon"" href=""https://lilianweng.github.io/safari-pinned-tab.'",en,
https://python.langchain.com/docs/integrations/document_loaders/airbyte_stripe,Airbyte Stripe (Deprecated) | ü¶úÔ∏èüîó Langchain,Note: This connector-specific loader is deprecated. Please use,"Airbyte Stripe (Deprecated)Note: This connector-specific loader is deprecated. Please use\nAirbyteLoader instead.Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases.This loader exposes the Stripe connector as a document loader, allowing\nyou to load various Stripe objects as documents.Installation‚ÄãFirst, you need to install the airbyte-source-stripe python package.%pip install --upgrade --quiet  airbyte-source-stripeExample‚ÄãCheck out the Airbyte documentation\npage for details\nabout how to configure the reader. The JSON schema the config object\nshould adhere to can be found on Github:\nhttps://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-stripe/source_stripe/spec.yaml.The general shape looks like this:{  ""client_secret"": ""<secret key>"",  ""account_id"": ""<account id>"",  ""start_date"": ""<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>"",}By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader.from langchain_community.document_loaders.airbyte import AirbyteStripeLoaderconfig = {    # your stripe configuration}loader = AirbyteStripeLoader(    config=config, stream_name=""invoices"")  # check the documentation linked above for a list of all streamsNow you can load documents the usual waydocs = loader.load()As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the\nlazy_load method which returns an iterator instead:docs_iterator = loader.lazy_load()Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader:from langchain.docstore.document import Documentdef handle_record(record, id):    return Document(page_content=record.data[""title""], metadata=record.data)loader = AirbyteStripeLoader(    config=config, record_handler=handle_record, stream_name=""invoices"")docs = loader.load()Incremental loads‚ÄãSome streams allow incremental loading, this means the source keeps\ntrack of synced records and won‚Äôt load them again. This is useful for\nsources that have a high volume of data and are updated frequently.To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded.last_state = loader.last_state  # store safelyincremental_loader = AirbyteStripeLoader(    config=config,    record_handler=handle_record,    stream_name=""invoices"",    state=last_state,)new_docs = incremental_loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/readthedocs_documentation,ReadTheDocs Documentation | ü¶úÔ∏èüîó Langchain,Read the Docs is an open-sourced free,"ReadTheDocs DocumentationRead the Docs is an open-sourced free\nsoftware documentation hosting platform. It generates documentation\nwritten with the Sphinx documentation generator.This notebook covers how to load content from HTML that was generated as\npart of a Read-The-Docs build.For an example of this in the wild, see\nhere.This assumes that the HTML has already been scraped into a folder. This\ncan be done by uncommenting and running the following command%pip install --upgrade --quiet  beautifulsoup4#!wget -r -A.html -P rtdocs https://python.langchain.com/en/latest/from langchain_community.document_loaders import ReadTheDocsLoaderloader = ReadTheDocsLoader(""rtdocs"", features=""html.parser"")docs = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/rst,RST | ü¶úÔ∏èüîó Langchain,A [reStructured Text,"RSTA reStructured Text\n(RST) file is a file\nformat for textual data used primarily in the Python programming\nlanguage community for technical documentation.UnstructuredRSTLoader‚ÄãYou can load data from RST files with UnstructuredRSTLoader using the\nfollowing workflow.from langchain_community.document_loaders import UnstructuredRSTLoaderloader = UnstructuredRSTLoader(file_path=""example_data/README.rst"", mode=""elements"")docs = loader.load()print(docs[0])page_content='Example Docs' metadata={'source': 'example_data/README.rst', 'filename': 'README.rst', 'file_directory': 'example_data', 'filetype': 'text/x-rst', 'page_number': 1, 'category': 'Title'}",en,
https://python.langchain.com/docs/integrations/document_loaders/blockchain,Blockchain | ü¶úÔ∏èüîó Langchain,Overview,"BlockchainOverview‚ÄãThe intention of this notebook is to provide a means of testing\nfunctionality in the Langchain Document Loader for Blockchain.Initially this Loader supports:Loading NFTs as Documents from NFT Smart Contracts (ERC721 and\nERC1155)Ethereum Mainnnet, Ethereum Testnet, Polygon Mainnet, Polygon\nTestnet (default is eth-mainnet)Alchemy‚Äôs getNFTsForCollection APIIt can be extended if the community finds value in this loader.\nSpecifically:Additional APIs can be added (e.g.¬†Tranction-related APIs)This Document Loader Requires:A free Alchemy API KeyThe output takes the following format:pageContent= Individual NFTmetadata={‚Äòsource‚Äô: ‚Äò0x1a92f7381b9f03921564a437210bb9396471050c‚Äô,\n‚Äòblockchain‚Äô: ‚Äòeth-mainnet‚Äô, ‚ÄòtokenId‚Äô: ‚Äò0x15‚Äô})Load NFTs into Document Loader‚Äã# get ALCHEMY_API_KEY from https://www.alchemy.com/alchemyApiKey = ""...""Option 1: Ethereum Mainnet (default BlockchainType)‚Äãfrom langchain_community.document_loaders.blockchain import (    BlockchainDocumentLoader,    BlockchainType,)contractAddress = ""0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d""  # Bored Ape Yacht Club contract addressblockchainType = BlockchainType.ETH_MAINNET  # default value, optional parameterblockchainLoader = BlockchainDocumentLoader(    contract_address=contractAddress, api_key=alchemyApiKey)nfts = blockchainLoader.load()nfts[:2]Option 2: Polygon Mainnet‚ÄãcontractAddress = (    ""0x448676ffCd0aDf2D85C1f0565e8dde6924A9A7D9""  # Polygon Mainnet contract address)blockchainType = BlockchainType.POLYGON_MAINNETblockchainLoader = BlockchainDocumentLoader(    contract_address=contractAddress,    blockchainType=blockchainType,    api_key=alchemyApiKey,)nfts = blockchainLoader.load()nfts[:2]",en,
https://python.langchain.com/docs/integrations/document_loaders/xorbits,Xorbits Pandas DataFrame | ü¶úÔ∏èüîó Langchain,This notebook goes over how to load data from a,"Xorbits Pandas DataFrameThis notebook goes over how to load data from a\nxorbits.pandas\nDataFrame.%pip install --upgrade --quiet  xorbitsimport xorbits.pandas as pddf = pd.read_csv(""example_data/mlb_teams_2012.csv"")df.head()  0%|          |   0.00/100 [00:00<?, ?it/s]\n\nTeam""Payroll (millions)""""Wins""0Nationals81.34981Reds82.20972Yankees197.96953Giants117.62944Braves83.3194\nfrom langchain_community.document_loaders import XorbitsLoaderloader = XorbitsLoader(df, page_content_column=""Team"")loader.load()  0%|          |   0.00/100 [00:00<?, ?it/s][Document(page_content='Nationals', metadata={' ""Payroll (millions)""': 81.34, ' ""Wins""': 98}), Document(page_content='Reds', metadata={' ""Payroll (millions)""': 82.2, ' ""Wins""': 97}), Document(page_content='Yankees', metadata={' ""Payroll (millions)""': 197.96, ' ""Wins""': 95}), Document(page_content='Giants', metadata={' ""Payroll (millions)""': 117.62, ' ""Wins""': 94}), Document(page_content='Braves', metadata={' ""Payroll (millions)""': 83.31, ' ""Wins""': 94}), Document(page_content='Athletics', metadata={' ""Payroll (millions)""': 55.37, ' ""Wins""': 94}), Document(page_content='Rangers', metadata={' ""Payroll (millions)""': 120.51, ' ""Wins""': 93}), Document(page_content='Orioles', metadata={' ""Payroll (millions)""': 81.43, ' ""Wins""': 93}), Document(page_content='Rays', metadata={' ""Payroll (millions)""': 64.17, ' ""Wins""': 90}), Document(page_content='Angels', metadata={' ""Payroll (millions)""': 154.49, ' ""Wins""': 89}), Document(page_content='Tigers', metadata={' ""Payroll (millions)""': 132.3, ' ""Wins""': 88}), Document(page_content='Cardinals', metadata={' ""Payroll (millions)""': 110.3, ' ""Wins""': 88}), Document(page_content='Dodgers', metadata={' ""Payroll (millions)""': 95.14, ' ""Wins""': 86}), Document(page_content='White Sox', metadata={' ""Payroll (millions)""': 96.92, ' ""Wins""': 85}), Document(page_content='Brewers', metadata={' ""Payroll (millions)""': 97.65, ' ""Wins""': 83}), Document(page_content='Phillies', metadata={' ""Payroll (millions)""': 174.54, ' ""Wins""': 81}), Document(page_content='Diamondbacks', metadata={' ""Payroll (millions)""': 74.28, ' ""Wins""': 81}), Document(page_content='Pirates', metadata={' ""Payroll (millions)""': 63.43, ' ""Wins""': 79}), Document(page_content='Padres', metadata={' ""Payroll (millions)""': 55.24, ' ""Wins""': 76}), Document(page_content='Mariners', metadata={' ""Payroll (millions)""': 81.97, ' ""Wins""': 75}), Document(page_content='Mets', metadata={' ""Payroll (millions)""': 93.35, ' ""Wins""': 74}), Document(page_content='Blue Jays', metadata={' ""Payroll (millions)""': 75.48, ' ""Wins""': 73}), Document(page_content='Royals', metadata={' ""Payroll (millions)""': 60.91, ' ""Wins""': 72}), Document(page_content='Marlins', metadata={' ""Payroll (millions)""': 118.07, ' ""Wins""': 69}), Document(page_content='Red Sox', metadata={' ""Payroll (millions)""': 173.18, ' ""Wins""': 69}), Document(page_content='Indians', metadata={' ""Payroll (millions)""': 78.43, ' ""Wins""': 68}), Document(page_content='Twins', metadata={' ""Payroll (millions)""': 94.08, ' ""Wins""': 66}), Document(page_content='Rockies', metadata={' ""Payroll (millions)""': 78.06, ' ""Wins""': 64}), Document(page_content='Cubs', metadata={' ""Payroll (millions)""': 88.19, ' ""Wins""': 61}), Document(page_content='Astros', metadata={' ""Payroll (millions)""': 60.65, ' ""Wins""': 55})]# Use lazy load for larger table, which won't read the full table into memoryfor i in loader.lazy_load():    print(i)  0%|          |   0.00/100 [00:00<?, ?it/s]page_content='Nationals' metadata={' ""Payroll (millions)""': 81.34, ' ""Wins""': 98}page_content='Reds' metadata={' ""Payroll (millions)""': 82.2, ' ""Wins""': 97}page_content='Yankees' metadata={' ""Payroll (millions)""': 197.96, ' ""Wins""': 95}page_content='Giants' metadata={' ""Payroll (millions)""': 117.62, ' ""Wins""': 94}page_content='Braves' metadata={' ""Payroll (millions)""': 83.31, ' ""Wins""': 94}page_content='Athletics' metadata={' ""Payroll (millions)""': 55.37, ' ""Wins""': 94}page_content='Rangers' metadata={' ""Payroll (millions)""': 120.51, ' ""Wins""': 93}page_content='Orioles' metadata={' ""Payroll (millions)""': 81.43, ' ""Wins""': 93}page_content='Rays' metadata={' ""Payroll (millions)""': 64.17, ' ""Wins""': 90}page_content='Angels' metadata={' ""Payroll (millions)""': 154.49, ' ""Wins""': 89}page_content='Tigers' metadata={' ""Payroll (millions)""': 132.3, ' ""Wins""': 88}page_content='Cardinals' metadata={' ""Payroll (millions)""': 110.3, ' ""Wins""': 88}page_content='Dodgers' metadata={' ""Payroll (millions)""': 95.14, ' ""Wins""': 86}page_content='White Sox' metadata={' ""Payroll (millions)""': 96.92, ' ""Wins""': 85}page_content='Brewers' metadata={' ""Payroll (millions)""': 97.65, ' ""Wins""': 83}page_content='Phillies' metadata={' ""Payroll (millions)""': 174.54, ' ""Wins""': 81}page_content='Diamondbacks' metadata={' ""Payroll (millions)""': 74.28, ' ""Wins""': 81}page_content='Pirates' metadata={' ""Payroll (millions)""': 63.43, ' ""Wins""': 79}page_content='Padres' metadata={' ""Payroll (millions)""': 55.24, ' ""Wins""': 76}page_content='Mariners' metadata={' ""Payroll (millions)""': 81.97, ' ""Wins""': 75}page_content='Mets' metadata={' ""Payroll (millions)""': 93.35, ' ""Wins""': 74}page_content='Blue Jays' metadata={' ""Payroll (millions)""': 75.48, ' ""Wins""': 73}page_content='Royals' metadata={' ""Payroll (millions)""': 60.91, ' ""Wins""': 72}page_content='Marlins' metadata={' ""Payroll (millions)""': 118.07, ' ""Wins""': 69}page_content='Red Sox' metadata={' ""Payroll (millions)""': 173.18, ' ""Wins""': 69}page_content='Indians' metadata={' ""Payroll (millions)""': 78.43, ' ""Wins""': 68}page_content='Twins' metadata={' ""Payroll (millions)""': 94.08, ' ""Wins""': 66}page_content='Rockies' metadata={' ""Payroll (millions)""': 78.06, ' ""Wins""': 64}page_content='Cubs' metadata={' ""Payroll (millions)""': 88.19, ' ""Wins""': 61}page_content='Astros' metadata={' ""Payroll (millions)""': 60.65, ' ""Wins""': 55}",en,
https://python.langchain.com/docs/integrations/document_loaders/yuque,Yuque | ü¶úÔ∏èüîó Langchain,Yuque is a professional cloud-based,"YuqueYuque is a professional cloud-based\nknowledge base for team collaboration in documentation.This notebook covers how to load documents from Yuque.You can obtain the personal access token by clicking on your personal\navatar in the Personal Settings\npage.from langchain_community.document_loaders import YuqueLoaderloader = YuqueLoader(access_token=""<your_personal_access_token>"")docs = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/ifixit,iFixit | ü¶úÔ∏èüîó Langchain,"iFixit is the largest, open repair community","iFixitiFixit is the largest, open repair community\non the web. The site contains nearly 100k repair manuals, 200k\nQuestions & Answers on 42k devices, and all the data is licensed under\nCC-BY-NC-SA 3.0.This loader will allow you to download the text of a repair guide, text\nof Q&A‚Äôs and wikis from devices on iFixit using their open APIs. It‚Äôs\nincredibly useful for context related to technical documents and answers\nto questions about devices in the corpus of data on iFixit.from langchain_community.document_loaders import IFixitLoaderloader = IFixitLoader(""https://www.ifixit.com/Teardown/Banana+Teardown/811"")data = loader.load()data[Document(page_content=""# Banana Teardown\nIn this teardown, we open a banana to see what's inside.  Yellow and delicious, but most importantly, yellow.\n\n\n###Tools Required:\n\n - Fingers\n\n - Teeth\n\n - Thumbs\n\n\n###Parts Required:\n\n - None\n\n\n## Step 1\nTake one banana from the bunch.\nDon't squeeze too hard!\n\n\n## Step 2\nHold the banana in your left hand and grip the stem between your right thumb and forefinger.\n\n\n## Step 3\nPull the stem downward until the peel splits.\n\n\n## Step 4\nInsert your thumbs into the split of the peel and pull the two sides apart.\nExpose the top of the banana.  It may be slightly squished from pulling on the stem, but this will not affect the flavor.\n\n\n## Step 5\nPull open the peel, starting from your original split, and opening it along the length of the banana.\n\n\n## Step 6\nRemove fruit from peel.\n\n\n## Step 7\nEat and enjoy!\nThis is where you'll need your teeth.\nDo not choke on banana!\n"", lookup_str='', metadata={'source': 'https://www.ifixit.com/Teardown/Banana+Teardown/811', 'title': 'Banana Teardown'}, lookup_index=0)]loader = IFixitLoader(    ""https://www.ifixit.com/Answers/View/318583/My+iPhone+6+is+typing+and+opening+apps+by+itself"")data = loader.load()data[Document(page_content='# My iPhone 6 is typing and opening apps by itself\nmy iphone 6 is typing and opening apps by itself. How do i fix this. I just bought it last week.\nI restored as manufactures cleaned up the screen\nthe problem continues\n\n## 27 Answers\n\nFilter by: \n\nMost Helpful\nNewest\nOldest\n\n### Accepted Answer\nHi,\nWhere did you buy it? If you bought it from Apple or from an official retailer like Carphone warehouse etc. Then you\'ll have a year warranty and can get it replaced free.\nIf you bought it second hand, from a third part repair shop or online, then it may still have warranty, unless it is refurbished and has been repaired elsewhere.\nIf this is the case, it may be the screen that needs replacing to solve your issue.\nEither way, wherever you got it, it\'s best to return it and get a refund or a replacement device. :-)\n\n\n\n### Most Helpful Answer\nI had the same issues, screen freezing, opening apps by itself, selecting the screens and typing on it\'s own. I first suspected aliens and then ghosts and then hackers.\niPhone 6 is weak physically and tend to bend on pressure. And my phone had no case or cover.\nI took the phone to apple stores and they said sensors need to be replaced and possibly screen replacement as well. My phone is just 17 months old.\nHere is what I did two days ago and since then it is working like a charm..\nHold the phone in portrait (as if watching a movie). Twist it very very gently. do it few times.Rest the phone for 10 mins (put it on a flat surface). You can now notice those self typing things gone and screen getting stabilized.\nThen, reset the hardware (hold the power and home button till the screen goes off and comes back with apple logo). release the buttons when you see this.\nThen, connect to your laptop and log in to iTunes and reset your phone completely. (please take a back-up first).\nAnd your phone should be good to use again.\nWhat really happened here for me is that the sensors might have stuck to the screen and with mild twisting, they got disengaged/released.\nI posted this in Apple Community and the moderators deleted it, for the best reasons known to them.\nInstead of throwing away your phone (or selling cheaply), try this and you could be saving your phone.\nLet me know how it goes.\n\n\n\n### Other Answer\nIt was the charging cord! I bought a gas station braided cord and it was the culprit. Once I plugged my OEM cord into the phone the GHOSTS went away.\n\n\n\n### Other Answer\nI\'ve same issue that I just get resolved.  I first tried to restore it from iCloud back, however it was not a software issue or any virus issue, so after restore same problem continues. Then I get my phone to local area iphone repairing lab, and they detected that it is an LCD issue. LCD get out of order without any reason (It was neither hit or nor slipped, but LCD get out of order all and sudden, while using it) it started opening things at random. I get LCD replaced with new one, that cost me $80.00 in total  ($70.00 LCD charges + $10.00 as labor charges to fix it). iPhone is back to perfect mode now.  It was iphone 6s. Thanks.\n\n\n\n### Other Answer\nI was having the same issue with my 6 plus, I took it to a repair shop, they opened the phone, disconnected the three ribbons the screen has, blew up and cleaned the connectors and connected the screen again and it solved the issue‚Ä¶ it‚Äôs hardware, not software.\n\n\n\n### Other Answer\nHey.\nJust had this problem now. As it turns out, you just need to plug in your phone. I use a case and when I took it off I noticed that there was a lot of dust and dirt around the areas that the case didn\'t cover. I shined a light in my ports and noticed they were filled with dust. Tomorrow I plan on using pressurized air to clean it out and the problem should be solved.  If you plug in your phone and unplug it and it stops the issue, I recommend cleaning your phone thoroughly.\n\n\n\n### Other Answer\nI simply changed the power supply and problem was gone. The block that plugs in the wall not the sub cord. The cord was fine but not the block.\n\n\n\n### Other Answer\nSomeone ask!  I purchased my iPhone 6s Plus for 1000 from at&t.  Before I touched it, I purchased a otter defender case.  I read where at&t said touch desease was due to dropping!  Bullshit!!  I am 56 I have never dropped it!! Looks brand new!  Never dropped or abused any way!  I have my original charger.  I am going to clean it and try everyone‚Äôs advice.  It really sucks!  I had 40,000,000 on my heart of Vegas slots!  I play every day.  I would be spinning and my fingers were no where max buttons and it would light up and switch to max.  It did it 3 times before I caught it light up by its self.  It sucks. Hope I can fix it!!!!\n\n\n\n### Other Answer\nNo answer, but same problem with iPhone 6 plus--random, self-generated jumping amongst apps and typing on its own--plus freezing regularly (aha--maybe that\'s what the ""plus"" in ""6 plus"" refers to?).  An Apple Genius recommended upgrading to iOS 11.3.1 from 11.2.2, to see if that fixed the trouble.  If it didn\'t, Apple will sell me a new phone for $168!  Of couese the OS upgrade didn\'t fix the problem.  Thanks for helping me figure out that it\'s most likely a hardware problem--which the ""genius"" probably knows too.\nI\'m getting ready to go Android.\n\n\n\n### Other Answer\nI experienced similar ghost touches.  Two weeks ago, I changed my iPhone 6 Plus shell (I had forced the phone into it because it‚Äôs pretty tight), and also put a new glass screen protector (the edges of the protector don‚Äôt stick to the screen, weird, so I brushed pressure on the edges at times to see if they may smooth out one day miraculously).  I‚Äôm not sure if I accidentally bend the phone when I installed the shell,  or, if I got a defective glass protector that messes up the touch sensor. Well, yesterday was the worse day, keeps dropping calls and ghost pressing keys for me when I was on a call.  I got fed up, so I removed the screen protector, and so far problems have not reoccurred yet. I‚Äôm crossing my fingers that problems indeed solved.\n\n\n\n### Other Answer\nthank you so much for this post! i was struggling doing the reset because i cannot type userids and passwords correctly because the iphone 6 plus i have kept on typing letters incorrectly. I have been doing it for a day until i come across this article. Very helpful! God bless you!!\n\n\n\n### Other Answer\nI just turned it off, and turned it back on.\n\n\n\n### Other Answer\nMy problem has not gone away completely but its better now i changed my charger and turned off prediction ....,,,now it rarely happens\n\n\n\n### Other Answer\nI tried all of the above. I then turned off my home cleaned it with isopropyl alcohol 90%. Then I baked it in my oven on warm for an hour and a half over foil. Took it out and set it cool completely on the glass top stove. Then I turned on and it worked.\n\n\n\n### Other Answer\nI think at& t should man up and fix your phone for free!  You pay a lot for a Apple they should back it.  I did the next 30 month payments and finally have it paid off in June.  My iPad sept.  Looking forward to a almost 100 drop in my phone bill!  Now this crap!!! Really\n\n\n\n### Other Answer\nIf your phone is JailBroken, suggest downloading a virus.  While all my symptoms were similar, there was indeed a virus/malware on the phone which allowed for remote control of my iphone (even while in lock mode).  My mistake for buying a third party iphone i suppose.  Anyway i have since had the phone restored to factory and everything is working as expected for now.  I will of course keep you posted if this changes.  Thanks to all for the helpful posts, really helped me narrow a few things down.\n\n\n\n### Other Answer\nWhen my phone was doing this, it ended up being the screen protector that i got from 5 below. I took it off and it stopped. I ordered more protectors from amazon and replaced it\n\n\n\n### Other Answer\niPhone 6 Plus first generation‚Ä¶.I had the same issues as all above, apps opening by themselves, self typing, ultra sensitive screen, items jumping around all over‚Ä¶.it even called someone on FaceTime twice by itself when I was not in the room‚Ä¶..I thought the phone was toast and i‚Äôd have to buy a new one took me a while to figure out but it was the extra cheap block plug I bought at a dollar store for convenience of an extra charging station when I move around the house from den to living room‚Ä¶..cord was fine but bought a new Apple brand block plug‚Ä¶no more problems works just fine now. This issue was a recent event so had to narrow things down to what had changed recently to my phone so I could figure it out.\nI even had the same problem on a laptop with documents opening up by themselves‚Ä¶..a laptop that was plugged in to the same wall plug as my phone charger with the dollar store block plug‚Ä¶.until I changed the block plug.\n\n\n\n### Other Answer\nHad the problem: Inherited a 6s Plus from my wife. She had no problem with it.\nLooks like it was merely the cheap phone case I purchased on Amazon. It was either pinching the edges or torquing the screen/body of the phone. Problem solved.\n\n\n\n### Other Answer\nI bought my phone on march 6 and it was a brand new, but It sucks me uo because it freezing, shaking and control by itself. I went to the store where I bought this and I told them to replacr it, but they told me I have to pay it because Its about lcd issue. Please help me what other ways to fix it. Or should I try to remove the screen or should I follow your step above.\n\n\n\n### Other Answer\nI tried everything and it seems to come back to needing the original iPhone cable‚Ä¶or at least another 1 that would have come with another iPhone‚Ä¶not the $5 Store fast charging cables.  My original cable is pretty beat up - like most that I see - but I‚Äôve been beaten up much MUCH less by sticking with its use!  I didn‚Äôt find that the casing/shell around it or not made any diff.\n\n\n\n### Other Answer\ngreat now I have to wait one more hour to reset my phone and while I was tryin to connect my phone to my computer the computer also restarted smh does anyone else knows how I can get my phone to work‚Ä¶ my problem is I have a black dot on the bottom left of my screen an it wont allow me to touch a certain part of my screen unless I rotate my phone and I know the password but the first number is a 2 and it won\'t let me touch 1,2, or 3 so now I have to find a way to get rid of my password and all of a sudden my phone wants to touch stuff on its own which got my phone disabled many times to the point where I have to wait a whole hour and I really need to finish something on my phone today PLEASE HELPPPP\n\n\n\n### Other Answer\nIn my case , iphone 6 screen was faulty. I got it replaced at local repair shop, so far phone is working fine.\n\n\n\n### Other Answer\nthis problem in iphone 6 has many different scenarios and solutions, first try to reconnect the lcd screen to the motherboard again, if didnt solve, try to replace the lcd connector on the motherboard, if not solved, then remains two issues, lcd screen it self or touch IC. in my country some repair shops just change them all for almost 40$ since they dont want to troubleshoot one by one. readers of this comment also should know that partial screen not responding in other iphone models might also have an issue in LCD connector on the motherboard, specially if you lock/unlock screen and screen works again for sometime. lcd connectors gets disconnected lightly from the motherboard due to multiple falls and hits after sometime. best of luck for all\n\n\n\n### Other Answer\nI am facing the same issue whereby these ghost touches type and open apps , I am using an original Iphone cable , how to I fix this issue.\n\n\n\n### Other Answer\nThere were two issues with the phone I had troubles with. It was my dads and turns out he carried it in his pocket. The phone itself had a little bend in it as a result. A little pressure in the opposite direction helped the issue. But it also had a tiny crack in the screen which wasnt obvious, once we added a screen protector this fixed the issues entirely.\n\n\n\n### Other Answer\nI had the same problem with my 64Gb iPhone 6+. Tried a lot of things and eventually downloaded all my images and videos to my PC and restarted the phone - problem solved. Been working now for two days.', lookup_str='', metadata={'source': 'https://www.ifixit.com/Answers/View/318583/My+iPhone+6+is+typing+and+opening+apps+by+itself', 'title': 'My iPhone 6 is typing and opening apps by itself'}, lookup_index=0)]loader = IFixitLoader(""https://www.ifixit.com/Device/Standard_iPad"")data = loader.load()data[Document(page_content=""Standard iPad\nThe standard edition of the tablet computer made by Apple.\n== Background Information ==\n\nOriginally introduced in January 2010, the iPad is Apple's standard edition of their tablet computer. In total, there have been ten generations of the standard edition of the iPad.\n\n== Additional Information ==\n\n* [link|https://www.apple.com/ipad-select/|Official Apple Product Page]\n* [link|https://en.wikipedia.org/wiki/IPad#iPad|Official iPad Wikipedia]"", lookup_str='', metadata={'source': 'https://www.ifixit.com/Device/Standard_iPad', 'title': 'Standard iPad'}, lookup_index=0)]Searching iFixit using /suggest‚ÄãIf you‚Äôre looking for a more general way to search iFixit based on a\nkeyword or phrase, the /suggest endpoint will return content related to\nthe search term, then the loader will load the content from each of the\nsuggested items and prep and return the documents.data = IFixitLoader.load_suggestions(""Banana"")data[Document(page_content='Banana\nTasty fruit. Good source of potassium. Yellow.\n== Background Information ==\n\nCommonly misspelled, this wildly popular, phone shaped fruit serves as nutrition and an obstacle to slow down vehicles racing close behind you. Also used commonly as a synonym for ‚Äúcrazy‚Äù or ‚Äúinsane‚Äù.\n\nBotanically, the banana is considered a berry, although it isn‚Äôt included in the culinary berry category containing strawberries and raspberries. Belonging to the genus Musa, the banana originated in Southeast Asia and Australia. Now largely cultivated throughout South and Central America, bananas are largely available throughout the world. They are especially valued as a staple food group in developing countries due to the banana tree‚Äôs ability to produce fruit year round.\n\nThe banana can be easily opened. Simply remove the outer yellow shell by cracking the top of the stem. Then, with the broken piece, peel downward on each side until the fruity components on the inside are exposed. Once the shell has been removed it cannot be put back together.\n\n== Technical Specifications ==\n\n* Dimensions: Variable depending on genetics of the parent tree\n* Color: Variable depending on ripeness, region, and season\n\n== Additional Information ==\n\n[link|https://en.wikipedia.org/wiki/Banana|Wiki: Banana]', lookup_str='', metadata={'source': 'https://www.ifixit.com/Device/Banana', 'title': 'Banana'}, lookup_index=0), Document(page_content=""# Banana Teardown\nIn this teardown, we open a banana to see what's inside.  Yellow and delicious, but most importantly, yellow.\n\n\n###Tools Required:\n\n - Fingers\n\n - Teeth\n\n - Thumbs\n\n\n###Parts Required:\n\n - None\n\n\n## Step 1\nTake one banana from the bunch.\nDon't squeeze too hard!\n\n\n## Step 2\nHold the banana in your left hand and grip the stem between your right thumb and forefinger.\n\n\n## Step 3\nPull the stem downward until the peel splits.\n\n\n## Step 4\nInsert your thumbs into the split of the peel and pull the two sides apart.\nExpose the top of the banana.  It may be slightly squished from pulling on the stem, but this will not affect the flavor.\n\n\n## Step 5\nPull open the peel, starting from your original split, and opening it along the length of the banana.\n\n\n## Step 6\nRemove fruit from peel.\n\n\n## Step 7\nEat and enjoy!\nThis is where you'll need your teeth.\nDo not choke on banana!\n"", lookup_str='', metadata={'source': 'https://www.ifixit.com/Teardown/Banana+Teardown/811', 'title': 'Banana Teardown'}, lookup_index=0)]",en,
https://python.langchain.com/docs/integrations/document_loaders/google_bigquery,Google BigQuery | ü¶úÔ∏èüîó Langchain,Google BigQuery is a serverless,"Google BigQueryGoogle BigQuery is a serverless\nand cost-effective enterprise data warehouse that works across clouds\nand scales with your data. BigQuery is a part of the\nGoogle Cloud Platform.Load a BigQuery query with one document per row.%pip install --upgrade --quiet  google-cloud-bigqueryfrom langchain_community.document_loaders import BigQueryLoaderBASE_QUERY = """"""SELECT  id,  dna_sequence,  organismFROM (  SELECT    ARRAY (    SELECT      AS STRUCT 1 AS id, ""ATTCGA"" AS dna_sequence, ""Lokiarchaeum sp. (strain GC14_75)."" AS organism    UNION ALL    SELECT      AS STRUCT 2 AS id, ""AGGCGA"" AS dna_sequence, ""Heimdallarchaeota archaeon (strain LC_2)."" AS organism    UNION ALL    SELECT      AS STRUCT 3 AS id, ""TCCGGA"" AS dna_sequence, ""Acidianus hospitalis (strain W1)."" AS organism) AS new_array),  UNNEST(new_array)""""""Basic Usage‚Äãloader = BigQueryLoader(BASE_QUERY)data = loader.load()print(data)[Document(page_content='id: 1\ndna_sequence: ATTCGA\norganism: Lokiarchaeum sp. (strain GC14_75).', lookup_str='', metadata={}, lookup_index=0), Document(page_content='id: 2\ndna_sequence: AGGCGA\norganism: Heimdallarchaeota archaeon (strain LC_2).', lookup_str='', metadata={}, lookup_index=0), Document(page_content='id: 3\ndna_sequence: TCCGGA\norganism: Acidianus hospitalis (strain W1).', lookup_str='', metadata={}, lookup_index=0)]Specifying Which Columns are Content vs Metadata‚Äãloader = BigQueryLoader(    BASE_QUERY,    page_content_columns=[""dna_sequence"", ""organism""],    metadata_columns=[""id""],)data = loader.load()print(data)[Document(page_content='dna_sequence: ATTCGA\norganism: Lokiarchaeum sp. (strain GC14_75).', lookup_str='', metadata={'id': 1}, lookup_index=0), Document(page_content='dna_sequence: AGGCGA\norganism: Heimdallarchaeota archaeon (strain LC_2).', lookup_str='', metadata={'id': 2}, lookup_index=0), Document(page_content='dna_sequence: TCCGGA\norganism: Acidianus hospitalis (strain W1).', lookup_str='', metadata={'id': 3}, lookup_index=0)]Adding Source to Metadata‚Äã# Note that the `id` column is being returned twice, with one instance aliased as `source`ALIASED_QUERY = """"""SELECT  id,  dna_sequence,  organism,  id as sourceFROM (  SELECT    ARRAY (    SELECT      AS STRUCT 1 AS id, ""ATTCGA"" AS dna_sequence, ""Lokiarchaeum sp. (strain GC14_75)."" AS organism    UNION ALL    SELECT      AS STRUCT 2 AS id, ""AGGCGA"" AS dna_sequence, ""Heimdallarchaeota archaeon (strain LC_2)."" AS organism    UNION ALL    SELECT      AS STRUCT 3 AS id, ""TCCGGA"" AS dna_sequence, ""Acidianus hospitalis (strain W1)."" AS organism) AS new_array),  UNNEST(new_array)""""""loader = BigQueryLoader(ALIASED_QUERY, metadata_columns=[""source""])data = loader.load()print(data)[Document(page_content='id: 1\ndna_sequence: ATTCGA\norganism: Lokiarchaeum sp. (strain GC14_75).\nsource: 1', lookup_str='', metadata={'source': 1}, lookup_index=0), Document(page_content='id: 2\ndna_sequence: AGGCGA\norganism: Heimdallarchaeota archaeon (strain LC_2).\nsource: 2', lookup_str='', metadata={'source': 2}, lookup_index=0), Document(page_content='id: 3\ndna_sequence: TCCGGA\norganism: Acidianus hospitalis (strain W1).\nsource: 3', lookup_str='', metadata={'source': 3}, lookup_index=0)]",en,
https://python.langchain.com/docs/integrations/document_loaders/tomarkdown,2Markdown | ü¶úÔ∏èüîó Langchain,2markdown service transforms website content,"2Markdown2markdown service transforms website content\ninto structured markdown files.# You will need to get your own API key. See https://2markdown.com/loginapi_key = """"from langchain_community.document_loaders import ToMarkdownLoaderloader = ToMarkdownLoader(url=""/docs/get_started/introduction"", api_key=api_key)docs = loader.load()print(docs[0].page_content)**LangChain** is a framework for developing applications powered by language models. It enables applications that:- **Are context-aware**: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)- **Reason**: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)This framework consists of several parts.- **LangChain Libraries**: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents.- **[LangChain Templates](/docs/templates)**: A collection of easily deployable reference architectures for a wide variety of tasks.- **[LangServe](/docs/langserve)**: A library for deploying LangChain chains as a REST API.- **[LangSmith](/docs/langsmith)**: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.![Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.](https://python.langchain.com/assets/images/langchain_stack-f21828069f74484521f38199910007c1.svg)Together, these products simplify the entire application lifecycle:- **Develop**: Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference.- **Productionize**: Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence.- **Deploy**: Turn any chain into an API with LangServe.## LangChain Libraries [‚Äã](\#langchain-libraries ""Direct link to LangChain Libraries"")The main value props of the LangChain packages are:1. **Components**: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not2. **Off-the-shelf chains**: built-in assemblages of components for accomplishing higher-level tasksOff-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.The LangChain libraries themselves are made up of several different packages.- **`langchain-core`**: Base abstractions and LangChain Expression Language.- **`langchain-community`**: Third party integrations.- **`langchain`**: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.## Get started [‚Äã](\#get-started ""Direct link to Get started"")[Here‚Äôs](/docs/get_started/installation) how to install LangChain, set up your environment, and start building.We recommend following our [Quickstart](/docs/get_started/quickstart) guide to familiarize yourself with the framework by building your first LangChain application.Read up on our [Security](/docs/security) best practices to make sure you're developing safely with LangChain.noteThese docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.## LangChain Expression Language (LCEL) [‚Äã](\#langchain-expression-language-lcel ""Direct link to LangChain Expression Language (LCEL)"")LCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest ‚Äúprompt + LLM‚Äù chain to the most complex chains.- **[Overview](/docs/expression_language/)**: LCEL and its benefits- **[Interface](/docs/expression_language/interface)**: The standard interface for LCEL objects- **[How-to](/docs/expression_language/how_to)**: Key features of LCEL- **[Cookbook](/docs/expression_language/cookbook)**: Example code for accomplishing common tasks## Modules [‚Äã](\#modules ""Direct link to Modules"")LangChain provides standard, extendable interfaces and integrations for the following modules:#### [Model I/O](/docs/modules/model_io/) [‚Äã](\#model-io ""Direct link to model-io"")Interface with language models#### [Retrieval](/docs/modules/data_connection/) [‚Äã](\#retrieval ""Direct link to retrieval"")Interface with application-specific data#### [Agents](/docs/modules/agents/) [‚Äã](\#agents ""Direct link to agents"")Let models choose which tools to use given high-level directives## Examples, ecosystem, and resources [‚Äã](\#examples-ecosystem-and-resources ""Direct link to Examples, ecosystem, and resources"")### [Use cases](/docs/use_cases/question_answering/) [‚Äã](\#use-cases ""Direct link to use-cases"")Walkthroughs and techniques for common end-to-end use cases, like:- [Document question answering](/docs/use_cases/question_answering/)- [Chatbots](/docs/use_cases/chatbots/)- [Analyzing structured data](/docs/use_cases/sql/)- and much more...### [Integrations](/docs/integrations/providers/) [‚Äã](\#integrations ""Direct link to integrations"")LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of [integrations](/docs/integrations/providers/).### [Guides](/docs/guides/debugging) [‚Äã](\#guides ""Direct link to guides"")Best practices for developing with LangChain.### [API reference](https://api.python.langchain.com) [‚Äã](\#api-reference ""Direct link to api-reference"")Head to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages.### [Developer's guide](/docs/contributing) [‚Äã](\#developers-guide ""Direct link to developers-guide"")Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.Head to the [Community navigator](/docs/community) to find places to ask questions, share feedback, meet other developers, and dream about the future of LLM‚Äôs.",en,
https://python.langchain.com/docs/integrations/document_loaders/hugging_face_dataset,HuggingFace dataset | ü¶úÔ∏èüîó Langchain,The Hugging Face Hub is home,"HuggingFace datasetThe Hugging Face Hub is home\nto over 5,000\ndatasets in more\nthan 100 languages that can be used for a broad range of tasks across\nNLP, Computer Vision, and Audio. They used for a diverse range of\ntasks such as translation, automatic speech recognition, and image\nclassification.This notebook shows how to load Hugging Face Hub datasets to\nLangChain.from langchain_community.document_loaders import HuggingFaceDatasetLoaderdataset_name = ""imdb""page_content_column = ""text""loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)data = loader.load()data[:15][Document(page_content='I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered ""controversial"" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\'t have much of a plot.', metadata={'label': 0}), Document(page_content='""I Am Curious: Yellow"" is a risible and pretentious steaming pile. It doesn\'t matter what one\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\'t true. I\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\'re treated to the site of Vincent Gallo\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) ""double-standard"" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\'s bodies.', metadata={'label': 0}), Document(page_content=""If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />"", metadata={'label': 0}), Document(page_content=""This film was probably inspired by Godard's Masculin, f√©minin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10."", metadata={'label': 0}), Document(page_content='Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />""Is that all there is??"" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into ""Goodbye Columbus""). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\'t for the censorship scandal, it would have been ignored, then forgotten.<br /><br />Instead, the ""I Am Blank, Blank"" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that ""naughty sex film"" that ""revolutionized the film industry""...<br /><br />Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the ""dirty"" parts, just to get it over with.<br /><br />', metadata={'label': 0}), Document(page_content=""I would put this at the top of my list of films in the category of unwatchable trash! There are films that are bad, but the worst kind are the ones that are unwatchable but you are suppose to like them because they are supposed to be good for you! The sex sequences, so shocking in its day, couldn't even arouse a rabbit. The so called controversial politics is strictly high school sophomore amateur night Marxism. The film is self-consciously arty in the worst sense of the term. The photography is in a harsh grainy black and white. Some scenes are out of focus or taken from the wrong angle. Even the sound is bad! And some people call this art?<br /><br />"", metadata={'label': 0}), Document(page_content=""Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me."", metadata={'label': 0}), Document(page_content='When I first saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York\'s portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe out of all the actresses in the world who could play a much better Lucy, the producers decided to get Rachel York. She might be a good actress in other roles but to play the role of Lucille Ball is tough. It is pretty hard to find someone who could resemble Lucille Ball, but they could at least find someone a bit similar in looks and talent. If you noticed York\'s portrayal of Lucy in episodes of I Love Lucy like the chocolate factory or vitavetavegamin, nothing is similar in any way-her expression, voice, or movement.<br /><br />To top it all off, Danny Pino playing Desi Arnaz is horrible. Pino does not qualify to play as Ricky. He\'s small and skinny, his accent is unreal, and once again, his acting is unbelievable. Although Fred and Ethel were not similar either, they were not as bad as the characters of Lucy and Ricky.<br /><br />Overall, extremely horrible casting and the story is badly told. If people want to understand the real life situation of Lucille Ball, I suggest watching A&E Biography of Lucy and Desi, read the book from Lucille Ball herself, or PBS\' American Masters: Finding Lucy. If you want to see a docudrama, ""Before the Laughter"" would be a better choice. The casting of Lucille Ball and Desi Arnaz in ""Before the Laughter"" is much better compared to this. At least, a similar aspect is shown rather than nothing.', metadata={'label': 0}), Document(page_content='Who are these ""They""- the actors? the filmmakers? Certainly couldn\'t be the audience- this is among the most air-puffed productions in existence. It\'s the kind of movie that looks like it was a lot of fun to shoot\x97 TOO much fun, nobody is getting any actual work done, and that almost always makes for a movie that\'s no fun to watch.<br /><br />Ritter dons glasses so as to hammer home his character\'s status as a sort of doppleganger of the bespectacled Bogdanovich; the scenes with the breezy Ms. Stratten are sweet, but have an embarrassing, look-guys-I\'m-dating-the-prom-queen feel to them. Ben Gazzara sports his usual cat\'s-got-canary grin in a futile attempt to elevate the meager plot, which requires him to pursue Audrey Hepburn with all the interest of a narcoleptic at an insomnia clinic. In the meantime, the budding couple\'s respective children (nepotism alert: Bogdanovich\'s daughters) spew cute and pick up some fairly disturbing pointers on \'love\' while observing their parents. (Ms. Hepburn, drawing on her dignity, manages to rise above the proceedings- but she has the monumental challenge of playing herself, ostensibly.) Everybody looks great, but so what? It\'s a movie and we can expect that much, if that\'s what you\'re looking for you\'d be better off picking up a copy of Vogue.<br /><br />Oh- and it has to be mentioned that Colleen Camp thoroughly annoys, even apart from her singing, which, while competent, is wholly unconvincing... the country and western numbers are woefully mismatched with the standards on the soundtrack. Surely this is NOT what Gershwin (who wrote the song from which the movie\'s title is derived) had in mind; his stage musicals of the 20\'s may have been slight, but at least they were long on charm. ""They All Laughed"" tries to coast on its good intentions, but nobody- least of all Peter Bogdanovich - has the good sense to put on the brakes.<br /><br />Due in no small part to the tragic death of Dorothy Stratten, this movie has a special place in the heart of Mr. Bogdanovich- he even bought it back from its producers, then distributed it on his own and went bankrupt when it didn\'t prove popular. His rise and fall is among the more sympathetic and tragic of Hollywood stories, so there\'s no joy in criticizing the film... there _is_ real emotional investment in Ms. Stratten\'s scenes. But ""Laughed"" is a faint echo of ""The Last Picture Show"", ""Paper Moon"" or ""What\'s Up, Doc""- following ""Daisy Miller"" and ""At Long Last Love"", it was a thundering confirmation of the phase from which P.B. has never emerged.<br /><br />All in all, though, the movie is harmless, only a waste of rental. I want to watch people having a good time, I\'ll go to the park on a sunny day. For filmic expressions of joy and love, I\'ll stick to Ernest Lubitsch and Jaques Demy...', metadata={'label': 0}), Document(page_content=""This is said to be a personal film for Peter Bogdonavitch. He based it on his life but changed things around to fit the characters, who are detectives. These detectives date beautiful models and have no problem getting them. Sounds more like a millionaire playboy filmmaker than a detective, doesn't it? This entire movie was written by Peter, and it shows how out of touch with real people he was. You're supposed to write what you know, and he did that, indeed. And leaves the audience bored and confused, and jealous, for that matter. This is a curio for people who want to see Dorothy Stratten, who was murdered right after filming. But Patti Hanson, who would, in real life, marry Keith Richards, was also a model, like Stratten, but is a lot better and has a more ample part. In fact, Stratten's part seemed forced; added. She doesn't have a lot to do with the story, which is pretty convoluted to begin with. All in all, every character in this film is somebody that very few people can relate with, unless you're millionaire from Manhattan with beautiful supermodels at your beckon call. For the rest of us, it's an irritating snore fest. That's what happens when you're out of touch. You entertain your few friends with inside jokes, and bore all the rest."", metadata={'label': 0}), Document(page_content='It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\'t go on to star in more and better films. Sadly, I didn\'t think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, ""Cat\'s Meow"" and all his early ones from ""Targets"" to ""Nickleodeon"". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\'s ex-girlfriend, Cybil Shepherd had a hit television series called ""Moonlighting"" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain\'t no ""Paper Moon"" and only a very pale version of ""What\'s Up, Doc"".', metadata={'label': 0}), Document(page_content=""I can't believe that those praising this movie herein aren't thinking of some other film. I was prepared for the possibility that this would be awful, but the script (or lack thereof) makes for a film that's also pointless. On the plus side, the general level of craft on the part of the actors and technical crew is quite competent, but when you've got a sow's ear to work with you can't make a silk purse. Ben G fans should stick with just about any other movie he's been in. Dorothy S fans should stick to Galaxina. Peter B fans should stick to Last Picture Show and Target. Fans of cheap laughs at the expense of those who seem to be asking for it should stick to Peter B's amazingly awful book, Killing of the Unicorn."", metadata={'label': 0}), Document(page_content='Never cast models and Playboy bunnies in your films! Bob Fosse\'s ""Star 80"" about Dorothy Stratten, of whom Bogdanovich was obsessed enough to have married her SISTER after her murder at the hands of her low-life husband, is a zillion times more interesting than Dorothy herself on the silver screen. Patty Hansen is no actress either..I expected to see some sort of lost masterpiece a la Orson Welles but instead got Audrey Hepburn cavorting in jeans and a god-awful ""poodlesque"" hair-do....Very disappointing....""Paper Moon"" and ""The Last Picture Show"" I could watch again and again. This clunker I could barely sit through once. This movie was reputedly not released because of the brouhaha surrounding Ms. Stratten\'s tawdry death; I think the real reason was because it was so bad!', metadata={'label': 0}), Document(page_content=""Its not the cast. A finer group of actors, you could not find. Its not the setting. The director is in love with New York City, and by the end of the film, so are we all! Woody Allen could not improve upon what Bogdonovich has done here. If you are going to fall in love, or find love, Manhattan is the place to go. No, the problem with the movie is the script. There is none. The actors fall in love at first sight, words are unnecessary. In the director's own experience in Hollywood that is what happens when they go to work on the set. It is reality to him, and his peers, but it is a fantasy to most of us in the real world. So, in the end, the movie is hollow, and shallow, and message-less."", metadata={'label': 0}), Document(page_content='Today I found ""They All Laughed"" on VHS on sale in a rental. It was a really old and very used VHS, I had no information about this movie, but I liked the references listed on its cover: the names of Peter Bogdanovich, Audrey Hepburn, John Ritter and specially Dorothy Stratten attracted me, the price was very low and I decided to risk and buy it. I searched IMDb, and the User Rating of 6.0 was an excellent reference. I looked in ""Mick Martin & Marsha Porter Video & DVD Guide 2003"" and \x96 wow \x96 four stars! So, I decided that I could not waste more time and immediately see it. Indeed, I have just finished watching ""They All Laughed"" and I found it a very boring overrated movie. The characters are badly developed, and I spent lots of minutes to understand their roles in the story. The plot is supposed to be funny (private eyes who fall in love for the women they are chasing), but I have not laughed along the whole story. The coincidences, in a huge city like New York, are ridiculous. Ben Gazarra as an attractive and very seductive man, with the women falling for him as if her were a Brad Pitt, Antonio Banderas or George Clooney, is quite ridiculous. In the end, the greater attractions certainly are the presence of the Playboy centerfold and playmate of the year Dorothy Stratten, murdered by her husband pretty after the release of this movie, and whose life was showed in ""Star 80"" and ""Death of a Centerfold: The Dorothy Stratten Story""; the amazing beauty of the sexy Patti Hansen, the future Mrs. Keith Richards; the always wonderful, even being fifty-two years old, Audrey Hepburn; and the song ""Amigo"", from Roberto Carlos. Although I do not like him, Roberto Carlos has been the most popular Brazilian singer since the end of the 60\'s and is called by his fans as ""The King"". I will keep this movie in my collection only because of these attractions (manly Dorothy Stratten). My vote is four.<br /><br />Title (Brazil): ""Muito Riso e Muita Alegria"" (""Many Laughs and Lots of Happiness"")', metadata={'label': 0})]Example‚ÄãIn this example, we use data from a dataset to answer a questionfrom langchain.indexes import VectorstoreIndexCreatorfrom langchain_community.document_loaders.hugging_face_dataset import (    HuggingFaceDatasetLoader,)dataset_name = ""tweet_eval""page_content_column = ""text""name = ""stance_climate""loader = HuggingFaceDatasetLoader(dataset_name, page_content_column, name)index = VectorstoreIndexCreator().from_loaders([loader])Found cached dataset tweet_evalUsing embedded DuckDB without persistence: data will be transient  0%|          | 0/3 [00:00<?, ?it/s]query = ""What are the most used hashtag?""result = index.query(query)result' The most used hashtags in this context are #UKClimate2015, #Sustainability, #TakeDownTheFlag, #LoveWins, #CSOTA, #ClimateSummitoftheAmericas, #SM, and #SocialMedia.'",en,
https://python.langchain.com/docs/integrations/document_loaders/notion,Notion DB 1/2 | ü¶úÔ∏èüîó Langchain,Notion is a collaboration platform with,"Notion DB 1/2Notion is a collaboration platform with\nmodified Markdown support that integrates kanban boards, tasks, wikis\nand databases. It is an all-in-one workspace for notetaking, knowledge\nand data management, and project and task management.This notebook covers how to load documents from a Notion database dump.In order to get this notion dump, follow these instructions:üßë Instructions for ingesting your own dataset‚ÄãExport your dataset from Notion. You can do this by clicking on the\nthree dots in the upper right hand corner and then clicking Export.When exporting, make sure to select the Markdown & CSV format option.This will produce a .zip file in your Downloads folder. Move the\n.zip file into this repository.Run the following command to unzip the zip file (replace the Export...\nwith your own file name as needed).unzip Export-d3adfe0f-3131-4bf3-8987-a52017fc1bae.zip -d Notion_DBRun the following command to ingest the data.from langchain_community.document_loaders import NotionDirectoryLoaderloader = NotionDirectoryLoader(""Notion_DB"")docs = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/airbyte_shopify,Airbyte Shopify (Deprecated) | ü¶úÔ∏èüîó Langchain,Note: This connector-specific loader is deprecated. Please use,"Airbyte Shopify (Deprecated)Note: This connector-specific loader is deprecated. Please use\nAirbyteLoader instead.Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases.This loader exposes the Shopify connector as a document loader, allowing\nyou to load various Shopify objects as documents.Installation‚ÄãFirst, you need to install the airbyte-source-shopify python package.%pip install --upgrade --quiet  airbyte-source-shopifyExample‚ÄãCheck out the Airbyte documentation\npage for\ndetails about how to configure the reader. The JSON schema the config\nobject should adhere to can be found on Github:\nhttps://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-shopify/source_shopify/spec.json.The general shape looks like this:{    ""start_date"": ""<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>"",    ""shop"": ""<name of the shop you want to retrieve documents from>"",    ""credentials"": {        ""auth_method"": ""api_password"",        ""api_password"": ""<your api password>""    }}By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader.from langchain_community.document_loaders.airbyte import AirbyteShopifyLoaderconfig = {    # your shopify configuration}loader = AirbyteShopifyLoader(    config=config, stream_name=""orders"")  # check the documentation linked above for a list of all streamsNow you can load documents the usual waydocs = loader.load()As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the\nlazy_load method which returns an iterator instead:docs_iterator = loader.lazy_load()Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader:from langchain.docstore.document import Documentdef handle_record(record, id):    return Document(page_content=record.data[""title""], metadata=record.data)loader = AirbyteShopifyLoader(    config=config, record_handler=handle_record, stream_name=""orders"")docs = loader.load()Incremental loads‚ÄãSome streams allow incremental loading, this means the source keeps\ntrack of synced records and won‚Äôt load them again. This is useful for\nsources that have a high volume of data and are updated frequently.To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded.last_state = loader.last_state  # store safelyincremental_loader = AirbyteShopifyLoader(    config=config, stream_name=""orders"", state=last_state)new_docs = incremental_loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/mastodon,Mastodon | ü¶úÔ∏èüîó Langchain,Mastodon is a federated social media and,"MastodonMastodon is a federated social media and\nsocial networking service.This loader fetches the text from the ‚Äútoots‚Äù of a list of Mastodon\naccounts, using the Mastodon.py Python package.Public accounts can the queried by default without any authentication.\nIf non-public accounts or instances are queried, you have to register an\napplication for your account which gets you an access token, and set\nthat token and your account‚Äôs API base URL.Then you need to pass in the Mastodon account names you want to extract,\nin the @account@instance format.from langchain_community.document_loaders import MastodonTootsLoader%pip install --upgrade --quiet  Mastodon.pyloader = MastodonTootsLoader(    mastodon_accounts=[""@Gargron@mastodon.social""],    number_toots=50,  # Default value is 100)# Or set up access information to use a Mastodon app.# Note that the access token can either be passed into# constructor or you can set the environment ""MASTODON_ACCESS_TOKEN"".# loader = MastodonTootsLoader(#     access_token=""<ACCESS TOKEN OF MASTODON APP>"",#     api_base_url=""<API BASE URL OF MASTODON APP INSTANCE>"",#     mastodon_accounts=[""@Gargron@mastodon.social""],#     number_toots=50,  # Default value is 100# )documents = loader.load()for doc in documents[:3]:    print(doc.page_content)    print(""="" * 80)<p>It is tough to leave this behind and go back to reality. And some people live here! I‚Äôm sure there are downsides but it sounds pretty good to me right now.</p>================================================================================<p>I wish we could stay here a little longer, but it is time to go home ü•≤</p>================================================================================<p>Last day of the honeymoon. And it‚Äôs <a href=""https://mastodon.social/tags/caturday"" class=""mention hashtag"" rel=""tag"">#<span>caturday</span></a>! This cute tabby came to the restaurant to beg for food and got some chicken.</p>================================================================================The toot texts (the documents‚Äô page_content) is by default HTML as\nreturned by the Mastodon API.",en,
https://python.langchain.com/docs/integrations/document_loaders/concurrent,Concurrent Loader | ü¶úÔ∏èüîó Langchain,Works just like the GenericLoader but concurrently for those who choose,"Concurrent LoaderWorks just like the GenericLoader but concurrently for those who choose\nto optimize their workflow.from langchain_community.document_loaders import ConcurrentLoaderloader = ConcurrentLoader.from_filesystem(""example_data/"", glob=""**/*.txt"")files = loader.load()len(files)2",en,
https://python.langchain.com/docs/integrations/document_loaders/notiondb,Notion DB 2/2 | ü¶úÔ∏èüîó Langchain,Notion is a collaboration platform with,"Notion DB 2/2Notion is a collaboration platform with\nmodified Markdown support that integrates kanban boards, tasks, wikis\nand databases. It is an all-in-one workspace for notetaking, knowledge\nand data management, and project and task management.NotionDBLoader is a Python class for loading content from a Notion\ndatabase. It retrieves pages from the database, reads their content, and\nreturns a list of Document objects.Requirements‚ÄãA Notion DatabaseNotion Integration TokenSetup‚Äã1. Create a Notion Table Database‚ÄãCreate a new table database in Notion. You can add any column to the\ndatabase and they will be treated as metadata. For example you can add\nthe following columns:Title: set Title as the default property.Categories: A Multi-select property to store categories associated\nwith the page.Keywords: A Multi-select property to store keywords associated with\nthe page.Add your content to the body of each page in the database. The\nNotionDBLoader will extract the content and metadata from these pages.2. Create a Notion Integration‚ÄãTo create a Notion Integration, follow these steps:Visit the Notion\nDevelopers page and log in\nwith your Notion account.Click on the ‚Äú+ New integration‚Äù button.Give your integration a name and choose the workspace where your\ndatabase is located.Select the require capabilities, this extension only need the Read\ncontent capabilityClick the ‚ÄúSubmit‚Äù button to create the integration. Once the\nintegration is created, you‚Äôll be provided with an\nIntegration Token (API key). Copy this token and keep it safe, as\nyou‚Äôll need it to use the NotionDBLoader.3. Connect the Integration to the Database‚ÄãTo connect your integration to the database, follow these steps:Open your database in Notion.Click on the three-dot menu icon in the top right corner of the\ndatabase view.Click on the ‚Äú+ New integration‚Äù button.Find your integration, you may need to start typing its name in the\nsearch box.Click on the ‚ÄúConnect‚Äù button to connect the integration to the\ndatabase.4. Get the Database ID‚ÄãTo get the database ID, follow these steps:Open your database in Notion.Click on the three-dot menu icon in the top right corner of the\ndatabase view.Select ‚ÄúCopy link‚Äù from the menu to copy the database URL to your\nclipboard.The database ID is the long string of alphanumeric characters found\nin the URL. It typically looks like this:\nhttps://www.notion.so/username/8935f9d140a04f95a872520c4f123456?v=‚Ä¶.\nIn this example, the database ID is\n8935f9d140a04f95a872520c4f123456.With the database properly set up and the integration token and database\nID in hand, you can now use the NotionDBLoader code to load content and\nmetadata from your Notion database.Usage‚ÄãNotionDBLoader is part of the langchain package‚Äôs document loaders. You\ncan use it as follows:from getpass import getpassNOTION_TOKEN = getpass()DATABASE_ID = getpass()¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑from langchain_community.document_loaders import NotionDBLoaderloader = NotionDBLoader(    integration_token=NOTION_TOKEN,    database_id=DATABASE_ID,    request_timeout_sec=30,  # optional, defaults to 10)docs = loader.load()print(docs)",en,
https://python.langchain.com/docs/integrations/document_loaders/modern_treasury,Modern Treasury | ü¶úÔ∏èüîó Langchain,Modern Treasury simplifies complex,"Modern TreasuryModern Treasury simplifies complex\npayment operations. It is a unified platform to power products and\nprocesses that move money. - Connect to banks and payment systems -\nTrack transactions and balances in real-time - Automate payment\noperations for scaleThis notebook covers how to load data from the\nModern Treasury REST API into a format that can be ingested into\nLangChain, along with example usage for vectorization.from langchain.indexes import VectorstoreIndexCreatorfrom langchain_community.document_loaders import ModernTreasuryLoaderThe Modern Treasury API requires an organization ID and API key, which\ncan be found in the Modern Treasury dashboard within developer settings.This document loader also requires a resource option which defines\nwhat data you want to load.Following resources are available:payment_orders\nDocumentationexpected_payments\nDocumentationreturns\nDocumentationincoming_payment_details\nDocumentationcounterparties\nDocumentationinternal_accounts\nDocumentationexternal_accounts\nDocumentationtransactions\nDocumentationledgers\nDocumentationledger_accounts\nDocumentationledger_transactions\nDocumentationevents\nDocumentationinvoices\nDocumentationmodern_treasury_loader = ModernTreasuryLoader(""payment_orders"")# Create a vectorstore retriever from the loader# see https://python.langchain.com/en/latest/modules/data_connection/getting_started.html for more detailsindex = VectorstoreIndexCreator().from_loaders([modern_treasury_loader])modern_treasury_doc_retriever = index.vectorstore.as_retriever()",en,
https://python.langchain.com/docs/integrations/document_loaders/quip,Quip | ü¶úÔ∏èüîó Langchain,Quip is a collaborative productivity software,"QuipQuip is a collaborative productivity software\nsuite for mobile and Web. It allows groups of people to create and\nedit documents and spreadsheets as a group, typically for business\npurposes.A loader for Quip docs.Please refer\nhere\nto know how to get personal access token.Specify a list folder_ids and/or thread_ids to load in the\ncorresponding docs into Document objects, if both are specified, loader\nwill get all thread_ids belong to this folder based on folder_ids,\ncombine with passed thread_ids, the union of both sets will be\nreturned.How to know folder_id ? go to quip folder, right click folder and\ncopy link, extract suffix from link as folder_id. Hint:\nhttps://example.quip.com/<folder_id>How to know thread_id ? thread_id is the document id. Go to quip\ndoc, right click doc and copy link, extract suffix from link as\nthread_id. Hint: https://exmaple.quip.com/<thread_id>You can also set include_all_folders as True will fetch\ngroup_folder_ids and You can also specify a boolean\ninclude_attachments to include attachments, this is set to False by\ndefault, if set to True all attachments will be downloaded and\nQuipLoader will extract the text from the attachments and add it to the\nDocument object. Currently supported attachment types are: PDF, PNG,\nJPEG/JPG, SVG, Word and Excel. Also you can sepcify a boolean\ninclude_comments to include comments in document, this is set to False\nby default, if set to True all comments in document will be fetched and\nQuipLoader will add them to Document objec.Before using QuipLoader make sure you have the latest version of the\nquip-api package installed:%pip install --upgrade --quiet  quip-apiExamples‚ÄãPersonal Access Token‚Äãfrom langchain_community.document_loaders import QuipLoaderloader = QuipLoader(    api_url=""https://platform.quip.com"", access_token=""change_me"", request_timeout=60)documents = loader.load(    folder_ids={""123"", ""456""},    thread_ids={""abc"", ""efg""},    include_attachments=False,    include_comments=False,)",en,
https://python.langchain.com/docs/integrations/document_loaders/airbyte_hubspot,Airbyte Hubspot (Deprecated) | ü¶úÔ∏èüîó Langchain,Note: AirbyteHubspotLoader is deprecated. Please use,"Airbyte Hubspot (Deprecated)Note: AirbyteHubspotLoader is deprecated. Please use\nAirbyteLoader instead.Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases.This loader exposes the Hubspot connector as a document loader, allowing\nyou to load various Hubspot objects as documents.Installation‚ÄãFirst, you need to install the airbyte-source-hubspot python package.%pip install --upgrade --quiet  airbyte-source-hubspotExample‚ÄãCheck out the Airbyte documentation\npage for\ndetails about how to configure the reader. The JSON schema the config\nobject should adhere to can be found on Github:\nhttps://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-hubspot/source_hubspot/spec.yaml.The general shape looks like this:{  ""start_date"": ""<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>"",  ""credentials"": {    ""credentials_title"": ""Private App Credentials"",    ""access_token"": ""<access token of your private app>""  }}By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader.from langchain_community.document_loaders.airbyte import AirbyteHubspotLoaderconfig = {    # your hubspot configuration}loader = AirbyteHubspotLoader(    config=config, stream_name=""products"")  # check the documentation linked above for a list of all streamsNow you can load documents the usual waydocs = loader.load()As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the\nlazy_load method which returns an iterator instead:docs_iterator = loader.lazy_load()Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To process\ndocuments, create a class inheriting from the base loader and implement\nthe _handle_records method yourself:from langchain.docstore.document import Documentdef handle_record(record, id):    return Document(page_content=record.data[""title""], metadata=record.data)loader = AirbyteHubspotLoader(    config=config, record_handler=handle_record, stream_name=""products"")docs = loader.load()Incremental loads‚ÄãSome streams allow incremental loading, this means the source keeps\ntrack of synced records and won‚Äôt load them again. This is useful for\nsources that have a high volume of data and are updated frequently.To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded.last_state = loader.last_state  # store safelyincremental_loader = AirbyteHubspotLoader(    config=config, stream_name=""products"", state=last_state)new_docs = incremental_loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/discord,Discord | ü¶úÔ∏èüîó Langchain,Discord is a VoIP and instant messaging social,"DiscordDiscord is a VoIP and instant messaging social\nplatform. Users have the ability to communicate with voice calls,\nvideo calls, text messaging, media and files in private chats or as\npart of communities called ‚Äúservers‚Äù. A server is a collection of\npersistent chat rooms and voice channels which can be accessed via\ninvite links.Follow these steps to download your Discord data:Go to your User SettingsThen go to Privacy and SafetyHead over to the Request all of my Data and click on Request\nData buttonIt might take 30 days for you to receive your data. You‚Äôll receive an\nemail at the address which is registered with Discord. That email will\nhave a download button using which you would be able to download your\npersonal Discord data.import osimport pandas as pdpath = input('Please enter the path to the contents of the Discord ""messages"" folder: ')li = []for f in os.listdir(path):    expected_csv_path = os.path.join(path, f, ""messages.csv"")    csv_exists = os.path.isfile(expected_csv_path)    if csv_exists:        df = pd.read_csv(expected_csv_path, index_col=None, header=0)        li.append(df)df = pd.concat(li, axis=0, ignore_index=True, sort=False)from langchain_community.document_loaders.discord import DiscordChatLoaderloader = DiscordChatLoader(df, user_id_col=""ID"")print(loader.load())",en,
https://python.langchain.com/docs/integrations/document_loaders/evernote,EverNote | ü¶úÔ∏èüîó Langchain,EverNote is intended for archiving and,"EverNoteEverNote is intended for archiving and\ncreating notes in which photos, audio and saved web content can be\nembedded. Notes are stored in virtual ‚Äúnotebooks‚Äù and can be tagged,\nannotated, edited, searched, and exported.This notebook shows how to load an Evernote\nexport\nfile (.enex) from disk.A document will be created for each note in the export.# lxml and html2text are required to parse EverNote notes%pip install --upgrade --quiet  lxml%pip install --upgrade --quiet  html2textfrom langchain_community.document_loaders import EverNoteLoader# By default all notes are combined into a single Documentloader = EverNoteLoader(""example_data/testing.enex"")loader.load()[Document(page_content='testing this\n\nwhat happens?\n\nto the world?**Jan - March 2022**', metadata={'source': 'example_data/testing.enex'})]# It's likely more useful to return a Document for each noteloader = EverNoteLoader(""example_data/testing.enex"", load_single_document=False)loader.load()[Document(page_content='testing this\n\nwhat happens?\n\nto the world?', metadata={'title': 'testing', 'created': time.struct_time(tm_year=2023, tm_mon=2, tm_mday=9, tm_hour=3, tm_min=47, tm_sec=46, tm_wday=3, tm_yday=40, tm_isdst=-1), 'updated': time.struct_time(tm_year=2023, tm_mon=2, tm_mday=9, tm_hour=3, tm_min=53, tm_sec=28, tm_wday=3, tm_yday=40, tm_isdst=-1), 'note-attributes.author': 'Harrison Chase', 'source': 'example_data/testing.enex'}), Document(page_content='**Jan - March 2022**', metadata={'title': 'Summer Training Program', 'created': time.struct_time(tm_year=2022, tm_mon=12, tm_mday=27, tm_hour=1, tm_min=59, tm_sec=48, tm_wday=1, tm_yday=361, tm_isdst=-1), 'note-attributes.author': 'Mike McGarry', 'note-attributes.source': 'mobile.iphone', 'source': 'example_data/testing.enex'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/pubmed,PubMed | ü¶úÔ∏èüîó Langchain,PubMed¬Æ by,"PubMedPubMed¬Æ by\nThe National Center for Biotechnology Information, National Library of Medicine\ncomprises more than 35 million citations for biomedical literature\nfrom MEDLINE, life science journals, and online books. Citations may\ninclude links to full text content from PubMed Central and publisher\nweb sites.from langchain_community.document_loaders import PubMedLoaderloader = PubMedLoader(""chatgpt"")docs = loader.load()len(docs)3docs[1].metadata{'uid': '37548997', 'Title': 'Performance of ChatGPT on the Situational Judgement Test-A Professional Dilemmas-Based Examination for Doctors in the United Kingdom.', 'Published': '2023-08-07', 'Copyright Information': '¬©Robin J Borchert, Charlotte R Hickman, Jack Pepys, Timothy J Sadler. Originally published in JMIR Medical Education (https://mededu.jmir.org), 07.08.2023.'}docs[1].page_content""BACKGROUND: ChatGPT is a large language model that has performed well on professional examinations in the fields of medicine, law, and business. However, it is unclear how ChatGPT would perform on an examination assessing professionalism and situational judgement for doctors.\nOBJECTIVE: We evaluated the performance of ChatGPT on the Situational Judgement Test (SJT): a national examination taken by all final-year medical students in the United Kingdom. This examination is designed to assess attributes such as communication, teamwork, patient safety, prioritization skills, professionalism, and ethics.\nMETHODS: All questions from the UK Foundation Programme Office's (UKFPO's) 2023 SJT practice examination were inputted into ChatGPT. For each question, ChatGPT's answers and rationales were recorded and assessed on the basis of the official UK Foundation Programme Office scoring template. Questions were categorized into domains of Good Medical Practice on the basis of the domains referenced in the rationales provided in the scoring sheet. Questions without clear domain links were screened by reviewers and assigned one or multiple domains. ChatGPT's overall performance, as well as its performance across the domains of Good Medical Practice, was evaluated.\nRESULTS: Overall, ChatGPT performed well, scoring 76% on the SJT but scoring full marks on only a few questions (9%), which may reflect possible flaws in ChatGPT's situational judgement or inconsistencies in the reasoning across questions (or both) in the examination itself. ChatGPT demonstrated consistent performance across the 4 outlined domains in Good Medical Practice for doctors.\nCONCLUSIONS: Further research is needed to understand the potential applications of large language models, such as ChatGPT, in medical education for standardizing questions and providing consistent rationales for examinations assessing professionalism and ethics.""",en,
https://python.langchain.com/docs/integrations/document_loaders/mediawikidump,MediaWiki Dump | ü¶úÔ∏èüîó Langchain,[MediaWiki XML,"MediaWiki DumpMediaWiki XML\nDumps\ncontain the content of a wiki (wiki pages with all their revisions),\nwithout the site-related data. A XML dump does not create a full\nbackup of the wiki database, the dump does not contain user accounts,\nimages, edit logs, etc.This covers how to load a MediaWiki XML dump file into a document format\nthat we can use downstream.It uses mwxml from mediawiki-utilities to dump and\nmwparserfromhell from earwig to parse MediaWiki wikicode.Dump files can be obtained with dumpBackup.php or on the\nSpecial:Statistics page of the Wiki.# mediawiki-utilities supports XML schema 0.11 in unmerged branches%pip install --upgrade --quiet  U git+https://github.com/mediawiki-utilities/python-mwtypes@updates_schema_0.11# mediawiki-utilities mwxml has a bug, fix PR pending%pip install --upgrade --quiet  U git+https://github.com/gdedrouas/python-mwxml@xml_format_0.11%pip install --upgrade --quiet  U mwparserfromhellfrom langchain_community.document_loaders import MWDumpLoaderloader = MWDumpLoader(    file_path=""example_data/testmw_pages_current.xml"",    encoding=""utf8"",    # namespaces = [0,2,3] Optional list to load only specific namespaces. Loads all namespaces by default.    skip_redirects=True,  # will skip over pages that just redirect to other pages (or not if False)    stop_on_error=False,  # will skip over pages that cause parsing errors (or not if False))documents = loader.load()print(f""You have {len(documents)} document(s) in your data "")You have 177 document(s) in your data documents[:5][Document(page_content='\t\n\t\n\tArtist\n\tReleased\n\tRecorded\n\tLength\n\tLabel\n\tProducer', metadata={'source': 'Album'}), Document(page_content='{| class=""article-table plainlinks"" style=""width:100%;""\n|- style=""font-size:18px;""\n! style=""padding:0px;"" | Template documentation\n|-\n| Note: portions of the template sample may not be visible without values provided.\n|-\n| View or edit this documentation. (About template documentation)\n|-\n| Editors can experiment in this template\'s [ sandbox] and [ test case] pages.\n|}Category:Documentation templates', metadata={'source': 'Documentation'}), Document(page_content='Description\nThis template is used to insert descriptions on template pages.\n\nSyntax\nAdd <noinclude></noinclude> at the end of the template page.\n\nAdd <noinclude></noinclude> to transclude an alternative page from the /doc subpage.\n\nUsage\n\nOn the Template page\nThis is the normal format when used:\n\nTEMPLATE CODE\n<includeonly>Any categories to be inserted into articles by the template</includeonly>\n<noinclude>{{Documentation}}</noinclude>\n\nIf your template is not a completed div or table, you may need to close the tags just before {{Documentation}} is inserted (within the noinclude tags).\n\nA line break right before {{Documentation}} can also be useful as it helps prevent the documentation template ""running into"" previous code.\n\nOn the documentation page\nThe documentation page is usually located on the /doc subpage for a template, but a different page can be specified with the first parameter of the template (see Syntax).\n\nNormally, you will want to write something like the following on the documentation page:\n\n==Description==\nThis template is used to do something.\n\n==Syntax==\nType <code>{{t|templatename}}</code> somewhere.\n\n==Samples==\n<code><nowiki>{{templatename|input}}</nowiki></code> \n\nresults in...\n\n{{templatename|input}}\n\n<includeonly>Any categories for the template itself</includeonly>\n<noinclude>[[Category:Template documentation]]</noinclude>\n\nUse any or all of the above description/syntax/sample output sections. You may also want to add ""see also"" or other sections.\n\nNote that the above example also uses the Template:T template.\n\nCategory:Documentation templatesCategory:Template documentation', metadata={'source': 'Documentation/doc'}), Document(page_content='Description\nA template link with a variable number of parameters (0-20).\n\nSyntax\n \n\nSource\nImproved version not needing t/piece subtemplate developed on Templates wiki see the list of authors. Copied here via CC-By-SA 3.0 license.\n\nExample\n\nCategory:General wiki templates\nCategory:Template documentation', metadata={'source': 'T/doc'}), Document(page_content='\t\n\t\t    \n\t\n\t\t    Aliases\n\t    Relatives\n\t    Affiliation\n        Occupation\n    \n            Biographical information\n        Marital status\n    \tDate of birth\n        Place of birth\n        Date of death\n        Place of death\n    \n            Physical description\n        Species\n        Gender\n        Height\n        Weight\n        Eye color\n\t\n           Appearances\n       Portrayed by\n       Appears in\n       Debut\n    ', metadata={'source': 'Character'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/airbyte_salesforce,Airbyte Salesforce (Deprecated) | ü¶úÔ∏èüîó Langchain,Note: This connector-specific loader is deprecated. Please use,"Airbyte Salesforce (Deprecated)Note: This connector-specific loader is deprecated. Please use\nAirbyteLoader instead.Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases.This loader exposes the Salesforce connector as a document loader,\nallowing you to load various Salesforce objects as documents.Installation‚ÄãFirst, you need to install the airbyte-source-salesforce python\npackage.%pip install --upgrade --quiet  airbyte-source-salesforceExample‚ÄãCheck out the Airbyte documentation\npage for\ndetails about how to configure the reader. The JSON schema the config\nobject should adhere to can be found on Github:\nhttps://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-salesforce/source_salesforce/spec.yaml.The general shape looks like this:{  ""client_id"": ""<oauth client id>"",  ""client_secret"": ""<oauth client secret>"",  ""refresh_token"": ""<oauth refresh token>"",  ""start_date"": ""<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>"",  ""is_sandbox"": False, # set to True if you're using a sandbox environment  ""streams_criteria"": [ # Array of filters for salesforce objects that should be loadable    {""criteria"": ""exacts"", ""value"": ""Account""}, # Exact name of salesforce object    {""criteria"": ""starts with"", ""value"": ""Asset""}, # Prefix of the name    # Other allowed criteria: ends with, contains, starts not with, ends not with, not contains, not exacts  ],}By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader.from langchain_community.document_loaders.airbyte import AirbyteSalesforceLoaderconfig = {    # your salesforce configuration}loader = AirbyteSalesforceLoader(    config=config, stream_name=""asset"")  # check the documentation linked above for a list of all streamsNow you can load documents the usual waydocs = loader.load()As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the\nlazy_load method which returns an iterator instead:docs_iterator = loader.lazy_load()Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader:from langchain.docstore.document import Documentdef handle_record(record, id):    return Document(page_content=record.data[""title""], metadata=record.data)loader = AirbyteSalesforceLoader(    config=config, record_handler=handle_record, stream_name=""asset"")docs = loader.load()Incremental loads‚ÄãSome streams allow incremental loading, this means the source keeps\ntrack of synced records and won‚Äôt load them again. This is useful for\nsources that have a high volume of data and are updated frequently.To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded.last_state = loader.last_state  # store safelyincremental_loader = AirbyteSalesforceLoader(    config=config, stream_name=""asset"", state=last_state)new_docs = incremental_loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/epub,EPub | ü¶úÔ∏èüîó Langchain,EPUB is an e-book file format,"EPubEPUB is an e-book file format\nthat uses the ‚Äú.epub‚Äù file extension. The term is short for electronic\npublication and is sometimes styled ePub. EPUB is supported by many\ne-readers, and compatible software is available for most smartphones,\ntablets, and computers.This covers how to load .epub documents into the Document format that\nwe can use downstream. You‚Äôll need to install the\npandoc package for this loader\nto work.%pip install --upgrade --quiet  pandocfrom langchain_community.document_loaders import UnstructuredEPubLoaderloader = UnstructuredEPubLoader(""winter-sports.epub"")data = loader.load()Retain Elements‚ÄãUnder the hood, Unstructured creates different ‚Äúelements‚Äù for different\nchunks of text. By default we combine those together, but you can easily\nkeep that separation by specifying mode=""elements"".loader = UnstructuredEPubLoader(""winter-sports.epub"", mode=""elements"")data = loader.load()data[0]Document(page_content='The Project Gutenberg eBook of Winter Sports in\nSwitzerland, by E. F. Benson', lookup_str='', metadata={'source': 'winter-sports.epub', 'page_number': 1, 'category': 'Title'}, lookup_index=0)",en,
https://python.langchain.com/docs/integrations/document_loaders/rspace,rspace | ü¶úÔ∏èüîó Langchain,This notebook shows how to use the RSpace document loader to import,"rspaceThis notebook shows how to use the RSpace document loader to import\nresearch notes and documents from RSpace Electronic Lab Notebook into\nLangchain pipelines.To start you‚Äôll need an RSpace account and an API key.You can set up a free account at https://community.researchspace.com\nor use your institutional RSpace.You can get an RSpace API token from your account‚Äôs profile page.%pip install --upgrade --quiet  rspace_clientIt‚Äôs best to store your RSpace API key as an environment variable.RSPACE_API_KEY=<YOUR_KEY>You‚Äôll also need to set the URL of your RSpace installation e.g.RSPACE_URL=https://community.researchspace.comIf you use these exact environment variable names, they will be detected\nautomatically.from langchain_community.document_loaders.rspace import RSpaceLoaderYou can import various items from RSpace:A single RSpace structured or basic document. This will map 1-1 to a\nLangchain document.A folder or noteook. All documents inside the notebook or folder are\nimported as Langchain documents.If you have PDF files in the RSpace Gallery, these can be imported\nindividually as well. Under the hood, Langchain‚Äôs PDF loader will be\nused and this creates one Langchain document per PDF page.## replace these ids with some from your own research notes.## Make sure to use  global ids (with the 2 character prefix). This helps the loader know which API calls to make## to RSpace API.rspace_ids = [""NB1932027"", ""FL1921314"", ""SD1932029"", ""GL1932384""]for rs_id in rspace_ids:    loader = RSpaceLoader(global_id=rs_id)    docs = loader.load()    for doc in docs:        ## the name and ID are added to the 'source' metadata property.        print(doc.metadata)        print(doc.page_content[:500])If you don‚Äôt want to use the environment variables as above, you can\npass these into the RSpaceLoaderloader = RSpaceLoader(    global_id=rs_id, api_key=""MY_API_KEY"", url=""https://my.researchspace.com"")",en,
https://python.langchain.com/docs/integrations/document_loaders/google_cloud_sql_mssql,Google Cloud SQL for SQL server | ü¶úÔ∏èüîó Langchain,Cloud SQL is a fully managed,"Google Cloud SQL for SQL serverCloud SQL is a fully managed\nrelational database service that offers high performance, seamless\nintegration, and impressive scalability. It offers\nMySQL,\nPostgreSQL, and SQL\nServer database engines.\nExtend your database application to build AI-powered experiences\nleveraging Cloud SQL‚Äôs Langchain integrations.This notebook goes over how to use Cloud SQL for SQL\nserver to save, load and\ndelete langchain\ndocuments with\nMSSQLLoader and MSSQLDocumentSaver.Learn more about the package on\nGitHub.Open In ColabBefore You Begin‚ÄãTo run this notebook, you will need to do the following:Create a Google Cloud\nProjectEnable the Cloud SQL Admin\nAPI.Create a Cloud SQL for SQL server\ninstanceCreate a Cloud SQL\ndatabaseAdd an IAM database user to the\ndatabase\n(Optional)After confirmed access to database in the runtime environment of this\nnotebook, filling the following values and run the cell before running\nexample scripts.# @markdown Please fill in the both the Google Cloud region and name of your Cloud SQL instance.REGION = ""us-central1""  # @param {type:""string""}INSTANCE = ""test-instance""  # @param {type:""string""}# @markdown Please fill in user name and password of your Cloud SQL instance.DB_USER = ""sqlserver""  # @param {type:""string""}DB_PASS = ""password""  # @param {type:""string""}# @markdown Please specify a database and a table for demo purpose.DATABASE = ""test""  # @param {type:""string""}TABLE_NAME = ""test-default""  # @param {type:""string""}ü¶úüîó Library Installation‚ÄãThe integration lives in its own langchain-google-cloud-sql-mssql\npackage, so we need to install it.%pip install --upgrade --quiet langchain-google-cloud-sql-mssqlColab only: Uncomment the following cell to restart the kernel or\nuse the button to restart the kernel. For Vertex AI Workbench you can\nrestart the terminal using the button on top.# # Automatically restart kernel after installs so that your environment can access the new packages# import IPython# app = IPython.Application.instance()# app.kernel.do_shutdown(True)üîê Authentication‚ÄãAuthenticate to Google Cloud as the IAM user logged into this notebook\nin order to access your Google Cloud Project.If you are using Colab to run this notebook, use the cell below and\ncontinue.If you are using Vertex AI Workbench, check out the setup\ninstructions\nhere.from google.colab import authauth.authenticate_user()‚òÅ Set Your Google Cloud Project‚ÄãSet your Google Cloud project so that you can leverage Google Cloud\nresources within this notebook.If you don‚Äôt know your project ID, try the following:Run gcloud config list.Run gcloud projects list.See the support page: Locate the project\nID.# @markdown Please fill in the value below with your Google Cloud project ID and then run the cell.PROJECT_ID = ""my-project-id""  # @param {type:""string""}# Set the project id!gcloud config set project {PROJECT_ID}üí° API Enablement‚ÄãThe langchain-google-cloud-sql-mssql package requires that you enable\nthe Cloud SQL Admin\nAPI\nin your Google Cloud Project.# enable Cloud SQL Admin API!gcloud services enable sqladmin.googleapis.comBasic Usage‚ÄãMSSQLEngine Connection Pool‚ÄãBefore saving or loading documents from MSSQL table, we need first\nconfigures a connection pool to Cloud SQL database. The MSSQLEngine\nconfigures a SQLAlchemy connection\npool\nto your Cloud SQL database, enabling successful connections from your\napplication and following industry best practices.To create a MSSQLEngine using MSSQLEngine.from_instance() you need\nto provide only 4 things:project_id : Project ID of the Google Cloud Project where the\nCloud SQL instance is located.region : Region where the Cloud SQL instance is located.instance : The name of the Cloud SQL instance.database : The name of the database to connect to on the Cloud SQL\ninstance.user : Database user to use for built-in database authentication\nand login.password : Database password to use for built-in database\nauthentication and login.from langchain_google_cloud_sql_mssql import MSSQLEngineengine = MSSQLEngine.from_instance(    project_id=PROJECT_ID,    region=REGION,    instance=INSTANCE,    database=DATABASE,    user=DB_USER,    password=DB_PASS,)Initialize a table‚ÄãInitialize a table of default schema via\nMSSQLEngine.init_document_table(<table_name>). Table Columns:page_content (type: text)langchain_metadata (type: JSON)overwrite_existing=True flag means the newly initialized table will\nreplace any existing table of the same name.engine.init_document_table(TABLE_NAME, overwrite_existing=True)Save documents‚ÄãSave langchain documents with\nMSSQLDocumentSaver.add_documents(<documents>). To initialize\nMSSQLDocumentSaver class you need to provide 2 things:engine - An instance of a MSSQLEngine engine.table_name - The name of the table within the Cloud SQL database\nto store langchain documents.from langchain_core.documents import Documentfrom langchain_google_cloud_sql_mssql import MSSQLDocumentSavertest_docs = [    Document(        page_content=""Apple Granny Smith 150 0.99 1"",        metadata={""fruit_id"": 1},    ),    Document(        page_content=""Banana Cavendish 200 0.59 0"",        metadata={""fruit_id"": 2},    ),    Document(        page_content=""Orange Navel 80 1.29 1"",        metadata={""fruit_id"": 3},    ),]saver = MSSQLDocumentSaver(engine=engine, table_name=TABLE_NAME)saver.add_documents(test_docs)Load documents‚ÄãLoad langchain documents with MSSQLLoader.load() or\nMSSQLLoader.lazy_load(). lazy_load returns a generator that only\nqueries database during the iteration. To initialize\nMSSQLDocumentSaver class you need to provide:engine - An instance of a MSSQLEngine engine.table_name - The name of the table within the Cloud SQL database\nto store langchain documents.from langchain_google_cloud_sql_mssql import MSSQLLoaderloader = MSSQLLoader(engine=engine, table_name=TABLE_NAME)docs = loader.lazy_load()for doc in docs:    print(""Loaded documents:"", doc)Load documents via query‚ÄãOther than loading documents from a table, we can also choose to load\ndocuments from a view generated from a SQL query. For example:from langchain_google_cloud_sql_mssql import MSSQLLoaderloader = MSSQLLoader(    engine=engine,    query=f""select * from \""{TABLE_NAME}\"" where JSON_VALUE(langchain_metadata, '$.fruit_id') = 1;"",)onedoc = loader.load()onedocThe view generated from SQL query can have different schema than default\ntable. In such cases, the behavior of MSSQLLoader is the same as loading\nfrom table with non-default schema. Please refer to section Load\ndocuments with customized document page content &\nmetadata.Delete documents‚ÄãDelete a list of langchain documents from MSSQL table with\nMSSQLDocumentSaver.delete(<documents>).For table with default schema (page_content, langchain_metadata), the\ndeletion criteria is:A row should be deleted if there exists a document in the list, such\nthatdocument.page_content equals row[page_content]document.metadata equals row[langchain_metadata]from langchain_google_cloud_sql_mssql import MSSQLLoaderloader = MSSQLLoader(engine=engine, table_name=TABLE_NAME)docs = loader.load()print(""Documents before delete:"", docs)saver.delete(onedoc)print(""Documents after delete:"", loader.load())Advanced Usage‚ÄãLoad documents with customized document page content & metadata‚ÄãFirst we prepare an example table with non-default schema, and populate\nit with some arbitary data.import sqlalchemywith engine.connect() as conn:    conn.execute(sqlalchemy.text(f'DROP TABLE IF EXISTS ""{TABLE_NAME}""'))    conn.commit()    conn.execute(        sqlalchemy.text(            f""""""            IF NOT EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[dbo].[{TABLE_NAME}]') AND type in (N'U'))                BEGIN                    CREATE TABLE [dbo].[{TABLE_NAME}](                        fruit_id INT IDENTITY(1,1) PRIMARY KEY,                        fruit_name VARCHAR(100) NOT NULL,                        variety VARCHAR(50),                        quantity_in_stock INT NOT NULL,                        price_per_unit DECIMAL(6,2) NOT NULL,                        organic BIT NOT NULL                    )                END            """"""        )    )    conn.execute(        sqlalchemy.text(            f""""""            INSERT INTO ""{TABLE_NAME}"" (fruit_name, variety, quantity_in_stock, price_per_unit, organic)            VALUES                ('Apple', 'Granny Smith', 150, 0.99, 1),                ('Banana', 'Cavendish', 200, 0.59, 0),                ('Orange', 'Navel', 80, 1.29, 1);            """"""        )    )    conn.commit()If we still load langchain documents with default parameters of\nMSSQLLoader from this example table, the page_content of loaded\ndocuments will be the first column of the table, and metadata will be\nconsisting of key-value pairs of all the other columns.loader = MSSQLLoader(    engine=engine,    table_name=TABLE_NAME,)loader.load()We can specify the content and metadata we want to load by setting the\ncontent_columns and metadata_columns when initializing the\nMSSQLLoader.content_columns: The columns to write into the page_content of\nthe document.metadata_columns: The columns to write into the metadata of the\ndocument.For example here, the values of columns in content_columns will be\njoined together into a space-separated string, as page_content of\nloaded documents, and metadata of loaded documents will only contain\nkey-value pairs of columns specified in metadata_columns.loader = MSSQLLoader(    engine=engine,    table_name=TABLE_NAME,    content_columns=[        ""variety"",        ""quantity_in_stock"",        ""price_per_unit"",        ""organic"",    ],    metadata_columns=[""fruit_id"", ""fruit_name""],)loader.load()Save document with customized page content & metadata‚ÄãIn order to save langchain document into table with customized metadata\nfields. We need first create such a table via\nMSSQLEngine.init_document_table(), and specify the list of\nmetadata_columns we want it to have. In this example, the created\ntable will have table columns:description (type: text): for storing fruit description.fruit_name (type text): for storing fruit name.organic (type tinyint(1)): to tell if the fruit is organic.other_metadata (type: JSON): for storing other metadata information\nof the fruit.We can use the following parameters with\nMSSQLEngine.init_document_table() to create the table:table_name: The name of the table within the Cloud SQL database to\nstore langchain documents.metadata_columns: A list of sqlalchemy.Column indicating the\nlist of metadata columns we need.content_column: The name of column to store page_content of\nlangchain document. Default: page_content.metadata_json_column: The name of JSON column to store extra\nmetadata of langchain document. Default: langchain_metadata.engine.init_document_table(    TABLE_NAME,    metadata_columns=[        sqlalchemy.Column(            ""fruit_name"",            sqlalchemy.UnicodeText,            primary_key=False,            nullable=True,        ),        sqlalchemy.Column(            ""organic"",            sqlalchemy.Boolean,            primary_key=False,            nullable=True,        ),    ],    content_column=""description"",    metadata_json_column=""other_metadata"",    overwrite_existing=True,)Save documents with MSSQLDocumentSaver.add_documents(<documents>). As\nyou can see in this example,document.page_content will be saved into description column.document.metadata.fruit_name will be saved into fruit_name\ncolumn.document.metadata.organic will be saved into organic column.document.metadata.fruit_id will be saved into other_metadata\ncolumn in JSON format.test_docs = [    Document(        page_content=""Granny Smith 150 0.99"",        metadata={""fruit_id"": 1, ""fruit_name"": ""Apple"", ""organic"": 1},    ),]saver = MSSQLDocumentSaver(    engine=engine,    table_name=TABLE_NAME,    content_column=""description"",    metadata_json_column=""other_metadata"",)saver.add_documents(test_docs)with engine.connect() as conn:    result = conn.execute(sqlalchemy.text(f'select * from ""{TABLE_NAME}"";'))    print(result.keys())    print(result.fetchall())Delete documents with customized page content & metadata‚ÄãWe can also delete documents from table with customized metadata columns\nvia MSSQLDocumentSaver.delete(<documents>). The deletion criteria is:A row should be deleted if there exists a document in the list, such\nthatdocument.page_content equals row[page_content]For every metadata field k in document.metadatadocument.metadata[k] equals row[k] or document.metadata[k]\nequals row[langchain_metadata][k]There no extra metadata field presents in row but not in\ndocument.metadata.loader = MSSQLLoader(engine=engine, table_name=TABLE_NAME)docs = loader.load()print(""Documents before delete:"", docs)saver.delete(docs)print(""Documents after delete:"", loader.load())",en,
https://python.langchain.com/docs/integrations/document_loaders/facebook_chat,Facebook Chat | ü¶úÔ∏èüîó Langchain,Messenger) is an,"Facebook ChatMessenger is an\nAmerican proprietary instant messaging app and platform developed by\nMeta Platforms. Originally developed as Facebook Chat in 2008, the\ncompany revamped its messaging service in 2010.This notebook covers how to load data from the Facebook\nChats into a\nformat that can be ingested into LangChain.# pip install pandasfrom langchain_community.document_loaders import FacebookChatLoaderloader = FacebookChatLoader(""example_data/facebook_chat.json"")loader.load()[Document(page_content='User 2 on 2023-02-05 03:46:11: Bye!\n\nUser 1 on 2023-02-05 03:43:55: Oh no worries! Bye\n\nUser 2 on 2023-02-05 03:24:37: No Im sorry it was my mistake, the blue one is not for sale\n\nUser 1 on 2023-02-05 03:05:40: I thought you were selling the blue one!\n\nUser 1 on 2023-02-05 03:05:09: Im not interested in this bag. Im interested in the blue one!\n\nUser 2 on 2023-02-05 03:04:28: Here is $129\n\nUser 2 on 2023-02-05 03:04:05: Online is at least $100\n\nUser 1 on 2023-02-05 02:59:59: How much do you want?\n\nUser 2 on 2023-02-04 22:17:56: Goodmorning! $50 is too low.\n\nUser 1 on 2023-02-04 14:17:02: Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\n\n', metadata={'source': 'example_data/facebook_chat.json'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/tsv,TSV | ü¶úÔ∏èüîó Langchain,A [tab-separated values,"TSVA tab-separated values\n(TSV) file is a\nsimple, text-based file format for storing tabular data.[3] Records\nare separated by newlines, and values within a record are separated by\ntab characters.UnstructuredTSVLoader‚ÄãYou can also load the table using the UnstructuredTSVLoader. One\nadvantage of using UnstructuredTSVLoader is that if you use it in\n""elements"" mode, an HTML representation of the table will be available\nin the metadata.from langchain_community.document_loaders.tsv import UnstructuredTSVLoaderloader = UnstructuredTSVLoader(    file_path=""example_data/mlb_teams_2012.csv"", mode=""elements"")docs = loader.load()print(docs[0].metadata[""text_as_html""])<table border=""1"" class=""dataframe"">  <tbody>    <tr>      <td>Nationals,     81.34, 98</td>    </tr>    <tr>      <td>Reds,          82.20, 97</td>    </tr>    <tr>      <td>Yankees,      197.96, 95</td>    </tr>    <tr>      <td>Giants,       117.62, 94</td>    </tr>    <tr>      <td>Braves,        83.31, 94</td>    </tr>    <tr>      <td>Athletics,     55.37, 94</td>    </tr>    <tr>      <td>Rangers,      120.51, 93</td>    </tr>    <tr>      <td>Orioles,       81.43, 93</td>    </tr>    <tr>      <td>Rays,          64.17, 90</td>    </tr>    <tr>      <td>Angels,       154.49, 89</td>    </tr>    <tr>      <td>Tigers,       132.30, 88</td>    </tr>    <tr>      <td>Cardinals,    110.30, 88</td>    </tr>    <tr>      <td>Dodgers,       95.14, 86</td>    </tr>    <tr>      <td>White Sox,     96.92, 85</td>    </tr>    <tr>      <td>Brewers,       97.65, 83</td>    </tr>    <tr>      <td>Phillies,     174.54, 81</td>    </tr>    <tr>      <td>Diamondbacks,  74.28, 81</td>    </tr>    <tr>      <td>Pirates,       63.43, 79</td>    </tr>    <tr>      <td>Padres,        55.24, 76</td>    </tr>    <tr>      <td>Mariners,      81.97, 75</td>    </tr>    <tr>      <td>Mets,          93.35, 74</td>    </tr>    <tr>      <td>Blue Jays,     75.48, 73</td>    </tr>    <tr>      <td>Royals,        60.91, 72</td>    </tr>    <tr>      <td>Marlins,      118.07, 69</td>    </tr>    <tr>      <td>Red Sox,      173.18, 69</td>    </tr>    <tr>      <td>Indians,       78.43, 68</td>    </tr>    <tr>      <td>Twins,         94.08, 66</td>    </tr>    <tr>      <td>Rockies,       78.06, 64</td>    </tr>    <tr>      <td>Cubs,          88.19, 61</td>    </tr>    <tr>      <td>Astros,        60.65, 55</td>    </tr>  </tbody></table>",en,
https://python.langchain.com/docs/integrations/document_loaders/browserless,Browserless | ü¶úÔ∏èüîó Langchain,Browserless is a service that allows you to run headless Chrome,"BrowserlessBrowserless is a service that allows you to run headless Chrome\ninstances in the cloud. It‚Äôs a great way to run browser-based automation\nat scale without having to worry about managing your own infrastructure.To use Browserless as a document loader, initialize a\nBrowserlessLoader instance as shown in this notebook. Note that by\ndefault, BrowserlessLoader returns the innerText of the page‚Äôs\nbody element. To disable this and get the raw HTML, set text_content\nto False.from langchain_community.document_loaders import BrowserlessLoaderBROWSERLESS_API_TOKEN = ""YOUR_BROWSERLESS_API_TOKEN""loader = BrowserlessLoader(    api_token=BROWSERLESS_API_TOKEN,    urls=[        ""https://en.wikipedia.org/wiki/Document_classification"",    ],    text_content=True,)documents = loader.load()print(documents[0].page_content[:1000])Jump to contentMain menuSearchCreate accountLog inPersonal toolsToggle the table of contentsDocument classification17 languagesArticleTalkReadEditView historyToolsFrom Wikipedia, the free encyclopediaDocument classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done ""manually"" (or ""intellectually"") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.Do",en,
https://python.langchain.com/docs/integrations/document_loaders/huawei_obs_file,Huawei OBS File | ü¶úÔ∏èüîó Langchain,The following code demonstrates how to load an object from the Huawei,"Huawei OBS FileThe following code demonstrates how to load an object from the Huawei\nOBS (Object Storage Service) as document.# Install the required package# pip install esdk-obs-pythonfrom langchain_community.document_loaders.obs_file import OBSFileLoaderendpoint = ""your-endpoint""from obs import ObsClientobs_client = ObsClient(    access_key_id=""your-access-key"",    secret_access_key=""your-secret-key"",    server=endpoint,)loader = OBSFileLoader(""your-bucket-name"", ""your-object-key"", client=obs_client)loader.load()Each Loader with Separate Authentication Information‚ÄãIf you don‚Äôt need to reuse OBS connections between different loaders,\nyou can directly configure the config. The loader will use the config\ninformation to initialize its own OBS client.# Configure your access credentials\nconfig = {""ak"": ""your-access-key"", ""sk"": ""your-secret-key""}loader = OBSFileLoader(    ""your-bucket-name"", ""your-object-key"", endpoint=endpoint, config=config)loader.load()Get Authentication Information from ECS‚ÄãIf your langchain is deployed on Huawei Cloud ECS and Agency is set\nup,\nthe loader can directly get the security token from ECS without needing\naccess key and secret key.config = {""get_token_from_ecs"": True}loader = OBSFileLoader(    ""your-bucket-name"", ""your-object-key"", endpoint=endpoint, config=config)loader.load()Access a Publicly Accessible Object‚ÄãIf the object you want to access allows anonymous user access (anonymous\nusers have GetObject permission), you can directly load the object\nwithout configuring the config parameter.loader = OBSFileLoader(""your-bucket-name"", ""your-object-key"", endpoint=endpoint)loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/subtitle,Subtitle | ü¶úÔ∏èüîó Langchain,[The SubRip file,"SubtitleThe SubRip file\nformat is\ndescribed on the Matroska multimedia container format website as\n‚Äúperhaps the most basic of all subtitle formats.‚Äù\nSubRip (SubRip Text) files are named with the extension .srt, and\ncontain formatted lines of plain text in groups separated by a blank\nline. Subtitles are numbered sequentially, starting at 1. The timecode\nformat used is hours:minutes:seconds,milliseconds with time units\nfixed to two zero-padded digits and fractions fixed to three\nzero-padded digits (00:00:00,000). The fractional separator used is\nthe comma, since the program was written in France.How to load data from subtitle (.srt) filesPlease, download the example .srt file from\nhere.%pip install --upgrade --quiet  pysrtfrom langchain_community.document_loaders import SRTLoaderloader = SRTLoader(    ""example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt"")docs = loader.load()docs[0].page_content[:100]'<i>Corruption discovered\nat the core of the Banking Clan!</i> <i>Reunited, Rush Clovis\nand Senator A'",en,
https://python.langchain.com/docs/integrations/document_loaders/csv,CSV | ü¶úÔ∏èüîó Langchain,A [comma-separated values,"CSVA comma-separated values\n(CSV) file is a\ndelimited text file that uses a comma to separate values. Each line of\nthe file is a data record. Each record consists of one or more fields,\nseparated by commas.Load csv data\nwith a single row per document.from langchain_community.document_loaders.csv_loader import CSVLoaderloader = CSVLoader(file_path=""./example_data/mlb_teams_2012.csv"")data = loader.load()print(data)[Document(page_content='Team: Nationals\n""Payroll (millions)"": 81.34\n""Wins"": 98', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 0}, lookup_index=0), Document(page_content='Team: Reds\n""Payroll (millions)"": 82.20\n""Wins"": 97', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 1}, lookup_index=0), Document(page_content='Team: Yankees\n""Payroll (millions)"": 197.96\n""Wins"": 95', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 2}, lookup_index=0), Document(page_content='Team: Giants\n""Payroll (millions)"": 117.62\n""Wins"": 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 3}, lookup_index=0), Document(page_content='Team: Braves\n""Payroll (millions)"": 83.31\n""Wins"": 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 4}, lookup_index=0), Document(page_content='Team: Athletics\n""Payroll (millions)"": 55.37\n""Wins"": 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 5}, lookup_index=0), Document(page_content='Team: Rangers\n""Payroll (millions)"": 120.51\n""Wins"": 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 6}, lookup_index=0), Document(page_content='Team: Orioles\n""Payroll (millions)"": 81.43\n""Wins"": 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 7}, lookup_index=0), Document(page_content='Team: Rays\n""Payroll (millions)"": 64.17\n""Wins"": 90', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 8}, lookup_index=0), Document(page_content='Team: Angels\n""Payroll (millions)"": 154.49\n""Wins"": 89', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 9}, lookup_index=0), Document(page_content='Team: Tigers\n""Payroll (millions)"": 132.30\n""Wins"": 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 10}, lookup_index=0), Document(page_content='Team: Cardinals\n""Payroll (millions)"": 110.30\n""Wins"": 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 11}, lookup_index=0), Document(page_content='Team: Dodgers\n""Payroll (millions)"": 95.14\n""Wins"": 86', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 12}, lookup_index=0), Document(page_content='Team: White Sox\n""Payroll (millions)"": 96.92\n""Wins"": 85', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 13}, lookup_index=0), Document(page_content='Team: Brewers\n""Payroll (millions)"": 97.65\n""Wins"": 83', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 14}, lookup_index=0), Document(page_content='Team: Phillies\n""Payroll (millions)"": 174.54\n""Wins"": 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 15}, lookup_index=0), Document(page_content='Team: Diamondbacks\n""Payroll (millions)"": 74.28\n""Wins"": 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 16}, lookup_index=0), Document(page_content='Team: Pirates\n""Payroll (millions)"": 63.43\n""Wins"": 79', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 17}, lookup_index=0), Document(page_content='Team: Padres\n""Payroll (millions)"": 55.24\n""Wins"": 76', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 18}, lookup_index=0), Document(page_content='Team: Mariners\n""Payroll (millions)"": 81.97\n""Wins"": 75', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 19}, lookup_index=0), Document(page_content='Team: Mets\n""Payroll (millions)"": 93.35\n""Wins"": 74', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 20}, lookup_index=0), Document(page_content='Team: Blue Jays\n""Payroll (millions)"": 75.48\n""Wins"": 73', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 21}, lookup_index=0), Document(page_content='Team: Royals\n""Payroll (millions)"": 60.91\n""Wins"": 72', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 22}, lookup_index=0), Document(page_content='Team: Marlins\n""Payroll (millions)"": 118.07\n""Wins"": 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 23}, lookup_index=0), Document(page_content='Team: Red Sox\n""Payroll (millions)"": 173.18\n""Wins"": 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 24}, lookup_index=0), Document(page_content='Team: Indians\n""Payroll (millions)"": 78.43\n""Wins"": 68', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 25}, lookup_index=0), Document(page_content='Team: Twins\n""Payroll (millions)"": 94.08\n""Wins"": 66', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 26}, lookup_index=0), Document(page_content='Team: Rockies\n""Payroll (millions)"": 78.06\n""Wins"": 64', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 27}, lookup_index=0), Document(page_content='Team: Cubs\n""Payroll (millions)"": 88.19\n""Wins"": 61', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 28}, lookup_index=0), Document(page_content='Team: Astros\n""Payroll (millions)"": 60.65\n""Wins"": 55', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 29}, lookup_index=0)]Customizing the csv parsing and loading‚ÄãSee the csv module\ndocumentation for more information of what csv args are supported.loader = CSVLoader(    file_path=""./example_data/mlb_teams_2012.csv"",    csv_args={        ""delimiter"": "","",        ""quotechar"": '""',        ""fieldnames"": [""MLB Team"", ""Payroll in millions"", ""Wins""],    },)data = loader.load()print(data)[Document(page_content='MLB Team: Team\nPayroll in millions: ""Payroll (millions)""\nWins: ""Wins""', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 0}, lookup_index=0), Document(page_content='MLB Team: Nationals\nPayroll in millions: 81.34\nWins: 98', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 1}, lookup_index=0), Document(page_content='MLB Team: Reds\nPayroll in millions: 82.20\nWins: 97', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 2}, lookup_index=0), Document(page_content='MLB Team: Yankees\nPayroll in millions: 197.96\nWins: 95', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 3}, lookup_index=0), Document(page_content='MLB Team: Giants\nPayroll in millions: 117.62\nWins: 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 4}, lookup_index=0), Document(page_content='MLB Team: Braves\nPayroll in millions: 83.31\nWins: 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 5}, lookup_index=0), Document(page_content='MLB Team: Athletics\nPayroll in millions: 55.37\nWins: 94', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 6}, lookup_index=0), Document(page_content='MLB Team: Rangers\nPayroll in millions: 120.51\nWins: 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 7}, lookup_index=0), Document(page_content='MLB Team: Orioles\nPayroll in millions: 81.43\nWins: 93', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 8}, lookup_index=0), Document(page_content='MLB Team: Rays\nPayroll in millions: 64.17\nWins: 90', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 9}, lookup_index=0), Document(page_content='MLB Team: Angels\nPayroll in millions: 154.49\nWins: 89', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 10}, lookup_index=0), Document(page_content='MLB Team: Tigers\nPayroll in millions: 132.30\nWins: 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 11}, lookup_index=0), Document(page_content='MLB Team: Cardinals\nPayroll in millions: 110.30\nWins: 88', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 12}, lookup_index=0), Document(page_content='MLB Team: Dodgers\nPayroll in millions: 95.14\nWins: 86', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 13}, lookup_index=0), Document(page_content='MLB Team: White Sox\nPayroll in millions: 96.92\nWins: 85', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 14}, lookup_index=0), Document(page_content='MLB Team: Brewers\nPayroll in millions: 97.65\nWins: 83', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 15}, lookup_index=0), Document(page_content='MLB Team: Phillies\nPayroll in millions: 174.54\nWins: 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 16}, lookup_index=0), Document(page_content='MLB Team: Diamondbacks\nPayroll in millions: 74.28\nWins: 81', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 17}, lookup_index=0), Document(page_content='MLB Team: Pirates\nPayroll in millions: 63.43\nWins: 79', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 18}, lookup_index=0), Document(page_content='MLB Team: Padres\nPayroll in millions: 55.24\nWins: 76', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 19}, lookup_index=0), Document(page_content='MLB Team: Mariners\nPayroll in millions: 81.97\nWins: 75', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 20}, lookup_index=0), Document(page_content='MLB Team: Mets\nPayroll in millions: 93.35\nWins: 74', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 21}, lookup_index=0), Document(page_content='MLB Team: Blue Jays\nPayroll in millions: 75.48\nWins: 73', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 22}, lookup_index=0), Document(page_content='MLB Team: Royals\nPayroll in millions: 60.91\nWins: 72', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 23}, lookup_index=0), Document(page_content='MLB Team: Marlins\nPayroll in millions: 118.07\nWins: 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 24}, lookup_index=0), Document(page_content='MLB Team: Red Sox\nPayroll in millions: 173.18\nWins: 69', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 25}, lookup_index=0), Document(page_content='MLB Team: Indians\nPayroll in millions: 78.43\nWins: 68', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 26}, lookup_index=0), Document(page_content='MLB Team: Twins\nPayroll in millions: 94.08\nWins: 66', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 27}, lookup_index=0), Document(page_content='MLB Team: Rockies\nPayroll in millions: 78.06\nWins: 64', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 28}, lookup_index=0), Document(page_content='MLB Team: Cubs\nPayroll in millions: 88.19\nWins: 61', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 29}, lookup_index=0), Document(page_content='MLB Team: Astros\nPayroll in millions: 60.65\nWins: 55', lookup_str='', metadata={'source': './example_data/mlb_teams_2012.csv', 'row': 30}, lookup_index=0)]Specify a column to identify the document source‚ÄãUse the source_column argument to specify a source for the document\ncreated from each row. Otherwise file_path will be used as the source\nfor all documents created from the CSV file.This is useful when using documents loaded from CSV files for chains\nthat answer questions using sources.loader = CSVLoader(file_path=""./example_data/mlb_teams_2012.csv"", source_column=""Team"")data = loader.load()print(data)[Document(page_content='Team: Nationals\n""Payroll (millions)"": 81.34\n""Wins"": 98', lookup_str='', metadata={'source': 'Nationals', 'row': 0}, lookup_index=0), Document(page_content='Team: Reds\n""Payroll (millions)"": 82.20\n""Wins"": 97', lookup_str='', metadata={'source': 'Reds', 'row': 1}, lookup_index=0), Document(page_content='Team: Yankees\n""Payroll (millions)"": 197.96\n""Wins"": 95', lookup_str='', metadata={'source': 'Yankees', 'row': 2}, lookup_index=0), Document(page_content='Team: Giants\n""Payroll (millions)"": 117.62\n""Wins"": 94', lookup_str='', metadata={'source': 'Giants', 'row': 3}, lookup_index=0), Document(page_content='Team: Braves\n""Payroll (millions)"": 83.31\n""Wins"": 94', lookup_str='', metadata={'source': 'Braves', 'row': 4}, lookup_index=0), Document(page_content='Team: Athletics\n""Payroll (millions)"": 55.37\n""Wins"": 94', lookup_str='', metadata={'source': 'Athletics', 'row': 5}, lookup_index=0), Document(page_content='Team: Rangers\n""Payroll (millions)"": 120.51\n""Wins"": 93', lookup_str='', metadata={'source': 'Rangers', 'row': 6}, lookup_index=0), Document(page_content='Team: Orioles\n""Payroll (millions)"": 81.43\n""Wins"": 93', lookup_str='', metadata={'source': 'Orioles', 'row': 7}, lookup_index=0), Document(page_content='Team: Rays\n""Payroll (millions)"": 64.17\n""Wins"": 90', lookup_str='', metadata={'source': 'Rays', 'row': 8}, lookup_index=0), Document(page_content='Team: Angels\n""Payroll (millions)"": 154.49\n""Wins"": 89', lookup_str='', metadata={'source': 'Angels', 'row': 9}, lookup_index=0), Document(page_content='Team: Tigers\n""Payroll (millions)"": 132.30\n""Wins"": 88', lookup_str='', metadata={'source': 'Tigers', 'row': 10}, lookup_index=0), Document(page_content='Team: Cardinals\n""Payroll (millions)"": 110.30\n""Wins"": 88', lookup_str='', metadata={'source': 'Cardinals', 'row': 11}, lookup_index=0), Document(page_content='Team: Dodgers\n""Payroll (millions)"": 95.14\n""Wins"": 86', lookup_str='', metadata={'source': 'Dodgers', 'row': 12}, lookup_index=0), Document(page_content='Team: White Sox\n""Payroll (millions)"": 96.92\n""Wins"": 85', lookup_str='', metadata={'source': 'White Sox', 'row': 13}, lookup_index=0), Document(page_content='Team: Brewers\n""Payroll (millions)"": 97.65\n""Wins"": 83', lookup_str='', metadata={'source': 'Brewers', 'row': 14}, lookup_index=0), Document(page_content='Team: Phillies\n""Payroll (millions)"": 174.54\n""Wins"": 81', lookup_str='', metadata={'source': 'Phillies', 'row': 15}, lookup_index=0), Document(page_content='Team: Diamondbacks\n""Payroll (millions)"": 74.28\n""Wins"": 81', lookup_str='', metadata={'source': 'Diamondbacks', 'row': 16}, lookup_index=0), Document(page_content='Team: Pirates\n""Payroll (millions)"": 63.43\n""Wins"": 79', lookup_str='', metadata={'source': 'Pirates', 'row': 17}, lookup_index=0), Document(page_content='Team: Padres\n""Payroll (millions)"": 55.24\n""Wins"": 76', lookup_str='', metadata={'source': 'Padres', 'row': 18}, lookup_index=0), Document(page_content='Team: Mariners\n""Payroll (millions)"": 81.97\n""Wins"": 75', lookup_str='', metadata={'source': 'Mariners', 'row': 19}, lookup_index=0), Document(page_content='Team: Mets\n""Payroll (millions)"": 93.35\n""Wins"": 74', lookup_str='', metadata={'source': 'Mets', 'row': 20}, lookup_index=0), Document(page_content='Team: Blue Jays\n""Payroll (millions)"": 75.48\n""Wins"": 73', lookup_str='', metadata={'source': 'Blue Jays', 'row': 21}, lookup_index=0), Document(page_content='Team: Royals\n""Payroll (millions)"": 60.91\n""Wins"": 72', lookup_str='', metadata={'source': 'Royals', 'row': 22}, lookup_index=0), Document(page_content='Team: Marlins\n""Payroll (millions)"": 118.07\n""Wins"": 69', lookup_str='', metadata={'source': 'Marlins', 'row': 23}, lookup_index=0), Document(page_content='Team: Red Sox\n""Payroll (millions)"": 173.18\n""Wins"": 69', lookup_str='', metadata={'source': 'Red Sox', 'row': 24}, lookup_index=0), Document(page_content='Team: Indians\n""Payroll (millions)"": 78.43\n""Wins"": 68', lookup_str='', metadata={'source': 'Indians', 'row': 25}, lookup_index=0), Document(page_content='Team: Twins\n""Payroll (millions)"": 94.08\n""Wins"": 66', lookup_str='', metadata={'source': 'Twins', 'row': 26}, lookup_index=0), Document(page_content='Team: Rockies\n""Payroll (millions)"": 78.06\n""Wins"": 64', lookup_str='', metadata={'source': 'Rockies', 'row': 27}, lookup_index=0), Document(page_content='Team: Cubs\n""Payroll (millions)"": 88.19\n""Wins"": 61', lookup_str='', metadata={'source': 'Cubs', 'row': 28}, lookup_index=0), Document(page_content='Team: Astros\n""Payroll (millions)"": 60.65\n""Wins"": 55', lookup_str='', metadata={'source': 'Astros', 'row': 29}, lookup_index=0)]UnstructuredCSVLoader‚ÄãYou can also load the table using the UnstructuredCSVLoader. One\nadvantage of using UnstructuredCSVLoader is that if you use it in\n""elements"" mode, an HTML representation of the table will be available\nin the metadata.from langchain_community.document_loaders.csv_loader import UnstructuredCSVLoaderloader = UnstructuredCSVLoader(    file_path=""example_data/mlb_teams_2012.csv"", mode=""elements"")docs = loader.load()print(docs[0].metadata[""text_as_html""])<table border=""1"" class=""dataframe"">  <tbody>    <tr>      <td>Nationals</td>      <td>81.34</td>      <td>98</td>    </tr>    <tr>      <td>Reds</td>      <td>82.20</td>      <td>97</td>    </tr>    <tr>      <td>Yankees</td>      <td>197.96</td>      <td>95</td>    </tr>    <tr>      <td>Giants</td>      <td>117.62</td>      <td>94</td>    </tr>    <tr>      <td>Braves</td>      <td>83.31</td>      <td>94</td>    </tr>    <tr>      <td>Athletics</td>      <td>55.37</td>      <td>94</td>    </tr>    <tr>      <td>Rangers</td>      <td>120.51</td>      <td>93</td>    </tr>    <tr>      <td>Orioles</td>      <td>81.43</td>      <td>93</td>    </tr>    <tr>      <td>Rays</td>      <td>64.17</td>      <td>90</td>    </tr>    <tr>      <td>Angels</td>      <td>154.49</td>      <td>89</td>    </tr>    <tr>      <td>Tigers</td>      <td>132.30</td>      <td>88</td>    </tr>    <tr>      <td>Cardinals</td>      <td>110.30</td>      <td>88</td>    </tr>    <tr>      <td>Dodgers</td>      <td>95.14</td>      <td>86</td>    </tr>    <tr>      <td>White Sox</td>      <td>96.92</td>      <td>85</td>    </tr>    <tr>      <td>Brewers</td>      <td>97.65</td>      <td>83</td>    </tr>    <tr>      <td>Phillies</td>      <td>174.54</td>      <td>81</td>    </tr>    <tr>      <td>Diamondbacks</td>      <td>74.28</td>      <td>81</td>    </tr>    <tr>      <td>Pirates</td>      <td>63.43</td>      <td>79</td>    </tr>    <tr>      <td>Padres</td>      <td>55.24</td>      <td>76</td>    </tr>    <tr>      <td>Mariners</td>      <td>81.97</td>      <td>75</td>    </tr>    <tr>      <td>Mets</td>      <td>93.35</td>      <td>74</td>    </tr>    <tr>      <td>Blue Jays</td>      <td>75.48</td>      <td>73</td>    </tr>    <tr>      <td>Royals</td>      <td>60.91</td>      <td>72</td>    </tr>    <tr>      <td>Marlins</td>      <td>118.07</td>      <td>69</td>    </tr>    <tr>      <td>Red Sox</td>      <td>173.18</td>      <td>69</td>    </tr>    <tr>      <td>Indians</td>      <td>78.43</td>      <td>68</td>    </tr>    <tr>      <td>Twins</td>      <td>94.08</td>      <td>66</td>    </tr>    <tr>      <td>Rockies</td>      <td>78.06</td>      <td>64</td>    </tr>    <tr>      <td>Cubs</td>      <td>88.19</td>      <td>61</td>    </tr>    <tr>      <td>Astros</td>      <td>60.65</td>      <td>55</td>    </tr>  </tbody></table>",en,
https://python.langchain.com/docs/integrations/document_loaders/athena,Athena | ü¶úÔ∏èüîó Langchain,This notebooks goes over how to load documents from AWS Athena,"AthenaThis notebooks goes over how to load documents from AWS Athena! pip install boto3from langchain_community.document_loaders.athena import AthenaLoaderdatabase_name = ""my_database""s3_output_path = ""s3://my_bucket/query_results/""query = ""SELECT * FROM my_table""profile_name = ""my_profile""loader = AthenaLoader(    query=query,    database=database_name,    s3_output_uri=s3_output_path,    profile_name=profile_name,)documents = loader.load()print(documents)Example with metadata columnsdatabase_name = ""my_database""s3_output_path = ""s3://my_bucket/query_results/""query = ""SELECT * FROM my_table""profile_name = ""my_profile""metadata_columns = [""_row"", ""_created_at""]loader = AthenaLoader(    query=query,    database=database_name,    s3_output_uri=s3_output_path,    profile_name=profile_name,    metadata_columns=metadata_columns,)documents = loader.load()print(documents)",en,
https://python.langchain.com/docs/integrations/document_loaders/acreom,acreom | ü¶úÔ∏èüîó Langchain,acreom is a dev-first knowledge base with tasks,"acreomacreom is a dev-first knowledge base with tasks\nrunning on local markdown files.Below is an example on how to load a local acreom vault into Langchain.\nAs the local vault in acreom is a folder of plain text .md files, the\nloader requires the path to the directory.Vault files may contain some metadata which is stored as a YAML header.\nThese values will be added to the document‚Äôs metadata if\ncollect_metadata is set to true.from langchain_community.document_loaders import AcreomLoaderloader = AcreomLoader(""<path-to-acreom-vault>"", collect_metadata=False)docs = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/source_code,Source Code | ü¶úÔ∏èüîó Langchain,This notebook covers how to load source code files using a special,"Source CodeThis notebook covers how to load source code files using a special\napproach with language parsing: each top-level function and class in the\ncode is loaded into separate documents. Any remaining code top-level\ncode outside the already loaded functions and classes will be loaded\ninto a separate document.This approach can potentially improve the accuracy of QA models over\nsource code.The supported languages for code parsing are:C (*)C++ (*)C# (*)COBOLGo (*)Java (*)JavaScript (requires package esprima)Kotlin (*)Lua (*)Perl (*)PythonRuby (*)Rust (*)Scala (*)TypeScript (*)Items marked with (*) require the packages tree_sitter and\ntree_sitter_languages. It is straightforward to add support for\nadditional languages using tree_sitter, although this currently\nrequires modifying LangChain.The language used for parsing can be configured, along with the minimum\nnumber of lines required to activate the splitting based on syntax.If a language is not explicitly specified, LanguageParser will infer\none from filename extensions, if present.%pip install -qU esprima esprima tree_sitter tree_sitter_languagesimport warningswarnings.filterwarnings(""ignore"")from pprint import pprintfrom langchain_community.document_loaders.generic import GenericLoaderfrom langchain_community.document_loaders.parsers import LanguageParserfrom langchain_text_splitters import Languageloader = GenericLoader.from_filesystem(    ""./example_data/source_code"",    glob=""*"",    suffixes=["".py"", "".js""],    parser=LanguageParser(),)docs = loader.load()len(docs)6for document in docs:    pprint(document.metadata){'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>, 'source': 'example_data/source_code/example.py'}{'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>, 'source': 'example_data/source_code/example.py'}{'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>, 'source': 'example_data/source_code/example.py'}{'content_type': 'functions_classes', 'language': <Language.JS: 'js'>, 'source': 'example_data/source_code/example.js'}{'content_type': 'functions_classes', 'language': <Language.JS: 'js'>, 'source': 'example_data/source_code/example.js'}{'content_type': 'simplified_code', 'language': <Language.JS: 'js'>, 'source': 'example_data/source_code/example.js'}print(""\n\n--8<--\n\n"".join([document.page_content for document in docs]))class MyClass:    def __init__(self, name):        self.name = name    def greet(self):        print(f""Hello, {self.name}!"")--8<--def main():    name = input(""Enter your name: "")    obj = MyClass(name)    obj.greet()--8<--# Code for: class MyClass:# Code for: def main():if __name__ == ""__main__"":    main()--8<--class MyClass {  constructor(name) {    this.name = name;  }  greet() {    console.log(`Hello, ${this.name}!`);  }}--8<--function main() {  const name = prompt(""Enter your name:"");  const obj = new MyClass(name);  obj.greet();}--8<--// Code for: class MyClass {// Code for: function main() {main();The parser can be disabled for small files.The parameter parser_threshold indicates the minimum number of lines\nthat the source code file must have to be segmented using the parser.loader = GenericLoader.from_filesystem(    ""./example_data/source_code"",    glob=""*"",    suffixes=["".py""],    parser=LanguageParser(language=Language.PYTHON, parser_threshold=1000),)docs = loader.load()len(docs)1print(docs[0].page_content)class MyClass:    def __init__(self, name):        self.name = name    def greet(self):        print(f""Hello, {self.name}!"")def main():    name = input(""Enter your name: "")    obj = MyClass(name)    obj.greet()if __name__ == ""__main__"":    main()Splitting‚ÄãAdditional splitting could be needed for those functions, classes, or\nscripts that are too big.loader = GenericLoader.from_filesystem(    ""./example_data/source_code"",    glob=""*"",    suffixes=["".js""],    parser=LanguageParser(language=Language.JS),)docs = loader.load()from langchain_text_splitters import (    Language,    RecursiveCharacterTextSplitter,)js_splitter = RecursiveCharacterTextSplitter.from_language(    language=Language.JS, chunk_size=60, chunk_overlap=0)result = js_splitter.split_documents(docs)len(result)7print(""\n\n--8<--\n\n"".join([document.page_content for document in result]))class MyClass {  constructor(name) {    this.name = name;--8<--}--8<--greet() {    console.log(`Hello, ${this.name}!`);  }}--8<--function main() {  const name = prompt(""Enter your name:"");--8<--const obj = new MyClass(name);  obj.greet();}--8<--// Code for: class MyClass {// Code for: function main() {--8<--main();Adding Languages using Tree-sitter Template‚ÄãExpanding language support using the Tree-Sitter template involves a few\nessential steps:Creating a New Language File:Begin by creating a new file in the designated directory\n(langchain/libs/community/langchain_community/document_loaders/parsers/language).Model this file based on the structure and parsing logic of\nexisting language files like cpp.py.You will also need to create a file in the langchain directory\n(langchain/libs/langchain/langchain/document_loaders/parsers/language).Parsing Language Specifics:Mimic the structure used in the cpp.py file, adapting it\nto suit the language you are incorporating.The primary alteration involves adjusting the chunk query array\nto suit the syntax and structure of the language you are\nparsing.Testing the Language Parser:For thorough validation, generate a test file specific to the\nnew language. Create test_language.py in the designated\ndirectory(langchain/libs/community/tests/unit_tests/document_loaders/parsers/language).Follow the example set by test_cpp.py to establish\nfundamental tests for the parsed elements in the new language.Integration into the Parser and Text Splitter:Incorporate your new language within the\nlanguage_parser.py file. Ensure to update\nLANGUAGE_EXTENSIONS and LANGUAGE_SEGMENTERS along with the\ndocstring for LanguageParser to recognize and handle the added\nlanguage.Also, confirm that your language is included in\ntext_splitter.py in class Language for proper parsing.By following these steps and ensuring comprehensive testing and\nintegration, you‚Äôll successfully extend language support using the\nTree-Sitter template.Best of luck!",en,
https://python.langchain.com/docs/integrations/document_loaders/blackboard,Blackboard | ü¶úÔ∏èüîó Langchain,Blackboard Learn,"BlackboardBlackboard Learn\n(previously the Blackboard Learning Management System) is a web-based\nvirtual learning environment and learning management system developed\nby Blackboard Inc.¬†The software features course management,\ncustomizable open architecture, and scalable design that allows\nintegration with student information systems and authentication\nprotocols. It may be installed on local servers, hosted by\nBlackboard ASP Solutions, or provided as Software as a Service\nhosted on Amazon Web Services. Its main purposes are stated to include\nthe addition of online elements to courses traditionally delivered\nface-to-face and development of completely online courses with few or\nno face-to-face meetingsThis covers how to load data from a Blackboard\nLearn\ninstance.This loader is not compatible with all Blackboard courses. It is only\ncompatible with courses that use the new Blackboard interface. To use\nthis loader, you must have the BbRouter cookie. You can get this cookie\nby logging into the course and then copying the value of the BbRouter\ncookie from the browser‚Äôs developer tools.from langchain_community.document_loaders import BlackboardLoaderloader = BlackboardLoader(    blackboard_course_url=""https://blackboard.example.com/webapps/blackboard/execute/announcement?method=search&context=course_entry&course_id=_123456_1"",    bbrouter=""expires:12345..."",    load_all_recursively=True,)documents = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/google_el_carro,Google El Carro for Oracle Workloads | ü¶úÔ∏èüîó Langchain,Google [El Carro Oracle,"Google El Carro for Oracle WorkloadsGoogle El Carro Oracle\nOperator\noffers a way to run Oracle databases in Kubernetes as a portable, open\nsource, community driven, no vendor lock-in container orchestration\nsystem. El Carro provides a powerful declarative API for comprehensive\nand consistent configuration and deployment as well as for real-time\noperations and monitoring. Extend your Oracle database‚Äôs capabilities\nto build AI-powered experiences by leveraging the El Carro Langchain\nintegration.This guide goes over how to use El Carro Langchain integration to save,\nload and delete langchain\ndocuments with\nElCarroLoader and ElCarroDocumentSaver. This integration works for\nany Oracle database, regardless of where it is running.Learn more about the package on\nGitHub.Open In ColabBefore You Begin‚ÄãPlease complete the Getting\nStarted\nsection of the README to set up your El Carro Oracle database.ü¶úüîó Library Installation‚ÄãThe integration lives in its own langchain-google-el-carro package, so\nwe need to install it.%pip install --upgrade --quiet langchain-google-el-carroBasic Usage‚ÄãSet Up Oracle Database Connection‚ÄãFill out the following variable with your Oracle database connections\ndetails.# @title Set Your Values Here { display-mode: ""form"" }HOST = ""127.0.0.1""  # @param {type: ""string""}PORT = 3307  # @param {type: ""integer""}DATABASE = ""my-database""  # @param {type: ""string""}TABLE_NAME = ""message_store""  # @param {type: ""string""}USER = ""my-user""  # @param {type: ""string""}PASSWORD = input(""Please provide a password to be used for the database user: "")If you are using El Carro, you can find the hostname and port values in\nthe status of the El Carro Kubernetes instance. Use the user password\nyou created for your PDB.Example Ouput:kubectl get -w instances.oracle.db.anthosapis.com -n dbNAME   DB ENGINE   VERSION   EDITION      ENDPOINT      URL                DB NAMES   BACKUP ID   READYSTATUS   READYREASON        DBREADYSTATUS   DBREADYREASONmydb   Oracle      18c       Express      mydb-svc.db   34.71.69.25:6021   ['pdbname']            TRUE          CreateComplete     True            CreateCompleteElCarroEngine Connection Pool‚ÄãElCarroEngine configures a connection pool to your Oracle database,\nenabling successful connections from your application and following\nindustry best practices.from langchain_google_el_carro import ElCarroEngineelcarro_engine = ElCarroEngine.from_instance(    db_host=HOST,    db_port=PORT,    db_name=DATABASE,    db_user=USER,    db_password=PASSWORD,)Initialize a table‚ÄãInitialize a table of default schema via\nelcarro_engine.init_document_table(<table_name>). Table Columns:page_content (type: text)langchain_metadata (type: JSON)elcarro_engine.drop_document_table(TABLE_NAME)elcarro_engine.init_document_table(    table_name=TABLE_NAME,)Save documents‚ÄãSave langchain documents with\nElCarroDocumentSaver.add_documents(<documents>). To initialize\nElCarroDocumentSaver class you need to provide 2 things:elcarro_engine - An instance of a ElCarroEngine engine.table_name - The name of the table within the Oracle database to\nstore langchain documents.from langchain_core.documents import Documentfrom langchain_google_el_carro import ElCarroDocumentSaverdoc = Document(    page_content=""Banana"",    metadata={""type"": ""fruit"", ""weight"": 100, ""organic"": 1},)saver = ElCarroDocumentSaver(    elcarro_engine=elcarro_engine,    table_name=TABLE_NAME,)saver.add_documents([doc])Load documents‚ÄãLoad langchain documents with ElCarroLoader.load() or\nElCarroLoader.lazy_load(). lazy_load returns a generator that only\nqueries database during the iteration. To initialize ElCarroLoader\nclass you need to provide:elcarro_engine - An instance of a ElCarroEngine engine.table_name - The name of the table within the Oracle database to\nstore langchain documents.from langchain_google_el_carro import ElCarroLoaderloader = ElCarroLoader(elcarro_engine=elcarro_engine, table_name=TABLE_NAME)docs = loader.lazy_load()for doc in docs:    print(""Loaded documents:"", doc)Load documents via query‚ÄãOther than loading documents from a table, we can also choose to load\ndocuments from a view generated from a SQL query. For example:from langchain_google_el_carro import ElCarroLoaderloader = ElCarroLoader(    elcarro_engine=elcarro_engine,    query=f""SELECT * FROM {TABLE_NAME} WHERE json_value(langchain_metadata, '$.organic') = '1'"",)onedoc = loader.load()print(onedoc)The view generated from SQL query can have different schema than default\ntable. In such cases, the behavior of ElCarroLoader is the same as\nloading from table with non-default schema. Please refer to section\nLoad documents with customized document page content &\nmetadata.Delete documents‚ÄãDelete a list of langchain documents from an Oracle table with\nElCarroDocumentSaver.delete(<documents>).For a table with a default schema (page_content, langchain_metadata),\nthe deletion criteria is:A row should be deleted if there exists a document in the list, such\nthatdocument.page_content equals row[page_content]document.metadata equals row[langchain_metadata]docs = loader.load()print(""Documents before delete:"", docs)saver.delete(onedoc)print(""Documents after delete:"", loader.load())Advanced Usage‚ÄãLoad documents with customized document page content & metadata‚ÄãFirst we prepare an example table with non-default schema, and populate\nit with some arbitrary data.import sqlalchemycreate_table_query = f""""""CREATE TABLE {TABLE_NAME} (    fruit_id NUMBER GENERATED BY DEFAULT AS IDENTITY (START WITH 1),    fruit_name VARCHAR2(100) NOT NULL,    variety VARCHAR2(50),    quantity_in_stock NUMBER(10) NOT NULL,    price_per_unit NUMBER(6,2) NOT NULL,    organic NUMBER(3) NOT NULL)""""""elcarro_engine.drop_document_table(TABLE_NAME)with elcarro_engine.connect() as conn:    conn.execute(sqlalchemy.text(create_table_query))    conn.commit()    conn.execute(        sqlalchemy.text(            f""""""            INSERT INTO {TABLE_NAME} (fruit_name, variety, quantity_in_stock, price_per_unit, organic)            VALUES ('Apple', 'Granny Smith', 150, 0.99, 1)            """"""        )    )    conn.execute(        sqlalchemy.text(            f""""""            INSERT INTO {TABLE_NAME} (fruit_name, variety, quantity_in_stock, price_per_unit, organic)            VALUES ('Banana', 'Cavendish', 200, 0.59, 0)            """"""        )    )    conn.execute(        sqlalchemy.text(            f""""""            INSERT INTO {TABLE_NAME} (fruit_name, variety, quantity_in_stock, price_per_unit, organic)            VALUES ('Orange', 'Navel', 80, 1.29, 1)            """"""        )    )    conn.commit()If we still load langchain documents with default parameters of\nElCarroLoader from this example table, the page_content of loaded\ndocuments will be the first column of the table, and metadata will be\nconsisting of key-value pairs of all the other columns.loader = ElCarroLoader(    elcarro_engine=elcarro_engine,    table_name=TABLE_NAME,)loaded_docs = loader.load()print(f""Loaded Documents: [{loaded_docs}]"")We can specify the content and metadata we want to load by setting the\ncontent_columns and metadata_columns when initializing the\nElCarroLoader.content_columns: The columns to write into the page_content of\nthe document.metadata_columns: The columns to write into the metadata of the\ndocument.For example here, the values of columns in content_columns will be\njoined together into a space-separated string, as page_content of\nloaded documents, and metadata of loaded documents will only contain\nkey-value pairs of columns specified in metadata_columns.loader = ElCarroLoader(    elcarro_engine=elcarro_engine,    table_name=TABLE_NAME,    content_columns=[        ""variety"",        ""quantity_in_stock"",        ""price_per_unit"",        ""organic"",    ],    metadata_columns=[""fruit_id"", ""fruit_name""],)loaded_docs = loader.load()print(f""Loaded Documents: [{loaded_docs}]"")Save document with customized page content & metadata‚ÄãIn order to save langchain document into table with customized metadata\nfields we need first create such a table via\nElCarroEngine.init_document_table(), and specify the list of\nmetadata_columns we want it to have. In this example, the created\ntable will have table columns:content (type: text): for storing fruit description.type (type VARCHAR2(200)): for storing fruit type.weight (type INT): for storing fruit weight.extra_json_metadata (type: JSON): for storing other metadata\ninformation of the fruit.We can use the following parameters with\nelcarro_engine.init_document_table() to create the table:table_name: The name of the table within the Oracle database to\nstore langchain documents.metadata_columns: A list of sqlalchemy.Column indicating the\nlist of metadata columns we need.content_column: column name to store page_content of langchain\ndocument. Default: ""page_content"", ""VARCHAR2(4000)""metadata_json_column: column name to store extra JSON metadata\nof langchain document. Default:\n""langchain_metadata"", ""VARCHAR2(4000)"".elcarro_engine.drop_document_table(TABLE_NAME)elcarro_engine.init_document_table(    table_name=TABLE_NAME,    metadata_columns=[        sqlalchemy.Column(""type"", sqlalchemy.dialects.oracle.VARCHAR2(200)),        sqlalchemy.Column(""weight"", sqlalchemy.INT),    ],    content_column=""content"",    metadata_json_column=""extra_json_metadata"",)Save documents with ElCarroDocumentSaver.add_documents(<documents>).\nAs you can see in this example,document.page_content will be saved into content column.document.metadata.type will be saved into type column.document.metadata.weight will be saved into weight column.document.metadata.organic will be saved into extra_json_metadata\ncolumn in JSON format.doc = Document(    page_content=""Banana"",    metadata={""type"": ""fruit"", ""weight"": 100, ""organic"": 1},)print(f""Original Document: [{doc}]"")saver = ElCarroDocumentSaver(    elcarro_engine=elcarro_engine,    table_name=TABLE_NAME,    content_column=""content"",    metadata_json_column=""extra_json_metadata"",)saver.add_documents([doc])loader = ElCarroLoader(    elcarro_engine=elcarro_engine,    table_name=TABLE_NAME,    content_columns=[""content""],    metadata_columns=[        ""type"",        ""weight"",    ],    metadata_json_column=""extra_json_metadata"",)loaded_docs = loader.load()print(f""Loaded Document: [{loaded_docs[0]}]"")Delete documents with customized page content & metadata‚ÄãWe can also delete documents from table with customized metadata columns\nvia ElCarroDocumentSaver.delete(<documents>). The deletion criteria\nis:A row should be deleted if there exists a document in the list, such\nthatdocument.page_content equals row[page_content]For every metadata field k in document.metadatadocument.metadata[k] equals row[k] or document.metadata[k]\nequals row[langchain_metadata][k]There is no extra metadata field present in row but not in\ndocument.metadata.loader = ElCarroLoader(elcarro_engine=elcarro_engine, table_name=TABLE_NAME)saver.delete(loader.load())print(f""Documents left: {len(loader.load())}"")More examples‚ÄãPlease look at\ndemo_doc_loader_basic.py\nand\ndemo_doc_loader_advanced.py\nfor complete code examples.",en,
https://python.langchain.com/docs/integrations/document_loaders/conll-u,CoNLL-U | ü¶úÔ∏èüîó Langchain,CoNLL-U is revised,"CoNLL-UCoNLL-U is revised\nversion of the CoNLL-X format. Annotations are encoded in plain text\nfiles (UTF-8, normalized to NFC, using only the LF character as line\nbreak, including an LF character at the end of file) with three types\nof lines: - Word lines containing the annotation of a word/token in 10\nfields separated by single tab characters; see below. - Blank lines\nmarking sentence boundaries. - Comment lines starting with hash (#).This is an example of how to load a file in\nCoNLL-U format. The\nwhole file is treated as one document. The example data\n(conllu.conllu) is based on one of the standard UD/CoNLL-U examples.from langchain_community.document_loaders import CoNLLULoaderloader = CoNLLULoader(""example_data/conllu.conllu"")document = loader.load()document[Document(page_content='They buy and sell books.', metadata={'source': 'example_data/conllu.conllu'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/slack,Slack | ü¶úÔ∏èüîó Langchain,Slack is an instant messaging program.,"SlackSlack is an instant messaging program.This notebook covers how to load documents from a Zipfile generated from\na Slack export.In order to get this Slack export, follow these instructions:üßë Instructions for ingesting your own dataset‚ÄãExport your Slack data. You can do this by going to your Workspace\nManagement page and clicking the Import/Export option\n({your_slack_domain}.slack.com/services/export). Then, choose the right\ndate range and click Start export. Slack will send you an email and a\nDM when the export is ready.The download will produce a .zip file in your Downloads folder (or\nwherever your downloads can be found, depending on your OS\nconfiguration).Copy the path to the .zip file, and assign it as LOCAL_ZIPFILE\nbelow.from langchain_community.document_loaders import SlackDirectoryLoader# Optionally set your Slack URL. This will give you proper URLs in the docs sources.SLACK_WORKSPACE_URL = ""https://xxx.slack.com""LOCAL_ZIPFILE = """"  # Paste the local paty to your Slack zip file here.loader = SlackDirectoryLoader(LOCAL_ZIPFILE, SLACK_WORKSPACE_URL)docs = loader.load()docs",en,
https://python.langchain.com/docs/integrations/document_loaders/apify_dataset,Apify Dataset | ü¶úÔ∏èüîó Langchain,Apify Dataset is a,"Apify DatasetApify Dataset is a\nscalable append-only storage with sequential access built for storing\nstructured web scraping results, such as a list of products or Google\nSERPs, and then export them to various formats like JSON, CSV, or\nExcel. Datasets are mainly used to save results of Apify\nActors‚Äîserverless cloud programs for various\nweb scraping, crawling, and data extraction use cases.This notebook shows how to load Apify datasets to LangChain.Prerequisites‚ÄãYou need to have an existing dataset on the Apify platform. If you don‚Äôt\nhave one, please first check out this\nnotebook on how to use Apify to extract\ncontent from documentation, knowledge bases, help centers, or blogs.%pip install --upgrade --quiet  apify-clientFirst, import ApifyDatasetLoader into your source code:from langchain_community.document_loaders import ApifyDatasetLoaderfrom langchain_community.document_loaders.base import DocumentThen provide a function that maps Apify dataset record fields to\nLangChain Document format.For example, if your dataset items are structured like this:{    ""url"": ""https://apify.com"",    ""text"": ""Apify is the best web scraping and automation platform.""}The mapping function in the code below will convert them to LangChain\nDocument format, so that you can use them further with any LLM model\n(e.g.¬†for question answering).loader = ApifyDatasetLoader(    dataset_id=""your-dataset-id"",    dataset_mapping_function=lambda dataset_item: Document(        page_content=dataset_item[""text""], metadata={""source"": dataset_item[""url""]}    ),)data = loader.load()An example with question answering‚ÄãIn this example, we use data from a dataset to answer a question.from langchain.docstore.document import Documentfrom langchain.indexes import VectorstoreIndexCreatorfrom langchain_community.document_loaders import ApifyDatasetLoaderloader = ApifyDatasetLoader(    dataset_id=""your-dataset-id"",    dataset_mapping_function=lambda item: Document(        page_content=item[""text""] or """", metadata={""source"": item[""url""]}    ),)index = VectorstoreIndexCreator().from_loaders([loader])query = ""What is Apify?""result = index.query_with_sources(query)print(result[""answer""])print(result[""sources""]) Apify is a platform for developing, running, and sharing serverless cloud programs. It enables users to create web scraping and automation tools and publish them on the Apify platform.https://docs.apify.com/platform/actors, https://docs.apify.com/platform/actors/running/actors-in-store, https://docs.apify.com/platform/security, https://docs.apify.com/platform/actors/examples",en,
https://python.langchain.com/docs/integrations/document_loaders/airbyte_typeform,Airbyte Typeform (Deprecated) | ü¶úÔ∏èüîó Langchain,Note: This connector-specific loader is deprecated. Please use,"Airbyte Typeform (Deprecated)Note: This connector-specific loader is deprecated. Please use\nAirbyteLoader instead.Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases.This loader exposes the Typeform connector as a document loader,\nallowing you to load various Typeform objects as documents.Installation‚ÄãFirst, you need to install the airbyte-source-typeform python package.%pip install --upgrade --quiet  airbyte-source-typeformExample‚ÄãCheck out the Airbyte documentation\npage for\ndetails about how to configure the reader. The JSON schema the config\nobject should adhere to can be found on Github:\nhttps://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-typeform/source_typeform/spec.json.The general shape looks like this:{  ""credentials"": {    ""auth_type"": ""Private Token"",    ""access_token"": ""<your auth token>""  },  ""start_date"": ""<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>"",  ""form_ids"": [""<id of form to load records for>""] # if omitted, records from all forms will be loaded}By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader.from langchain_community.document_loaders.airbyte import AirbyteTypeformLoaderconfig = {    # your typeform configuration}loader = AirbyteTypeformLoader(    config=config, stream_name=""forms"")  # check the documentation linked above for a list of all streamsNow you can load documents the usual waydocs = loader.load()As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the\nlazy_load method which returns an iterator instead:docs_iterator = loader.lazy_load()Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader:from langchain.docstore.document import Documentdef handle_record(record, id):    return Document(page_content=record.data[""title""], metadata=record.data)loader = AirbyteTypeformLoader(    config=config, record_handler=handle_record, stream_name=""forms"")docs = loader.load()Incremental loads‚ÄãSome streams allow incremental loading, this means the source keeps\ntrack of synced records and won‚Äôt load them again. This is useful for\nsources that have a high volume of data and are updated frequently.To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded.last_state = loader.last_state  # store safelyincremental_loader = AirbyteTypeformLoader(    config=config, record_handler=handle_record, stream_name=""forms"", state=last_state)new_docs = incremental_loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/psychic,Psychic | ü¶úÔ∏èüîó Langchain,This notebook covers how to load documents from Psychic. See,"PsychicThis notebook covers how to load documents from Psychic. See\nhere for more details.Prerequisites‚ÄãFollow the Quick Start section in this\ndocumentLog into the Psychic dashboard and\nget your secret keyInstall the frontend react library into your web app and have a user\nauthenticate a connection. The connection will be created using the\nconnection id that you specify.Loading documents‚ÄãUse the PsychicLoader class to load in documents from a connection.\nEach connection has a connector id (corresponding to the SaaS app that\nwas connected) and a connection id (which you passed in to the frontend\nlibrary).# Uncomment this to install psychicapi if you don't already have it installed!poetry run pip -q install psychicapi[notice] A new release of pip is available: 23.0.1 -> 23.1.2[notice] To update, run: pip install --upgrade pipfrom langchain_community.document_loaders import PsychicLoaderfrom psychicapi import ConnectorId# Create a document loader for google drive. We can also load from other connectors by setting the connector_id to the appropriate value e.g. ConnectorId.notion.value# This loader uses our test credentialsgoogle_drive_loader = PsychicLoader(    api_key=""7ddb61c1-8b6a-4d31-a58e-30d1c9ea480e"",    connector_id=ConnectorId.gdrive.value,    connection_id=""google-test"",)documents = google_drive_loader.load()Converting the docs to embeddings‚ÄãWe can now convert these documents into embeddings and store them in a\nvector database like Chromafrom langchain.chains import RetrievalQAWithSourcesChainfrom langchain_community.vectorstores import Chromafrom langchain_openai import OpenAI, OpenAIEmbeddingsfrom langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)embeddings = OpenAIEmbeddings()docsearch = Chroma.from_documents(texts, embeddings)chain = RetrievalQAWithSourcesChain.from_chain_type(    OpenAI(temperature=0), chain_type=""stuff"", retriever=docsearch.as_retriever())chain({""question"": ""what is psychic?""}, return_only_outputs=True)",en,
https://python.langchain.com/docs/integrations/document_loaders/odt,Open Document Format (ODT) | ü¶úÔ∏èüîó Langchain,The [Open Document Format for Office Applications,"Open Document Format (ODT)The Open Document Format for Office Applications\n(ODF), also known as\nOpenDocument, is an open file format for word processing documents,\nspreadsheets, presentations and graphics and using ZIP-compressed XML\nfiles. It was developed with the aim of providing an open, XML-based\nfile format specification for office applications.The standard is developed and maintained by a technical committee in\nthe Organization for the Advancement of Structured Information\nStandards (OASIS) consortium. It was based on the Sun Microsystems\nspecification for OpenOffice.org XML, the default format for\nOpenOffice.org and LibreOffice. It was originally developed for\nStarOffice ‚Äúto provide an open standard for office documents.‚ÄùThe UnstructuredODTLoader is used to load Open Office ODT files.from langchain_community.document_loaders import UnstructuredODTLoaderloader = UnstructuredODTLoader(""example_data/fake.odt"", mode=""elements"")docs = loader.load()docs[0]Document(page_content='Lorem ipsum dolor sit amet.', metadata={'source': 'example_data/fake.odt', 'filename': 'example_data/fake.odt', 'category': 'Title'})",en,
https://python.langchain.com/docs/integrations/document_loaders/rss,RSS Feeds | ü¶úÔ∏èüîó Langchain,This covers how to load HTML news articles from a list of RSS feed URLs,"RSS FeedsThis covers how to load HTML news articles from a list of RSS feed URLs\ninto a document format that we can use downstream.%pip install --upgrade --quiet  feedparser newspaper3k listparserfrom langchain_community.document_loaders import RSSFeedLoaderurls = [""https://news.ycombinator.com/rss""]Pass in urls to load them into Documentsloader = RSSFeedLoader(urls=urls)data = loader.load()print(len(data))print(data[0].page_content)(next Rich)04 August 2023Rich HickeyIt is with a mixture of heartache and optimism that I announce today my (long planned) retirement from commercial software development, and my employment at Nubank. It‚Äôs been thrilling to see Clojure and Datomic successfully applied at scale.I look forward to continuing to lead ongoing work maintaining and enhancing Clojure with Alex, Stu, Fogus and many others, as an independent developer once again. We have many useful things planned for 1.12 and beyond. The community remains friendly, mature and productive, and is taking Clojure into many interesting new domains.I want to highlight and thank Nubank for their ongoing sponsorship of Alex, Fogus and the core team, as well as the Clojure community at large.Stu will continue to lead the development of Datomic at Nubank, where the Datomic team grows and thrives. I‚Äôm particularly excited to see where the new free availability of Datomic will lead.My time with Cognitect remains the highlight of my career. I have learned from absolutely everyone on our team, and am forever grateful to all for our interactions. There are too many people to thank here, but I must extend my sincerest appreciation and love to Stu and Justin for (repeatedly) taking a risk on me and my ideas, and for being the best of partners and friends, at all times fully embodying the notion of integrity. And of course to Alex Miller - who possesses in abundance many skills I lack, and without whose indomitable spirit, positivity and friendship Clojure would not have become what it did.I have made many friends through Clojure and Cognitect, and I hope to nurture those friendships moving forward.Retirement returns me to the freedom and independence I had when originally developing Clojure. The journey continues!You can pass arguments to the NewsURLLoader which it uses to load\narticles.loader = RSSFeedLoader(urls=urls, nlp=True)data = loader.load()print(len(data))Error fetching or processing https://twitter.com/andrewmccalip/status/1687405505604734978, exception: You must `parse()` an article first!Error processing entry https://twitter.com/andrewmccalip/status/1687405505604734978, exception: list index out of range13data[0].metadata[""keywords""]['nubank', 'alex', 'stu', 'taking', 'team', 'remains', 'rich', 'clojure', 'thank', 'planned', 'datomic']data[0].metadata[""summary""]'It‚Äôs been thrilling to see Clojure and Datomic successfully applied at scale.\nI look forward to continuing to lead ongoing work maintaining and enhancing Clojure with Alex, Stu, Fogus and many others, as an independent developer once again.\nThe community remains friendly, mature and productive, and is taking Clojure into many interesting new domains.\nI want to highlight and thank Nubank for their ongoing sponsorship of Alex, Fogus and the core team, as well as the Clojure community at large.\nStu will continue to lead the development of Datomic at Nubank, where the Datomic team grows and thrives.'You can also use an OPML file such as a Feedly export. Pass in either a\nURL or the OPML contents.with open(""example_data/sample_rss_feeds.opml"", ""r"") as f:    loader = RSSFeedLoader(opml=f.read())data = loader.load()print(len(data))Error fetching http://www.engadget.com/rss-full.xml, exception: Error fetching http://www.engadget.com/rss-full.xml, exception: document declared as us-ascii, but parsed as utf-820data[0].page_content'The electric vehicle startup Fisker made a splash in Huntington Beach last night, showing off a range of new EVs it plans to build alongside the Fisker Ocean, which is slowly beginning deliveries in Europe and the US. With shades of Lotus circa 2010, it seems there\'s something for most tastes, with a powerful four-door GT, a versatile pickup truck, and an affordable electric city car.\n\n""We want the world to know that we have big plans and intend to move into several different segments, redefining each with our unique blend of design, innovation, and sustainability,"" said CEO Henrik Fisker.\n\nStarting with the cheapest, the Fisker PEAR‚Äîa cutesy acronym for ""Personal Electric Automotive Revolution""‚Äîis said to use 35 percent fewer parts than other small EVs. Although it\'s a smaller car, the PEAR seats six thanks to front and rear bench seats. Oh, and it has a frunk, which the company is calling the ""froot,"" something that will satisfy some British English speakers like Ars\' friend and motoring journalist Jonny Smith.\n\nBut most exciting is the price‚Äîstarting at $29,900 and scheduled for 2025. Fisker plans to contract with Foxconn to build the PEAR in Lordstown, Ohio, meaning it would be eligible for federal tax incentives.\n\nAdvertisement\n\nThe Fisker Alaska is the company\'s pickup truck, built on a modified version of the platform used by the Ocean. It has an extendable cargo bed, which can be as little as 4.5 feet (1,371 mm) or as much as 9.2 feet (2,804 mm) long. Fisker claims it will be both the lightest EV pickup on sale and the most sustainable pickup truck in the world. Range will be an estimated 230‚Äì240 miles (370‚Äì386 km).\n\nThis, too, is slated for 2025, and also at a relatively affordable price, starting at $45,400. Fisker hopes to build this car in North America as well, although it isn\'t saying where that might take place.\n\nFinally, there\'s the Ronin, a four-door GT that bears more than a passing resemblance to the Fisker Karma, Henrik Fisker\'s 2012 creation. There\'s no price for this one, but Fisker says its all-wheel drive powertrain will boast 1,000 hp (745 kW) and will hit 60 mph from a standing start in two seconds‚Äîjust about as fast as modern tires will allow. Expect a massive battery in this one, as Fisker says it\'s targeting a 600-mile (956 km) range.\n\n""Innovation and sustainability, along with design, are our three brand values. By 2027, we intend to produce the world‚Äôs first climate-neutral vehicle, and as our customers reinvent their relationships with mobility, we want to be a leader in software-defined transportation,"" Fisker said.'",en,
https://python.langchain.com/docs/integrations/document_loaders/google_drive,Google Drive | ü¶úÔ∏èüîó Langchain,Google Drive is a file,"Google DriveGoogle Drive is a file\nstorage and synchronization service developed by Google.This notebook covers how to load documents from Google Drive.\nCurrently, only Google Docs are supported.Prerequisites‚ÄãCreate a Google Cloud project or use an existing projectEnable the Google Drive\nAPIAuthorize credentials for desktop\napppip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlibüßë Instructions for ingesting your Google Docs data‚ÄãBy default, the GoogleDriveLoader expects the credentials.json file\nto be ~/.credentials/credentials.json, but this is configurable using\nthe credentials_path keyword argument. Same thing with token.json -\ntoken_path. Note that token.json will be created automatically the\nfirst time you use the loader.The first time you use GoogleDriveLoader, you will be displayed with the\nconsent screen in your browser. If this doesn‚Äôt happen and you get a\nRefreshError, do not use credentials_path in your\nGoogleDriveLoader constructor call. Instead, put that path in a\nGOOGLE_APPLICATION_CREDENTIALS environmental variable.GoogleDriveLoader can load from a list of Google Docs document ids or\na folder id. You can obtain your folder and document id from the URL:Folder:\nhttps://drive.google.com/drive/u/0/folders/1yucgL9WGgWZdM1TOuKkeghlPizuzMYb5\n-> folder id is ""1yucgL9WGgWZdM1TOuKkeghlPizuzMYb5""Document:\nhttps://docs.google.com/document/d/1bfaMQ18_i56204VaQDVeAFpqEijJTgvurupdEDiaUQw/edit\n-> document id is ""1bfaMQ18_i56204VaQDVeAFpqEijJTgvurupdEDiaUQw""%pip install --upgrade --quiet  google-api-python-client google-auth-httplib2 google-auth-oauthlibfrom langchain_community.document_loaders import GoogleDriveLoaderloader = GoogleDriveLoader(    folder_id=""1yucgL9WGgWZdM1TOuKkeghlPizuzMYb5"",    token_path=""/path/where/you/want/token/to/be/created/google_token.json"",    # Optional: configure whether to recursively fetch files from subfolders. Defaults to False.    recursive=False,)docs = loader.load()When you pass a folder_id by default all files of type document, sheet\nand pdf are loaded. You can modify this behaviour by passing a\nfile_types argumentloader = GoogleDriveLoader(    folder_id=""1yucgL9WGgWZdM1TOuKkeghlPizuzMYb5"",    file_types=[""document"", ""sheet""],    recursive=False,)Passing in Optional File Loaders‚ÄãWhen processing files other than Google Docs and Google Sheets, it can\nbe helpful to pass an optional file loader to GoogleDriveLoader. If\nyou pass in a file loader, that file loader will be used on documents\nthat do not have a Google Docs or Google Sheets MIME type. Here is an\nexample of how to load an Excel document from Google Drive using a file\nloader.from langchain_community.document_loaders import (    GoogleDriveLoader,    UnstructuredFileIOLoader,)file_id = ""1x9WBtFPWMEAdjcJzPScRsjpjQvpSo_kz""loader = GoogleDriveLoader(    file_ids=[file_id],    file_loader_cls=UnstructuredFileIOLoader,    file_loader_kwargs={""mode"": ""elements""},)docs = loader.load()docs[0]You can also process a folder with a mix of files and Google Docs/Sheets\nusing the following pattern:folder_id = ""1asMOHY1BqBS84JcRbOag5LOJac74gpmD""loader = GoogleDriveLoader(    folder_id=folder_id,    file_loader_cls=UnstructuredFileIOLoader,    file_loader_kwargs={""mode"": ""elements""},)docs = loader.load()docs[0]Extended usage‚ÄãAn external component can manage the complexity of Google Drive :\nlangchain-googledrive It‚Äôs compatible with the\nÃÄlangchain_community.document_loaders.GoogleDriveLoader and can be\nused in its place.To be compatible with containers, the authentication uses an environment\nvariable ÃÄGOOGLE_ACCOUNT_FILE to credential file (for user or service).%pip install --upgrade --quiet  langchain-googledrivefolder_id = ""root""# folder_id='1yucgL9WGgWZdM1TOuKkeghlPizuzMYb5'# Use the advanced version.from langchain_googledrive.document_loaders import GoogleDriveLoaderloader = GoogleDriveLoader(    folder_id=folder_id,    recursive=False,    num_results=2,  # Maximum number of file to load)By default, all files with these mime-type can be converted to\nDocument. - text/text - text/plain - text/html - text/csv -\ntext/markdown - image/png - image/jpeg - application/epub+zip -\napplication/pdf - application/rtf - application/vnd.google-apps.document\n(GDoc) - application/vnd.google-apps.presentation (GSlide) -\napplication/vnd.google-apps.spreadsheet (GSheet) -\napplication/vnd.google.colaboratory (Notebook colab) -\napplication/vnd.openxmlformats-officedocument.presentationml.presentation\n(PPTX) -\napplication/vnd.openxmlformats-officedocument.wordprocessingml.document\n(DOCX)It‚Äôs possible to update or customize this. See the documentation of\nGDriveLoader.But, the corresponding packages must be installed.%pip install --upgrade --quiet  unstructuredfor doc in loader.load():    print(""---"")    print(doc.page_content.strip()[:60] + ""..."")Customize the search pattern‚ÄãAll parameter compatible with Google\nlist()\nAPI can be set.To specify the new pattern of the Google request, you can use a\nPromptTemplate(). The variables for the prompt can be set with\nkwargs in the constructor. Some pre-formated request are proposed (use\n{query}, {folder_id} and/or {mime_type}):You can customize the criteria to select the files. A set of predefined\nfilter are proposed:templatedescriptiongdrive-all-in-folderReturn all compatible files from a folder_idgdrive-querySearch query in all drivesgdrive-by-nameSearch file with name querygdrive-query-in-folderSearch query in folder_id (and sub-folders if recursive=true)gdrive-mime-typeSearch a specific mime_typegdrive-mime-type-in-folderSearch a specific mime_type in folder_idgdrive-query-with-mime-typeSearch query with a specific mime_typegdrive-query-with-mime-type-and-folderSearch query with a specific mime_type and in folder_idloader = GoogleDriveLoader(    folder_id=folder_id,    recursive=False,    template=""gdrive-query"",  # Default template to use    query=""machine learning"",    num_results=2,  # Maximum number of file to load    supportsAllDrives=False,  # GDrive `list()` parameter)for doc in loader.load():    print(""---"")    print(doc.page_content.strip()[:60] + ""..."")You can customize your pattern.from langchain.prompts.prompt import PromptTemplateloader = GoogleDriveLoader(    folder_id=folder_id,    recursive=False,    template=PromptTemplate(        input_variables=[""query"", ""query_name""],        template=""fullText contains '{query}' and name contains '{query_name}' and trashed=false"",    ),  # Default template to use    query=""machine learning"",    query_name=""ML"",    num_results=2,  # Maximum number of file to load)for doc in loader.load():    print(""---"")    print(doc.page_content.strip()[:60] + ""..."")The conversion can manage in Markdown format: - bullet - link - table -\ntitlesSet the attribut return_link to True to export links.Modes for GSlide and GSheet‚ÄãThe parameter mode accepts different values:‚Äúdocument‚Äù: return the body of each document‚Äúsnippets‚Äù: return the description of each file (set in metadata of\nGoogle Drive files).The parameter gslide_mode accepts different values:‚Äúsingle‚Äù : one document with \<PAGE BREAK>‚Äúslide‚Äù : one document by slide‚Äúelements‚Äù : one document for each elements.loader = GoogleDriveLoader(    template=""gdrive-mime-type"",    mime_type=""application/vnd.google-apps.presentation"",  # Only GSlide files    gslide_mode=""slide"",    num_results=2,  # Maximum number of file to load)for doc in loader.load():    print(""---"")    print(doc.page_content.strip()[:60] + ""..."")The parameter gsheet_mode accepts different values: - ""single"":\nGenerate one document by line - ""elements"" : one document with\nmarkdown array and \<PAGE BREAK> tags.loader = GoogleDriveLoader(    template=""gdrive-mime-type"",    mime_type=""application/vnd.google-apps.spreadsheet"",  # Only GSheet files    gsheet_mode=""elements"",    num_results=2,  # Maximum number of file to load)for doc in loader.load():    print(""---"")    print(doc.page_content.strip()[:60] + ""..."")Advanced usage‚ÄãAll Google File have a ‚Äòdescription‚Äô in the metadata. This field can be\nused to memorize a summary of the document or others indexed tags (See\nmethod lazy_update_description_with_summary()).If you use the mode=""snippet"", only the description will be used for\nthe body. Else, the metadata['summary'] has the field.Sometime, a specific filter can be used to extract some information from\nthe filename, to select some files with specific criteria. You can use a\nfilter.Sometimes, many documents are returned. It‚Äôs not necessary to have all\ndocuments in memory at the same time. You can use the lazy versions of\nmethods, to get one document at a time. It‚Äôs better to use a complex\nquery in place of a recursive search. For each folder, a query must be\napplied if you activate recursive=True.import osloader = GoogleDriveLoader(    gdrive_api_file=os.environ[""GOOGLE_ACCOUNT_FILE""],    num_results=2,    template=""gdrive-query"",    filter=lambda search, file: ""#test"" not in file.get(""description"", """"),    query=""machine learning"",    supportsAllDrives=False,)for doc in loader.load():    print(""---"")    print(doc.page_content.strip()[:60] + ""..."")",en,
https://python.langchain.com/docs/integrations/document_loaders/gitbook,GitBook | ü¶úÔ∏èüîó Langchain,GitBook is a modern documentation,"GitBookGitBook is a modern documentation\nplatform where teams can document everything from products to internal\nknowledge bases and APIs.This notebook shows how to pull page data from any GitBook.from langchain_community.document_loaders import GitbookLoaderLoad from single GitBook page‚Äãloader = GitbookLoader(""https://docs.gitbook.com"")page_data = loader.load()page_data[Document(page_content='Introduction to GitBook\nGitBook is a modern documentation platform where teams can document everything from products to internal knowledge bases and APIs.\nWe want to help \nteams to work more efficiently\n by creating a simple yet powerful platform for them to \nshare their knowledge\n.\nOur mission is to make a \nuser-friendly\n and \ncollaborative\n product for everyone to create, edit and share knowledge through documentation.\nPublish your documentation in 5 easy steps\nImport\n\nMove your existing content to GitBook with ease.\nGit Sync\n\nBenefit from our bi-directional synchronisation with GitHub and GitLab.\nOrganise your content\n\nCreate pages and spaces and organize them into collections\nCollaborate\n\nInvite other users and collaborate asynchronously with ease.\nPublish your docs\n\nShare your documentation with selected users or with everyone.\nNext\n - Getting started\nOverview\nLast modified \n3mo ago', lookup_str='', metadata={'source': 'https://docs.gitbook.com', 'title': 'Introduction to GitBook'}, lookup_index=0)]Load from all paths in a given GitBook‚ÄãFor this to work, the GitbookLoader needs to be initialized with the\nroot path (https://docs.gitbook.com in this example) and have\nload_all_paths set to True.loader = GitbookLoader(""https://docs.gitbook.com"", load_all_paths=True)all_pages_data = loader.load()Fetching text from https://docs.gitbook.com/Fetching text from https://docs.gitbook.com/getting-started/overviewFetching text from https://docs.gitbook.com/getting-started/importFetching text from https://docs.gitbook.com/getting-started/git-syncFetching text from https://docs.gitbook.com/getting-started/content-structureFetching text from https://docs.gitbook.com/getting-started/collaborationFetching text from https://docs.gitbook.com/getting-started/publishingFetching text from https://docs.gitbook.com/tour/quick-findFetching text from https://docs.gitbook.com/tour/editorFetching text from https://docs.gitbook.com/tour/customizationFetching text from https://docs.gitbook.com/tour/member-managementFetching text from https://docs.gitbook.com/tour/pdf-exportFetching text from https://docs.gitbook.com/tour/activity-historyFetching text from https://docs.gitbook.com/tour/insightsFetching text from https://docs.gitbook.com/tour/notificationsFetching text from https://docs.gitbook.com/tour/internationalizationFetching text from https://docs.gitbook.com/tour/keyboard-shortcutsFetching text from https://docs.gitbook.com/tour/seoFetching text from https://docs.gitbook.com/advanced-guides/custom-domainFetching text from https://docs.gitbook.com/advanced-guides/advanced-sharing-and-securityFetching text from https://docs.gitbook.com/advanced-guides/integrationsFetching text from https://docs.gitbook.com/billing-and-admin/account-settingsFetching text from https://docs.gitbook.com/billing-and-admin/plansFetching text from https://docs.gitbook.com/troubleshooting/faqsFetching text from https://docs.gitbook.com/troubleshooting/hard-refreshFetching text from https://docs.gitbook.com/troubleshooting/report-bugsFetching text from https://docs.gitbook.com/troubleshooting/connectivity-issuesFetching text from https://docs.gitbook.com/troubleshooting/supportprint(f""fetched {len(all_pages_data)} documents."")# show second documentall_pages_data[2]fetched 28 documents.Document(page_content=""Import\nFind out how to easily migrate your existing documentation and which formats are supported.\nThe import function allows you to migrate and unify existing documentation in GitBook. You can choose to import single or multiple pages although limits apply. \nPermissions\nAll members with editor permission or above can use the import feature.\nSupported formats\nGitBook supports imports from websites or files that are:\nMarkdown (.md or .markdown)\nHTML (.html)\nMicrosoft Word (.docx).\nWe also support import from:\nConfluence\nNotion\nGitHub Wiki\nQuip\nDropbox Paper\nGoogle Docs\nYou can also upload a ZIP\n \ncontaining HTML or Markdown files when \nimporting multiple pages.\nNote: this feature is in beta.\nFeel free to suggest import sources we don't support yet and \nlet us know\n if you have any issues.\nImport panel\nWhen you create a new space, you'll have the option to import content straight away:\nThe new page menu\nImport a page or subpage by selecting \nImport Page\n from the New Page menu, or \nImport Subpage\n in the page action menu, found in the table of contents:\nImport from the page action menu\nWhen you choose your input source, instructions will explain how to proceed.\nAlthough GitBook supports importing content from different kinds of sources, the end result might be different from your source due to differences in product features and document format.\nLimits\nGitBook currently has the following limits for imported content:\nThe maximum number of pages that can be uploaded in a single import is \n20.\nThe maximum number of files (images etc.) that can be uploaded in a single import is \n20.\nGetting started - \nPrevious\nOverview\nNext\n - Getting started\nGit Sync\nLast modified \n4mo ago"", lookup_str='', metadata={'source': 'https://docs.gitbook.com/getting-started/import', 'title': 'Import'}, lookup_index=0)",en,
https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_file,Google Cloud Storage File | ü¶úÔ∏èüîó Langchain,[Google Cloud,"Google Cloud Storage FileGoogle Cloud\nStorage is a\nmanaged service for storing unstructured data.This covers how to load document objects from an\nGoogle Cloud Storage (GCS) file object (blob).%pip install --upgrade --quiet  google-cloud-storagefrom langchain_community.document_loaders import GCSFileLoaderloader = GCSFileLoader(project_name=""aist"", bucket=""testing-hwc"", blob=""fake.docx"")loader.load()/Users/harrisonchase/workplace/langchain/.venv/lib/python3.10/site-packages/google/auth/_default.py:83: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a ""quota exceeded"" or ""API not enabled"" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)[Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmp3srlf8n8/fake.docx'}, lookup_index=0)]If you want to use an alternative loader, you can provide a custom\nfunction, for example:from langchain_community.document_loaders import PyPDFLoaderdef load_pdf(file_path):    return PyPDFLoader(file_path)loader = GCSFileLoader(    project_name=""aist"", bucket=""testing-hwc"", blob=""fake.pdf"", loader_func=load_pdf)",en,
https://python.langchain.com/docs/integrations/document_loaders/imsdb,IMSDb | ü¶úÔ∏èüîó Langchain,IMSDb is the Internet Movie Script Database.,"IMSDbIMSDb is the Internet Movie Script Database.This covers how to load IMSDb webpages into a document format that we\ncan use downstream.from langchain_community.document_loaders import IMSDbLoaderloader = IMSDbLoader(""https://imsdb.com/scripts/BlacKkKlansman.html"")data = loader.load()data[0].page_content[:500]'\n\r\n\r\n\r\n\r\n                                    BLACKKKLANSMAN\r\n                         \r\n                         \r\n                         \r\n                         \r\n                                      Written by\r\n\r\n                          Charlie Wachtel & David Rabinowitz\r\n\r\n                                         and\r\n\r\n                              Kevin Willmott & Spike Lee\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n                         FADE IN:\r\n                         \r\n          SCENE FROM ""GONE WITH'data[0].metadata{'source': 'https://imsdb.com/scripts/BlacKkKlansman.html'}",en,
https://python.langchain.com/docs/integrations/document_loaders/azure_ai_data,Azure AI Data | ü¶úÔ∏èüîó Langchain,Azure AI Studio provides the capability to,"Azure AI DataAzure AI Studio provides the capability to\nupload data assets to cloud storage and register existing data assets\nfrom the following sources:Microsoft OneLakeAzure Blob StorageAzure Data Lake gen 2The benefit of this approach over AzureBlobStorageContainerLoader and\nAzureBlobStorageFileLoader is that authentication is handled\nseamlessly to cloud storage. You can use either identity-based data\naccess control to the data or credential-based (e.g.¬†SAS token,\naccount key). In the case of credential-based data access you do not\nneed to specify secrets in your code or set up key vaults - the system\nhandles that for you.This notebook covers how to load document objects from a data asset in\nAI Studio.%pip install --upgrade --quiet  azureml-fsspec, azure-ai-generativefrom azure.ai.resources.client import AIClientfrom azure.identity import DefaultAzureCredentialfrom langchain_community.document_loaders import AzureAIDataLoader# Create a connection to your projectclient = AIClient(    credential=DefaultAzureCredential(),    subscription_id=""<subscription_id>"",    resource_group_name=""<resource_group_name>"",    project_name=""<project_name>"",)# get the latest version of your data assetdata_asset = client.data.get(name=""<data_asset_name>"", label=""latest"")# load the data assetloader = AzureAIDataLoader(url=data_asset.path)loader.load()[Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmpaa9xl6ch/fake.docx'}, lookup_index=0)]Specifying a glob pattern‚ÄãYou can also specify a glob pattern for more finegrained control over\nwhat files to load. In the example below, only files with a pdf\nextension will be loaded.loader = AzureAIDataLoader(url=data_asset.path, glob=""*.pdf"")loader.load()[Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmpujbkzf_l/fake.docx'}, lookup_index=0)]",en,
https://python.langchain.com/docs/integrations/document_loaders/alibaba_cloud_maxcompute,Alibaba Cloud MaxCompute | ü¶úÔ∏èüîó Langchain,[Alibaba Cloud,"Alibaba Cloud MaxComputeAlibaba Cloud\nMaxCompute\n(previously known as ODPS) is a general purpose, fully managed,\nmulti-tenancy data processing platform for large-scale data\nwarehousing. MaxCompute supports various data importing solutions and\ndistributed computing models, enabling users to effectively query\nmassive datasets, reduce production costs, and ensure data security.The MaxComputeLoader lets you execute a MaxCompute SQL query and loads\nthe results as one document per row.%pip install --upgrade --quiet  pyodpsCollecting pyodps  Downloading pyodps-0.11.4.post0-cp39-cp39-macosx_10_9_universal2.whl (2.0 MB)     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.0/2.0 MB 1.7 MB/s eta 0:00:0000:0100:010mRequirement already satisfied: charset-normalizer>=2 in /Users/newboy/anaconda3/envs/langchain/lib/python3.9/site-packages (from pyodps) (3.1.0)Requirement already satisfied: urllib3<2.0,>=1.26.0 in /Users/newboy/anaconda3/envs/langchain/lib/python3.9/site-packages (from pyodps) (1.26.15)Requirement already satisfied: idna>=2.5 in /Users/newboy/anaconda3/envs/langchain/lib/python3.9/site-packages (from pyodps) (3.4)Requirement already satisfied: certifi>=2017.4.17 in /Users/newboy/anaconda3/envs/langchain/lib/python3.9/site-packages (from pyodps) (2023.5.7)Installing collected packages: pyodpsSuccessfully installed pyodps-0.11.4.post0Basic Usage‚ÄãTo instantiate the loader you‚Äôll need a SQL query to execute, your\nMaxCompute endpoint and project name, and you access ID and secret\naccess key. The access ID and secret access key can either be passed in\ndirect via the access_id and secret_access_key parameters or they\ncan be set as environment variables MAX_COMPUTE_ACCESS_ID and\nMAX_COMPUTE_SECRET_ACCESS_KEY.from langchain_community.document_loaders import MaxComputeLoaderbase_query = """"""SELECT *FROM (    SELECT 1 AS id, 'content1' AS content, 'meta_info1' AS meta_info    UNION ALL    SELECT 2 AS id, 'content2' AS content, 'meta_info2' AS meta_info    UNION ALL    SELECT 3 AS id, 'content3' AS content, 'meta_info3' AS meta_info) mydata;""""""endpoint = ""<ENDPOINT>""project = ""<PROJECT>""ACCESS_ID = ""<ACCESS ID>""SECRET_ACCESS_KEY = ""<SECRET ACCESS KEY>""loader = MaxComputeLoader.from_params(    base_query,    endpoint,    project,    access_id=ACCESS_ID,    secret_access_key=SECRET_ACCESS_KEY,)data = loader.load()print(data)[Document(page_content='id: 1\ncontent: content1\nmeta_info: meta_info1', metadata={}), Document(page_content='id: 2\ncontent: content2\nmeta_info: meta_info2', metadata={}), Document(page_content='id: 3\ncontent: content3\nmeta_info: meta_info3', metadata={})]print(data[0].page_content)id: 1content: content1meta_info: meta_info1print(data[0].metadata){}Specifying Which Columns are Content vs Metadata‚ÄãYou can configure which subset of columns should be loaded as the\ncontents of the Document and which as the metadata using the\npage_content_columns and metadata_columns parameters.loader = MaxComputeLoader.from_params(    base_query,    endpoint,    project,    page_content_columns=[""content""],  # Specify Document page content    metadata_columns=[""id"", ""meta_info""],  # Specify Document metadata    access_id=ACCESS_ID,    secret_access_key=SECRET_ACCESS_KEY,)data = loader.load()print(data[0].page_content)content: content1print(data[0].metadata){'id': 1, 'meta_info': 'meta_info1'}",en,
https://python.langchain.com/docs/integrations/document_loaders/rockset,Rockset | ü¶úÔ∏èüîó Langchain,Rockset is a real-time analytics database which enables queries on,"RocksetRockset is a real-time analytics database which enables queries on\nmassive, semi-structured data without operational burden. With\nRockset, ingested data is queryable within one second and analytical\nqueries against that data typically execute in milliseconds. Rockset\nis compute optimized, making it suitable for serving high concurrency\napplications in the sub-100TB range (or larger than 100s of TBs with\nrollups).This notebook demonstrates how to use Rockset as a document loader in\nlangchain. To get started, make sure you have a Rockset account and an\nAPI key available.Setting up the environment‚ÄãGo to the Rockset console and\nget an API key. Find your API region from the API\nreference. For the\npurpose of this notebook, we will assume you‚Äôre using Rockset from\nOregon(us-west-2).Set your the environment variable ROCKSET_API_KEY.Install the Rockset python client, which will be used by langchain\nto interact with the Rockset database.%pip install --upgrade --quiet  rocksetLoading DocumentsThe Rockset integration with LangChain allows you to load documents from\nRockset collections with SQL queries. In order to do this you must\nconstruct a RocksetLoader object. Here is an example snippet that\ninitializes a RocksetLoader.from langchain_community.document_loaders import RocksetLoaderfrom rockset import Regions, RocksetClient, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, ""<api key>""),    models.QueryRequestSql(query=""SELECT * FROM langchain_demo LIMIT 3""),  # SQL query    [""text""],  # content columns    metadata_keys=[""id"", ""date""],  # metadata columns)Here, you can see that the following query is run:SELECT * FROM langchain_demo LIMIT 3The text column in the collection is used as the page content, and the\nrecord‚Äôs id and date columns are used as metadata (if you do not\npass anything into metadata_keys, the whole Rockset document will be\nused as metadata).To execute the query and access an iterator over the resulting\nDocuments, run:loader.lazy_load()To execute the query and access all resulting Documents at once, run:loader.load()Here is an example response of loader.load():[    Document(        page_content=""Lorem ipsum dolor sit amet, consectetur adipiscing elit. Maecenas a libero porta, dictum ipsum eget, hendrerit neque. Morbi blandit, ex ut suscipit viverra, enim velit tincidunt tellus, a tempor velit nunc et ex. Proin hendrerit odio nec convallis lobortis. Aenean in purus dolor. Vestibulum orci orci, laoreet eget magna in, commodo euismod justo."",         metadata={""id"": 83209, ""date"": ""2022-11-13T18:26:45.000000Z""}    ),    Document(        page_content=""Integer at finibus odio. Nam sit amet enim cursus lacus gravida feugiat vestibulum sed libero. Aenean eleifend est quis elementum tincidunt. Curabitur sit amet ornare erat. Nulla id dolor ut magna volutpat sodales fringilla vel ipsum. Donec ultricies, lacus sed fermentum dignissim, lorem elit aliquam ligula, sed suscipit sapien purus nec ligula."",         metadata={""id"": 89313, ""date"": ""2022-11-13T18:28:53.000000Z""}    ),    Document(        page_content=""Morbi tortor enim, commodo id efficitur vitae, fringilla nec mi. Nullam molestie faucibus aliquet. Praesent a est facilisis, condimentum justo sit amet, viverra erat. Fusce volutpat nisi vel purus blandit, et facilisis felis accumsan. Phasellus luctus ligula ultrices tellus tempor hendrerit. Donec at ultricies leo."",         metadata={""id"": 87732, ""date"": ""2022-11-13T18:49:04.000000Z""}    )]Using multiple columns as content‚ÄãYou can choose to use multiple columns as content:from langchain_community.document_loaders import RocksetLoaderfrom rockset import Regions, RocksetClient, modelsloader = RocksetLoader(    RocksetClient(Regions.usw2a1, ""<api key>""),    models.QueryRequestSql(query=""SELECT * FROM langchain_demo LIMIT 1 WHERE id=38""),    [""sentence1"", ""sentence2""],  # TWO content columns)Assuming the ‚Äúsentence1‚Äù field is ""This is the first sentence."" and\nthe ‚Äúsentence2‚Äù field is ""This is the second sentence."", the\npage_content of the resulting Document would be:This is the first sentence.This is the second sentence.You can define you own function to join content columns by setting the\ncontent_columns_joiner argument in the RocksetLoader constructor.\ncontent_columns_joiner is a method that takes in a\nList[Tuple[str, Any]]] as an argument, representing a list of tuples\nof (column name, column value). By default, this is a method that joins\neach column value with a new line.For example, if you wanted to join sentence1 and sentence2 with a space\ninstead of a new line, you could set content_columns_joiner like so:RocksetLoader(    RocksetClient(Regions.usw2a1, ""<api key>""),    models.QueryRequestSql(query=""SELECT * FROM langchain_demo LIMIT 1 WHERE id=38""),    [""sentence1"", ""sentence2""],    content_columns_joiner=lambda docs: "" "".join(        [doc[1] for doc in docs]    ),  # join with space instead of /n)The page_content of the resulting Document would be:This is the first sentence. This is the second sentence.Oftentimes you want to include the column name in the page_content.\nYou can do that like this:RocksetLoader(    RocksetClient(Regions.usw2a1, ""<api key>""),    models.QueryRequestSql(query=""SELECT * FROM langchain_demo LIMIT 1 WHERE id=38""),    [""sentence1"", ""sentence2""],    content_columns_joiner=lambda docs: ""\n"".join(        [f""{doc[0]}: {doc[1]}"" for doc in docs]    ),)This would result in the following page_content:sentence1: This is the first sentence.sentence2: This is the second sentence.",en,
https://python.langchain.com/docs/integrations/document_loaders/airtable,Airtable | ü¶úÔ∏èüîó Langchain,-   Get your API key,"Airtable%pip install --upgrade --quiet  pyairtablefrom langchain_community.document_loaders import AirtableLoaderGet your API key\nhere.Get ID of your base\nhere.Get your table ID from the table url as shown\nhere.api_key = ""xxx""base_id = ""xxx""table_id = ""xxx""loader = AirtableLoader(api_key, table_id, base_id)docs = loader.load()Returns each table row as dict.len(docs)3eval(docs[0].page_content){'id': 'recF3GbGZCuh9sXIQ', 'createdTime': '2023-06-09T04:47:21.000Z', 'fields': {'Priority': 'High',  'Status': 'In progress',  'Name': 'Document Splitters'}}",en,
https://python.langchain.com/docs/integrations/document_loaders/github,GitHub | ü¶úÔ∏èüîó Langchain,This notebooks shows how you can load issues and pull requests (PRs) for,"GitHubThis notebooks shows how you can load issues and pull requests (PRs) for\na given repository on GitHub. Also shows how you\ncan load github files for a given repository on\nGitHub. We will use the LangChain Python\nrepository as an example.Setup access token‚ÄãTo access the GitHub API, you need a personal access token - you can set\nup yours here: https://github.com/settings/tokens?type=beta. You can\neither set this token as the environment variable\nGITHUB_PERSONAL_ACCESS_TOKEN and it will be automatically pulled in,\nor you can pass it in directly at initialization as the access_token\nnamed parameter.# If you haven't set your access token as an environment variable, pass it in here.from getpass import getpassACCESS_TOKEN = getpass()Load Issues and PRs‚Äãfrom langchain_community.document_loaders import GitHubIssuesLoaderloader = GitHubIssuesLoader(    repo=""langchain-ai/langchain"",    access_token=ACCESS_TOKEN,  # delete/comment out this argument if you've set the access token as an env var.    creator=""UmerHA"",)Let‚Äôs load all issues and PRs created by ‚ÄúUmerHA‚Äù.Here‚Äôs a list of all filters you can use: - include_prs - milestone -\nstate - assignee - creator - mentioned - labels - sort - direction -\nsinceFor more info, see\nhttps://docs.github.com/en/rest/issues/issues?apiVersion=2022-11-28#list-repository-issues.docs = loader.load()print(docs[0].page_content)print(docs[0].metadata)Only load issues‚ÄãBy default, the GitHub API returns considers pull requests to also be\nissues. To only get ‚Äòpure‚Äô issues (i.e., no pull requests), use\ninclude_prs=Falseloader = GitHubIssuesLoader(    repo=""langchain-ai/langchain"",    access_token=ACCESS_TOKEN,  # delete/comment out this argument if you've set the access token as an env var.    creator=""UmerHA"",    include_prs=False,)docs = loader.load()print(docs[0].page_content)print(docs[0].metadata)Load Github File Content‚ÄãFor below code, loads all markdown file in rpeo langchain-ai/langchainfrom langchain.document_loaders import GithubFileLoaderloader = GithubFileLoader(    repo=""langchain-ai/langchain"",  # the repo name    access_token=ACCESS_TOKEN,    github_api_url=""https://api.github.com"",    file_filter=lambda file_path: file_path.endswith(        "".md""    ),  # load all markdowns files.)documents = loader.load()example output of one of document:documents.metadata:     {      ""path"": ""README.md"",      ""sha"": ""82f1c4ea88ecf8d2dfsfx06a700e84be4"",      ""source"": ""https://github.com/langchain-ai/langchain/blob/master/README.md""    }documents.content:    mock content",en,
https://python.langchain.com/docs/integrations/document_loaders/pebblo,Pebblo Safe DocumentLoader | ü¶úÔ∏èüîó Langchain,Pebblo enables developers to,"Pebblo Safe DocumentLoaderPebblo enables developers to\nsafely load data and promote their Gen AI app to deployment without\nworrying about the organization‚Äôs compliance and security\nrequirements. The project identifies semantic topics and entities\nfound in the loaded data and summarizes them on the UI or a PDF\nreport.Pebblo has two components.Pebblo Safe DocumentLoader for LangchainPebblo DaemonThis document describes how to augment your existing Langchain\nDocumentLoader with Pebblo Safe DocumentLoader to get deep data\nvisibility on the types of Topics and Entities ingested into the Gen-AI\nLangchain application. For details on Pebblo Daemon see this pebblo\ndaemon document.Pebblo Safeloader enables safe data ingestion for Langchain\nDocumentLoader. This is done by wrapping the document loader call with\nPebblo Safe DocumentLoader.How to Pebblo enable Document Loading?‚ÄãAssume a Langchain RAG application snippet using CSVLoader to read a\nCSV document for inference.Here is the snippet of Document loading using CSVLoader.from langchain.document_loaders.csv_loader import CSVLoaderloader = CSVLoader(""data/corp_sens_data.csv"")documents = loader.load()print(documents)The Pebblo SafeLoader can be enabled with few lines of code change to\nthe above snippet.from langchain.document_loaders.csv_loader import CSVLoaderfrom langchain_community.document_loaders import PebbloSafeLoaderloader = PebbloSafeLoader(    CSVLoader(""data/corp_sens_data.csv""),    name=""acme-corp-rag-1"",  # App name (Mandatory)    owner=""Joe Smith"",  # Owner (Optional)    description=""Support productivity RAG application"",  # Description (Optional))documents = loader.load()print(documents)",en,
https://python.langchain.com/docs/integrations/document_loaders/airbyte_json,Airbyte JSON (Deprecated) | ü¶úÔ∏èüîó Langchain,Note: AirbyteJSONLoader is deprecated. Please use,"Airbyte JSON (Deprecated)Note: AirbyteJSONLoader is deprecated. Please use\nAirbyteLoader instead.Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases.This covers how to load any source from Airbyte into a local JSON file\nthat can be read in as a documentPrereqs: Have docker desktop installedSteps:Clone Airbyte from GitHub -\ngit clone https://github.com/airbytehq/airbyte.gitSwitch into Airbyte directory - cd airbyteStart Airbyte - docker compose upIn your browser, just visit¬†http://localhost:8000. You will be asked\nfor a username and password. By default, that‚Äôs\nusername¬†airbyte¬†and password¬†password.Setup any source you wish.Set destination as Local JSON, with specified destination path -\nlets say /json_data. Set up manual sync.Run the connection.To see what files are create, you can navigate to:\nfile:///tmp/airbyte_localFind your data and copy path. That path should be saved in the file\nvariable below. It should start with /tmp/airbyte_localfrom langchain_community.document_loaders import AirbyteJSONLoader!ls /tmp/airbyte_local/json_data/_airbyte_raw_pokemon.jsonlloader = AirbyteJSONLoader(""/tmp/airbyte_local/json_data/_airbyte_raw_pokemon.jsonl"")data = loader.load()print(data[0].page_content[:500])abilities: ability: name: blazeurl: https://pokeapi.co/api/v2/ability/66/is_hidden: Falseslot: 1ability: name: solar-powerurl: https://pokeapi.co/api/v2/ability/94/is_hidden: Trueslot: 3base_experience: 267forms: name: charizardurl: https://pokeapi.co/api/v2/pokemon-form/6/game_indices: game_index: 180version: name: redurl: https://pokeapi.co/api/v2/version/1/game_index: 180version: name: blueurl: https://pokeapi.co/api/v2/version/2/game_index: 180version: n",en,
https://python.langchain.com/docs/integrations/document_loaders/git,Git | ü¶úÔ∏èüîó Langchain,Git is a distributed version,"GitGit is a distributed version\ncontrol system that tracks changes in any set of computer files,\nusually used for coordinating work among programmers collaboratively\ndeveloping source code during software development.This notebook shows how to load text files from Git repository.Load existing repository from disk‚Äã%pip install --upgrade --quiet  GitPythonfrom git import Reporepo = Repo.clone_from(    ""https://github.com/langchain-ai/langchain"", to_path=""./example_data/test_repo1"")branch = repo.head.referencefrom langchain_community.document_loaders import GitLoaderloader = GitLoader(repo_path=""./example_data/test_repo1/"", branch=branch)data = loader.load()len(data)print(data[0])page_content='.venv\n.github\n.git\n.mypy_cache\n.pytest_cache\nDockerfile' metadata={'file_path': '.dockerignore', 'file_name': '.dockerignore', 'file_type': ''}Clone repository from url‚Äãfrom langchain_community.document_loaders import GitLoaderloader = GitLoader(    clone_url=""https://github.com/langchain-ai/langchain"",    repo_path=""./example_data/test_repo2/"",    branch=""master"",)data = loader.load()len(data)1074Filtering files to load‚Äãfrom langchain_community.document_loaders import GitLoader# e.g. loading only python filesloader = GitLoader(    repo_path=""./example_data/test_repo1/"",    file_filter=lambda file_path: file_path.endswith("".py""),)",en,
https://python.langchain.com/docs/integrations/document_loaders/google_spanner,Google Spanner | ü¶úÔ∏èüîó Langchain,Spanner is a highly scalable,"Google SpannerSpanner is a highly scalable\ndatabase that combines unlimited scalability with relational\nsemantics, such as secondary indexes, strong consistency, schemas, and\nSQL providing 99.999% availability in one easy solution.This notebook goes over how to use\nSpanner to save, load and delete\nlangchain documents\nwith SpannerLoader and SpannerDocumentSaver.Learn more about the package on\nGitHub.Open In ColabBefore You Begin‚ÄãTo run this notebook, you will need to do the following:Create a Google Cloud\nProjectEnable the Cloud Spanner\nAPICreate a Spanner\ninstanceCreate a Spanner\ndatabaseCreate a Spanner\ntableAfter confirmed access to database in the runtime environment of this\nnotebook, filling the following values and run the cell before running\nexample scripts.# @markdown Please specify an instance id, a database, and a table for demo purpose.INSTANCE_ID = ""test_instance""  # @param {type:""string""}DATABASE_ID = ""test_database""  # @param {type:""string""}TABLE_NAME = ""test_table""  # @param {type:""string""}ü¶úüîó Library Installation‚ÄãThe integration lives in its own langchain-google-spanner package, so\nwe need to install it.%pip install -upgrade --quiet langchain-google-spanner langchainColab only: Uncomment the following cell to restart the kernel or\nuse the button to restart the kernel. For Vertex AI Workbench you can\nrestart the terminal using the button on top.# # Automatically restart kernel after installs so that your environment can access the new packages# import IPython# app = IPython.Application.instance()# app.kernel.do_shutdown(True)‚òÅ Set Your Google Cloud Project‚ÄãSet your Google Cloud project so that you can leverage Google Cloud\nresources within this notebook.If you don‚Äôt know your project ID, try the following:Run gcloud config list.Run gcloud projects list.See the support page: Locate the project\nID.# @markdown Please fill in the value below with your Google Cloud project ID and then run the cell.PROJECT_ID = ""my-project-id""  # @param {type:""string""}# Set the project id!gcloud config set project {PROJECT_ID}üîê Authentication‚ÄãAuthenticate to Google Cloud as the IAM user logged into this notebook\nin order to access your Google Cloud Project.If you are using Colab to run this notebook, use the cell below and\ncontinue.If you are using Vertex AI Workbench, check out the setup\ninstructions\nhere.from google.colab import authauth.authenticate_user()Basic Usage‚ÄãSave documents‚ÄãSave langchain documents with\nSpannerDocumentSaver.add_documents(<documents>). To initialize\nSpannerDocumentSaver class you need to provide 3 things:instance_id - An instance of Spanner to load data from.database_id - An instance of Spanner database to load data from.table_name - The name of the table within the Spanner database to\nstore langchain documents.from langchain_core.documents import Documentfrom langchain_google_spanner import SpannerDocumentSavertest_docs = [    Document(        page_content=""Apple Granny Smith 150 0.99 1"",        metadata={""fruit_id"": 1},    ),    Document(        page_content=""Banana Cavendish 200 0.59 0"",        metadata={""fruit_id"": 2},    ),    Document(        page_content=""Orange Navel 80 1.29 1"",        metadata={""fruit_id"": 3},    ),]saver = SpannerDocumentSaver(    instance_id=INSTANCE_ID,    database_id=DATABASE_ID,    table_name=TABLE_NAME,)saver.add_documents(test_docs)Querying for Documents from Spanner‚ÄãFor more details on connecting to a Spanner table, please check the\nPython SDK\ndocumentation.Load documents from table‚ÄãLoad langchain documents with SpannerLoader.load() or\nSpannerLoader.lazy_load(). lazy_load returns a generator that only\nqueries database during the iteration. To initialize SpannerLoader\nclass you need to provide:instance_id - An instance of Spanner to load data from.database_id - An instance of Spanner database to load data from.query - A query of the database dialect.from langchain_google_spanner import SpannerLoaderquery = f""SELECT * from {TABLE_NAME}""loader = SpannerLoader(    instance_id=INSTANCE_ID,    database_id=DATABASE_ID,    query=query,)for doc in loader.lazy_load():    print(doc)    breakDelete documents‚ÄãDelete a list of langchain documents from the table with\nSpannerDocumentSaver.delete(<documents>).docs = loader.load()print(""Documents before delete:"", docs)doc = test_docs[0]saver.delete([doc])print(""Documents after delete:"", loader.load())Advanced Usage‚ÄãCustom client‚ÄãThe client created by default is the default client. To pass in\ncredentials and project explicitly, a custom client can be passed to\nthe constructor.from google.cloud import spannerfrom google.oauth2 import service_accountcreds = service_account.Credentials.from_service_account_file(""/path/to/key.json"")custom_client = spanner.Client(project=""my-project"", credentials=creds)loader = SpannerLoader(    INSTANCE_ID,    DATABASE_ID,    query,    client=custom_client,)Customize Document Page Content & Metadata‚ÄãThe loader will returns a list of Documents with page content from a\nspecific data columns. All other data columns will be added to metadata.\nEach row becomes a document.Customize page content format‚ÄãThe SpannerLoader assumes there is a column called page_content. These\ndefaults can be changed like so:custom_content_loader = SpannerLoader(    INSTANCE_ID, DATABASE_ID, query, content_columns=[""custom_content""])If multiple columns are specified, the page content‚Äôs string format will\ndefault to text (space-separated string concatenation). There are\nother format that user can specify, including text, JSON, YAML,\nCSV.Customize metadata format‚ÄãThe SpannerLoader assumes there is a metadata column called\nlangchain_metadata that store JSON data. The metadata column will be\nused as the base dictionary. By default, all other column data will be\nadded and may overwrite the original value. These defaults can be\nchanged like so:custom_metadata_loader = SpannerLoader(    INSTANCE_ID, DATABASE_ID, query, metadata_columns=[""column1"", ""column2""])Customize JSON metadata column name‚ÄãBy default, the loader uses langchain_metadata as the base dictionary.\nThis can be customized to select a JSON column to use as base dictionary\nfor the Document‚Äôs metadata.custom_metadata_json_loader = SpannerLoader(    INSTANCE_ID, DATABASE_ID, query, metadata_json_column=""another-json-column"")Custom staleness‚ÄãThe default\nstaleness\nis 15s. This can be customized by specifying a weaker bound (which can\neither be to perform all reads as of a given timestamp), or as of a\ngiven duration in the past.import datetimetimestamp = datetime.datetime.utcnow()custom_timestamp_loader = SpannerLoader(    INSTANCE_ID,    DATABASE_ID,    query,    staleness=timestamp,)duration = 20.0custom_duration_loader = SpannerLoader(    INSTANCE_ID,    DATABASE_ID,    query,    staleness=duration,)Turn on data boost‚ÄãBy default, the loader will not use data\nboost\nsince it has additional costs associated, and require additional IAM\npermissions. However, user can choose to turn it on.custom_databoost_loader = SpannerLoader(    INSTANCE_ID,    DATABASE_ID,    query,    databoost=True,)Custom client‚ÄãThe client created by default is the default client. To pass in\ncredentials and project explicitly, a custom client can be passed to\nthe constructor.from google.cloud import spannercustom_client = spanner.Client(project=""my-project"", credentials=creds)saver = SpannerDocumentSaver(    INSTANCE_ID,    DATABASE_ID,    TABLE_NAME,    client=custom_client,)Custom initialization for SpannerDocumentSaver‚ÄãThe SpannerDocumentSaver allows custom initialization. This allows user\nto specify how the Document is saved into the table.content_column: This will be used as the column name for the Document‚Äôs\npage content. Defaulted to page_content.metadata_columns: These metadata will be saved into specific columns if\nthe key exists in the Document‚Äôs metadata.metadata_json_column: This will be the column name for the spcial JSON\ncolumn. Defaulted to langchain_metadata.custom_saver = SpannerDocumentSaver(    INSTANCE_ID,    DATABASE_ID,    TABLE_NAME,    content_column=""my-content"",    metadata_columns=[""foo""],    metadata_json_column=""my-special-json-column"",)Initialize custom schema for Spanner‚ÄãThe SpannerDocumentSaver will have a init_document_table method to\ncreate a new table to store docs with custom schema.from langchain_google_spanner import Columnnew_table_name = ""my_new_table""SpannerDocumentSaver.init_document_table(    INSTANCE_ID,    DATABASE_ID,    new_table_name,    content_column=""my-page-content"",    metadata_columns=[        Column(""category"", ""STRING(36)"", True),        Column(""price"", ""FLOAT64"", False),    ],)",en,
https://python.langchain.com/docs/integrations/document_loaders/roam,Roam | ü¶úÔ∏èüîó Langchain,ROAM is a note-taking tool for networked,"RoamROAM is a note-taking tool for networked\nthought, designed to create a personal knowledge base.This notebook covers how to load documents from a Roam database. This\ntakes a lot of inspiration from the example repo\nhere.üßë Instructions for ingesting your own dataset‚ÄãExport your dataset from Roam Research. You can do this by clicking on\nthe three dots in the upper right hand corner and then clicking\nExport.When exporting, make sure to select the Markdown & CSV format option.This will produce a .zip file in your Downloads folder. Move the\n.zip file into this repository.Run the following command to unzip the zip file (replace the Export...\nwith your own file name as needed).unzip Roam-Export-1675782732639.zip -d Roam_DBfrom langchain_community.document_loaders import RoamLoaderloader = RoamLoader(""Roam_DB"")docs = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong,Airbyte Gong (Deprecated) | ü¶úÔ∏èüîó Langchain,Note: This connector-specific loader is deprecated. Please use,"Airbyte Gong (Deprecated)Note: This connector-specific loader is deprecated. Please use\nAirbyteLoader instead.Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases.This loader exposes the Gong connector as a document loader, allowing\nyou to load various Gong objects as documents.Installation‚ÄãFirst, you need to install the airbyte-source-gong python package.%pip install --upgrade --quiet  airbyte-source-gongExample‚ÄãCheck out the Airbyte documentation\npage for details\nabout how to configure the reader. The JSON schema the config object\nshould adhere to can be found on Github:\nhttps://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-gong/source_gong/spec.yaml.The general shape looks like this:{  ""access_key"": ""<access key name>"",  ""access_key_secret"": ""<access key secret>"",  ""start_date"": ""<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>"",}By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader.from langchain_community.document_loaders.airbyte import AirbyteGongLoaderconfig = {    # your gong configuration}loader = AirbyteGongLoader(    config=config, stream_name=""calls"")  # check the documentation linked above for a list of all streamsNow you can load documents the usual waydocs = loader.load()As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the\nlazy_load method which returns an iterator instead:docs_iterator = loader.lazy_load()Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To process\ndocuments, create a class inheriting from the base loader and implement\nthe _handle_records method yourself:from langchain.docstore.document import Documentdef handle_record(record, id):    return Document(page_content=record.data[""title""], metadata=record.data)loader = AirbyteGongLoader(    config=config, record_handler=handle_record, stream_name=""calls"")docs = loader.load()Incremental loads‚ÄãSome streams allow incremental loading, this means the source keeps\ntrack of synced records and won‚Äôt load them again. This is useful for\nsources that have a high volume of data and are updated frequently.To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded.last_state = loader.last_state  # store safelyincremental_loader = AirbyteGongLoader(    config=config, stream_name=""calls"", state=last_state)new_docs = incremental_loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/diffbot,Diffbot | ü¶úÔ∏èüîó Langchain,"Unlike traditional web scraping tools,","DiffbotUnlike traditional web scraping tools,\nDiffbot doesn‚Äôt require any rules to\nread the content on a page. It starts with computer vision, which\nclassifies a page into one of 20 possible types. Content is then\ninterpreted by a machine learning model trained to identify the key\nattributes on a page based on its type. The result is a website\ntransformed into clean structured data (like JSON or CSV), ready for\nyour application.This covers how to extract HTML documents from a list of URLs using the\nDiffbot extract API, into a\ndocument format that we can use downstream.urls = [    ""https://python.langchain.com/en/latest/index.html"",]The Diffbot Extract API Requires an API token. Once you have it, you can\nextract the data.Read instructions\nhow to get the Diffbot API Token.import osfrom langchain_community.document_loaders import DiffbotLoaderloader = DiffbotLoader(urls=urls, api_token=os.environ.get(""DIFFBOT_API_TOKEN""))With the .load() method, you can see the documents loadedloader.load()[Document(page_content='LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:\nBe data-aware: connect a language model to other sources of data\nBe agentic: allow a language model to interact with its environment\nThe LangChain framework is designed with the above principles in mind.\nThis is the Python specific portion of the documentation. For a purely conceptual guide to LangChain, see here. For the JavaScript documentation, see here.\nGetting Started\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\nGetting Started Documentation\nModules\nThere are several main modules that LangChain provides support for. For each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides. These modules are, in increasing order of complexity:\nModels: The various model types and model integrations LangChain supports.\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\nUse Cases\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\nExtraction: Extract structured information from text.\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\nReference Docs\nAll of LangChain‚Äôs reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\nReference Documentation\nLangChain Ecosystem\nGuides for how other companies/products can be used with LangChain\nLangChain Ecosystem\nAdditional Resources\nAdditional collection of resources we think may be useful as you develop your application!\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\nModel Laboratory: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\nDiscord: Join us on our Discord to discuss all things LangChain!\nProduction Support: As you move your LangChains into production, we‚Äôd love to offer more comprehensive support. Please fill out this form and we‚Äôll set up a dedicated support Slack channel.', metadata={'source': 'https://python.langchain.com/en/latest/index.html'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/arxiv,Arxiv | ü¶úÔ∏èüîó Langchain,arXiv is an open-access archive for 2 million,"ArxivarXiv is an open-access archive for 2 million\nscholarly articles in the fields of physics, mathematics, computer\nscience, quantitative biology, quantitative finance, statistics,\nelectrical engineering and systems science, and economics.This notebook shows how to load scientific articles from Arxiv.org\ninto a document format that we can use downstream.Installation‚ÄãFirst, you need to install arxiv python package.%pip install --upgrade --quiet  arxivSecond, you need to install PyMuPDF python package which transforms\nPDF files downloaded from the arxiv.org site into the text format.%pip install --upgrade --quiet  pymupdfExamples‚ÄãArxivLoader has these arguments: - query: free text which used to\nfind documents in the Arxiv - optional load_max_docs: default=100. Use\nit to limit number of downloaded documents. It takes time to download\nall 100 documents, so use a small number for experiments. - optional\nload_all_available_meta: default=False. By default only the most\nimportant fields downloaded: Published (date when document was\npublished/last updated), Title, Authors, Summary. If True, other\nfields also downloaded.from langchain_community.document_loaders import ArxivLoaderdocs = ArxivLoader(query=""1605.08386"", load_max_docs=2).load()len(docs)docs[0].metadata  # meta-information of the Document{'Published': '2016-05-26', 'Title': 'Heat-bath random walks with Markov bases', 'Authors': 'Caprice Stanley, Tobias Windisch', 'Summary': 'Graphs on lattice points are studied whose edges come from a finite set of\nallowed moves of arbitrary length. We show that the diameter of these graphs on\nfibers of a fixed integer matrix can be bounded from above by a constant. We\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\nalso state explicit conditions on the set of moves so that the heat-bath random\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\ndimension.'}docs[0].page_content[:400]  # all pages of the Document content'arXiv:1605.08386v1  [math.CO]  26 May 2016\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\nCAPRICE STANLEY AND TOBIAS WINDISCH\nAbstract. Graphs on lattice points are studied whose edges come from a Ô¨Ånite set of\nallowed moves of arbitrary length. We show that the diameter of these graphs on Ô¨Åbers of a\nÔ¨Åxed integer matrix can be bounded from above by a constant. We then study the mixing\nbehaviour of heat-b'",en,
https://python.langchain.com/docs/integrations/document_loaders/amazon_textract,Amazon Textract | ü¶úÔ∏èüîó Langchain,[Amazon,"Amazon TextractAmazon\nTextract\nis a machine learning (ML) service that automatically extracts text,\nhandwriting, and data from scanned documents.It goes beyond simple optical character recognition (OCR) to identify,\nunderstand, and extract data from forms and tables. Today, many\ncompanies manually extract data from scanned documents such as PDFs,\nimages, tables, and forms, or through simple OCR software that\nrequires manual configuration (which often must be updated when the\nform changes). To overcome these manual and expensive processes,\nTextract uses ML to read and process any type of document,\naccurately extracting text, handwriting, tables, and other data with\nno manual effort.This sample demonstrates the use of Amazon Textract in combination\nwith LangChain as a DocumentLoader.Textract supportsPDF, TIFF, PNG and JPEG format.Textract supports these document sizes, languages and\ncharacters.%pip install --upgrade --quiet  boto3 langchain-openai tiktoken python-dotenv%pip install --upgrade --quiet  ""amazon-textract-caller>=0.2.0""Sample 1‚ÄãThe first example uses a local file, which internally will be send to\nAmazon Textract sync API\nDetectDocumentText.Local files or URL endpoints like HTTP:// are limited to one page\ndocuments for Textract. Multi-page documents have to reside on S3. This\nsample file is a jpeg.from langchain_community.document_loaders import AmazonTextractPDFLoaderloader = AmazonTextractPDFLoader(""example_data/alejandro_rosalez_sample-small.jpeg"")documents = loader.load()Output from the filedocuments[Document(page_content='Patient Information First Name: ALEJANDRO Last Name: ROSALEZ Date of Birth: 10/10/1982 Sex: M Marital Status: MARRIED Email Address: Address: 123 ANY STREET City: ANYTOWN State: CA Zip Code: 12345 Phone: 646-555-0111 Emergency Contact 1: First Name: CARLOS Last Name: SALAZAR Phone: 212-555-0150 Relationship to Patient: BROTHER Emergency Contact 2: First Name: JANE Last Name: DOE Phone: 650-555-0123 Relationship FRIEND to Patient: Did you feel fever or feverish lately? Yes No Are you having shortness of breath? Yes No Do you have a cough? Yes No Did you experience loss of taste or smell? Yes No Where you in contact with any confirmed COVID-19 positive patients? Yes No Did you travel in the past 14 days to any regions affected by COVID-19? Yes No Patient Information First Name: ALEJANDRO Last Name: ROSALEZ Date of Birth: 10/10/1982 Sex: M Marital Status: MARRIED Email Address: Address: 123 ANY STREET City: ANYTOWN State: CA Zip Code: 12345 Phone: 646-555-0111 Emergency Contact 1: First Name: CARLOS Last Name: SALAZAR Phone: 212-555-0150 Relationship to Patient: BROTHER Emergency Contact 2: First Name: JANE Last Name: DOE Phone: 650-555-0123 Relationship FRIEND to Patient: Did you feel fever or feverish lately? Yes No Are you having shortness of breath? Yes No Do you have a cough? Yes No Did you experience loss of taste or smell? Yes No Where you in contact with any confirmed COVID-19 positive patients? Yes No Did you travel in the past 14 days to any regions affected by COVID-19? Yes No ', metadata={'source': 'example_data/alejandro_rosalez_sample-small.jpeg', 'page': 1})]Sample 2‚ÄãThe next sample loads a file from an HTTPS endpoint. It has to be single\npage, as Amazon Textract requires all multi-page documents to be stored\non S3.from langchain_community.document_loaders import AmazonTextractPDFLoaderloader = AmazonTextractPDFLoader(    ""https://amazon-textract-public-content.s3.us-east-2.amazonaws.com/langchain/alejandro_rosalez_sample_1.jpg"")documents = loader.load()documents[Document(page_content='Patient Information First Name: ALEJANDRO Last Name: ROSALEZ Date of Birth: 10/10/1982 Sex: M Marital Status: MARRIED Email Address: Address: 123 ANY STREET City: ANYTOWN State: CA Zip Code: 12345 Phone: 646-555-0111 Emergency Contact 1: First Name: CARLOS Last Name: SALAZAR Phone: 212-555-0150 Relationship to Patient: BROTHER Emergency Contact 2: First Name: JANE Last Name: DOE Phone: 650-555-0123 Relationship FRIEND to Patient: Did you feel fever or feverish lately? Yes No Are you having shortness of breath? Yes No Do you have a cough? Yes No Did you experience loss of taste or smell? Yes No Where you in contact with any confirmed COVID-19 positive patients? Yes No Did you travel in the past 14 days to any regions affected by COVID-19? Yes No Patient Information First Name: ALEJANDRO Last Name: ROSALEZ Date of Birth: 10/10/1982 Sex: M Marital Status: MARRIED Email Address: Address: 123 ANY STREET City: ANYTOWN State: CA Zip Code: 12345 Phone: 646-555-0111 Emergency Contact 1: First Name: CARLOS Last Name: SALAZAR Phone: 212-555-0150 Relationship to Patient: BROTHER Emergency Contact 2: First Name: JANE Last Name: DOE Phone: 650-555-0123 Relationship FRIEND to Patient: Did you feel fever or feverish lately? Yes No Are you having shortness of breath? Yes No Do you have a cough? Yes No Did you experience loss of taste or smell? Yes No Where you in contact with any confirmed COVID-19 positive patients? Yes No Did you travel in the past 14 days to any regions affected by COVID-19? Yes No ', metadata={'source': 'example_data/alejandro_rosalez_sample-small.jpeg', 'page': 1})]Sample 3‚ÄãProcessing a multi-page document requires the document to be on S3. The\nsample document resides in a bucket in us-east-2 and Textract needs to\nbe called in that same region to be successful, so we set the\nregion_name on the client and pass that in to the loader to ensure\nTextract is called from us-east-2. You could also to have your notebook\nrunning in us-east-2, setting the AWS_DEFAULT_REGION set to us-east-2 or\nwhen running in a different environment, pass in a boto3 Textract client\nwith that region name like in the cell below.import boto3textract_client = boto3.client(""textract"", region_name=""us-east-2"")file_path = ""s3://amazon-textract-public-content/langchain/layout-parser-paper.pdf""loader = AmazonTextractPDFLoader(file_path, client=textract_client)documents = loader.load()Now getting the number of pages to validate the response (printing out\nthe full response would be quite long‚Ä¶). We expect 16 pages.len(documents)16Sample 4‚ÄãYou have the option to pass an additional parameter called\nlinearization_config to the AmazonTextractPDFLoader which will\ndetermine how the the text output will be linearized by the parser after\nTextract runs.from langchain_community.document_loaders import AmazonTextractPDFLoaderfrom textractor.data.text_linearization_config import TextLinearizationConfigloader = AmazonTextractPDFLoader(    ""s3://amazon-textract-public-content/langchain/layout-parser-paper.pdf"",    linearization_config=TextLinearizationConfig(        hide_header_layout=True,        hide_footer_layout=True,        hide_figure_layout=True,    ),)documents = loader.load()Using the AmazonTextractPDFLoader in an LangChain chain (e. g. OpenAI)‚ÄãThe AmazonTextractPDFLoader can be used in a chain the same way the\nother loaders are used. Textract itself does have a Query\nfeature,\nwhich offers similar functionality to the QA chain in this sample, which\nis worth checking out as well.# You can store your OPENAI_API_KEY in a .env file as well# import os# from dotenv import load_dotenv# load_dotenv()# Or set the OpenAI key in the environment directlyimport osos.environ[""OPENAI_API_KEY""] = ""your-OpenAI-API-key""from langchain.chains.question_answering import load_qa_chainfrom langchain_openai import OpenAIchain = load_qa_chain(llm=OpenAI(), chain_type=""map_reduce"")query = [""Who are the autors?""]chain.run(input_documents=documents, question=query)' The authors are Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin Charles Germain Lee, Jacob Carlson, Weining Li, Gardner, M., Grus, J., Neumann, M., Tafjord, O., Dasigi, P., Liu, N., Peters, M., Schmitz, M., Zettlemoyer, L., Lukasz Garncarek, Powalski, R., Stanislawek, T., Topolski, B., Halama, P., Gralinski, F., Graves, A., Fern√°ndez, S., Gomez, F., Schmidhuber, J., Harley, A.W., Ufkes, A., Derpanis, K.G., He, K., Gkioxari, G., Doll√°r, P., Girshick, R., He, K., Zhang, X., Ren, S., Sun, J., Kay, A., Lamiroy, B., Lopresti, D., Mears, J., Jakeway, E., Ferriter, M., Adams, C., Yarasavage, N., Thomas, D., Zwaard, K., Li, M., Cui, L., Huang,'",en,
https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive,Microsoft OneDrive | ü¶úÔ∏èüîó Langchain,Microsoft OneDrive (formerly,"Microsoft OneDriveMicrosoft OneDrive (formerly\nSkyDrive) is a file hosting service operated by Microsoft.This notebook covers how to load documents from OneDrive. Currently,\nonly docx, doc, and pdf files are supported.Prerequisites‚ÄãRegister an application with the Microsoft identity\nplatform\ninstructions.When registration finishes, the Azure portal displays the app\nregistration‚Äôs Overview pane. You see the Application (client) ID.\nAlso called the client ID, this value uniquely identifies your\napplication in the Microsoft identity platform.During the steps you will be following at item 1, you can set\nthe redirect URI as http://localhost:8000/callbackDuring the steps you will be following at item 1, generate a new\npassword (client_secret) under¬†Application Secrets¬†section.Follow the instructions at this\ndocument\nto add the following SCOPES (offline_access and\nFiles.Read.All) to your application.Visit the Graph Explorer\nPlayground\nto obtain your OneDrive ID. The first step is to ensure you are\nlogged in with the account associated your OneDrive account. Then\nyou need to make a request to\nhttps://graph.microsoft.com/v1.0/me/drive and the response will\nreturn a payload with a field id that holds the ID of your\nOneDrive account.You need to install the o365 package using the command\npip install o365.At the end of the steps you must have the following values:CLIENT_IDCLIENT_SECRETDRIVE_IDüßë Instructions for ingesting your documents from OneDrive‚Äãüîë Authentication‚ÄãBy default, the OneDriveLoader expects that the values of CLIENT_ID\nand CLIENT_SECRET must be stored as environment variables named\nO365_CLIENT_ID and O365_CLIENT_SECRET respectively. You could pass\nthose environment variables through a .env file at the root of your\napplication or using the following command in your script.os.environ['O365_CLIENT_ID'] = ""YOUR CLIENT ID""os.environ['O365_CLIENT_SECRET'] = ""YOUR CLIENT SECRET""This loader uses an authentication called on behalf of a\nuser.\nIt is a 2 step authentication with user consent. When you instantiate\nthe loader, it will call will print a url that the user must visit to\ngive consent to the app on the required permissions. The user must then\nvisit this url and give consent to the application. Then the user must\ncopy the resulting page url and paste it back on the console. The method\nwill then return True if the login attempt was successful.from langchain_community.document_loaders.onedrive import OneDriveLoaderloader = OneDriveLoader(drive_id=""YOUR DRIVE ID"")Once the authentication has been done, the loader will store a token\n(o365_token.txt) at ~/.credentials/ folder. This token could be used\nlater to authenticate without the copy/paste steps explained earlier. To\nuse this token for authentication, you need to change the\nauth_with_token parameter to True in the instantiation of the loader.from langchain_community.document_loaders.onedrive import OneDriveLoaderloader = OneDriveLoader(drive_id=""YOUR DRIVE ID"", auth_with_token=True)üóÇÔ∏è Documents loader‚Äãüìë Loading documents from a OneDrive Directory‚ÄãOneDriveLoader can load documents from a specific folder within your\nOneDrive. For instance, you want to load all documents that are stored\nat Documents/clients folder within your OneDrive.from langchain_community.document_loaders.onedrive import OneDriveLoaderloader = OneDriveLoader(drive_id=""YOUR DRIVE ID"", folder_path=""Documents/clients"", auth_with_token=True)documents = loader.load()üìë Loading documents from a list of Documents IDs‚ÄãAnother possibility is to provide a list of object_id for each\ndocument you want to load. For that, you will need to query the\nMicrosoft Graph\nAPI to find\nall the documents ID that you are interested in. This\nlink\nprovides a list of endpoints that will be helpful to retrieve the\ndocuments ID.For instance, to retrieve information about all objects that are stored\nat the root of the Documents folder, you need make a request to:\nhttps://graph.microsoft.com/v1.0/drives/{YOUR DRIVE ID}/root/children.\nOnce you have the list of IDs that you are interested in, then you can\ninstantiate the loader with the following parameters.from langchain_community.document_loaders.onedrive import OneDriveLoaderloader = OneDriveLoader(drive_id=""YOUR DRIVE ID"", object_ids=[""ID_1"", ""ID_2""], auth_with_token=True)documents = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/trello,Trello | ü¶úÔ∏èüîó Langchain,Trello is a web-based,"TrelloTrello is a web-based\nproject management and collaboration tool that allows individuals and\nteams to organize and track their tasks and projects. It provides a\nvisual interface known as a ‚Äúboard‚Äù where users can create lists and\ncards to represent their tasks and activities.The TrelloLoader allows you to load cards from a Trello board and is\nimplemented on top of py-trelloThis currently supports api_key/token only.Credentials generation: https://trello.com/power-ups/admin/Click in the manual token generation link to get the token.To specify the API key and token you can either set the environment\nvariables TRELLO_API_KEY and TRELLO_TOKEN or you can pass api_key\nand token directly into the from_credentials convenience constructor\nmethod.This loader allows you to provide the board name to pull in the\ncorresponding cards into Document objects.Notice that the board ‚Äúname‚Äù is also called ‚Äútitle‚Äù in oficial\ndocumentation:https://support.atlassian.com/trello/docs/changing-a-boards-title-and-description/You can also specify several load parameters to include / remove\ndifferent fields both from the document page_content properties and\nmetadata.Features‚ÄãLoad cards from a Trello board.Filter cards based on their status (open or closed).Include card names, comments, and checklists in the loaded\ndocuments.Customize the additional metadata fields to include in the document.By default all card fields are included for the full text page_content\nand metadata accordinly.%pip install --upgrade --quiet  py-trello beautifulsoup4 lxml# If you have already set the API key and token using environment variables,# you can skip this cell and comment out the `api_key` and `token` named arguments# in the initialization steps below.from getpass import getpassAPI_KEY = getpass()TOKEN = getpass()¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑from langchain_community.document_loaders import TrelloLoader# Get the open cards from ""Awesome Board""loader = TrelloLoader.from_credentials(    ""Awesome Board"",    api_key=API_KEY,    token=TOKEN,    card_filter=""open"",)documents = loader.load()print(documents[0].page_content)print(documents[0].metadata)Review Tech partner pagesComments:{'title': 'Review Tech partner pages', 'id': '6475357890dc8d17f73f2dcc', 'url': 'https://trello.com/c/b0OTZwkZ/1-review-tech-partner-pages', 'labels': ['Demand Marketing'], 'list': 'Done', 'closed': False, 'due_date': ''}# Get all the cards from ""Awesome Board"" but only include the# card list(column) as extra metadata.loader = TrelloLoader.from_credentials(    ""Awesome Board"",    api_key=API_KEY,    token=TOKEN,    extra_metadata=(""list""),)documents = loader.load()print(documents[0].page_content)print(documents[0].metadata)Review Tech partner pagesComments:{'title': 'Review Tech partner pages', 'id': '6475357890dc8d17f73f2dcc', 'url': 'https://trello.com/c/b0OTZwkZ/1-review-tech-partner-pages', 'list': 'Done'}# Get the cards from ""Another Board"" and exclude the card name,# checklist and comments from the Document page_content text.loader = TrelloLoader.from_credentials(    ""test"",    api_key=API_KEY,    token=TOKEN,    include_card_name=False,    include_checklist=False,    include_comments=False,)documents = loader.load()print(""Document: "" + documents[0].page_content)print(documents[0].metadata)",en,
https://python.langchain.com/docs/integrations/document_loaders/google_memorystore_redis,Google Memorystore for Redis | ü¶úÔ∏èüîó Langchain,[Google Memorystore for,"Google Memorystore for RedisGoogle Memorystore for\nRedis\nis a fully-managed service that is powered by the Redis in-memory data\nstore to build application caches that provide sub-millisecond data\naccess. Extend your database application to build AI-powered\nexperiences leveraging Memorystore for Redis‚Äôs Langchain integrations.This notebook goes over how to use Memorystore for\nRedis\nto save, load and delete langchain\ndocuments with\nMemorystoreDocumentLoader and MemorystoreDocumentSaver.Learn more about the package on\nGitHub.Open In ColabBefore You Begin‚ÄãTo run this notebook, you will need to do the following:Create a Google Cloud\nProjectEnable the Memorystore for Redis\nAPICreate a Memorystore for Redis\ninstance.\nEnsure that the version is greater than or equal to 5.0.After confirmed access to database in the runtime environment of this\nnotebook, filling the following values and run the cell before running\nexample scripts.# @markdown Please specify an endpoint associated with the instance and a key prefix for demo purpose.ENDPOINT = ""redis://127.0.0.1:6379""  # @param {type:""string""}KEY_PREFIX = ""doc:""  # @param {type:""string""}ü¶úüîó Library Installation‚ÄãThe integration lives in its own langchain-google-memorystore-redis\npackage, so we need to install it.%pip install -upgrade --quiet langchain-google-memorystore-redisColab only: Uncomment the following cell to restart the kernel or\nuse the button to restart the kernel. For Vertex AI Workbench you can\nrestart the terminal using the button on top.# # Automatically restart kernel after installs so that your environment can access the new packages# import IPython# app = IPython.Application.instance()# app.kernel.do_shutdown(True)‚òÅ Set Your Google Cloud Project‚ÄãSet your Google Cloud project so that you can leverage Google Cloud\nresources within this notebook.If you don‚Äôt know your project ID, try the following:Run gcloud config list.Run gcloud projects list.See the support page: Locate the project\nID.# @markdown Please fill in the value below with your Google Cloud project ID and then run the cell.PROJECT_ID = ""my-project-id""  # @param {type:""string""}# Set the project id!gcloud config set project {PROJECT_ID}üîê Authentication‚ÄãAuthenticate to Google Cloud as the IAM user logged into this notebook\nin order to access your Google Cloud Project.If you are using Colab to run this notebook, use the cell below and\ncontinue.If you are using Vertex AI Workbench, check out the setup\ninstructions\nhere.from google.colab import authauth.authenticate_user()Basic Usage‚ÄãSave documents‚ÄãSave langchain documents with\nMemorystoreDocumentSaver.add_documents(<documents>). To initialize\nMemorystoreDocumentSaver class you need to provide 2 things:client - A redis.Redis client object.key_prefix - A prefix for the keys to store Documents in Redis.The Documents will be stored into randomly generated keys with the\nspecified prefix of key_prefix. Alternatively, you can designate the\nsuffixes of the keys by specifying ids in the add_documents method.import redisfrom langchain_core.documents import Documentfrom langchain_google_memorystore_redis import MemorystoreDocumentSavertest_docs = [    Document(        page_content=""Apple Granny Smith 150 0.99 1"",        metadata={""fruit_id"": 1},    ),    Document(        page_content=""Banana Cavendish 200 0.59 0"",        metadata={""fruit_id"": 2},    ),    Document(        page_content=""Orange Navel 80 1.29 1"",        metadata={""fruit_id"": 3},    ),]doc_ids = [f""{i}"" for i in range(len(test_docs))]redis_client = redis.from_url(ENDPOINT)saver = MemorystoreDocumentSaver(    client=redis_client,    key_prefix=KEY_PREFIX,    content_field=""page_content"",)saver.add_documents(test_docs, ids=doc_ids)Load documents‚ÄãInitialize a loader that loads all documents stored in the Memorystore\nfor Redis instance with a specific prefix.Load langchain documents with MemorystoreDocumentLoader.load() or\nMemorystoreDocumentLoader.lazy_load(). lazy_load returns a generator\nthat only queries database during the iteration. To initialize\nMemorystoreDocumentLoader class you need to provide:client - A redis.Redis client object.key_prefix - A prefix for the keys to store Documents in Redis.import redisfrom langchain_google_memorystore_redis import MemorystoreDocumentLoaderredis_client = redis.from_url(ENDPOINT)loader = MemorystoreDocumentLoader(    client=redis_client,    key_prefix=KEY_PREFIX,    content_fields=set([""page_content""]),)for doc in loader.lazy_load():    print(""Loaded documents:"", doc)Delete documents‚ÄãDelete all of keys with the specified prefix in the Memorystore for\nRedis instance with MemorystoreDocumentSaver.delete(). You can also\nspecify the suffixes of the keys if you know.docs = loader.load()print(""Documents before delete:"", docs)saver.delete(ids=[0])print(""Documents after delete:"", loader.load())saver.delete()print(""Documents after delete all:"", loader.load())Advanced Usage‚ÄãCustomize Document Page Content & Metadata‚ÄãWhen initializing a loader with more than 1 content field, the\npage_content of the loaded Documents will contain a JSON-encoded\nstring with top level fields equal to the specified fields in\ncontent_fields.If the metadata_fields are specified, the metadata field of the\nloaded Documents will only have the top level fields equal to the\nspecified metadata_fields. If any of the values of the metadata fields\nis stored as a JSON-encoded string, it will be decoded prior to being\nloaded to the metadata fields.loader = MemorystoreDocumentLoader(    client=redis_client,    key_prefix=KEY_PREFIX,    content_fields=set([""content_field_1"", ""content_field_2""]),    metadata_fields=set([""title"", ""author""]),)",en,
https://python.langchain.com/docs/integrations/document_loaders/fauna,Fauna | ü¶úÔ∏èüîó Langchain,Fauna is a Document Database.,"FaunaFauna is a Document Database.Query Fauna documents%pip install --upgrade --quiet  faunaQuery data example‚Äãfrom langchain_community.document_loaders.fauna import FaunaLoadersecret = ""<enter-valid-fauna-secret>""query = ""Item.all()""  # Fauna query. Assumes that the collection is called ""Item""field = ""text""  # The field that contains the page content. Assumes that the field is called ""text""loader = FaunaLoader(query, field, secret)docs = loader.lazy_load()for value in docs:    print(value)Query with Pagination‚ÄãYou get a after value if there are more data. You can get values after\nthe curcor by passing in the after string in query.To learn more following this\nlinkquery = """"""Item.paginate(""hs+DzoPOg ... aY1hOohozrV7A"")Item.all()""""""loader = FaunaLoader(query, field, secret)",en,
https://python.langchain.com/docs/integrations/document_loaders/airbyte,AirbyteLoader | ü¶úÔ∏èüîó Langchain,Airbyte is a data integration,"AirbyteLoaderAirbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases.This covers how to load any source from Airbyte into LangChain documentsInstallation‚ÄãIn order to use AirbyteLoader you need to install the\nlangchain-airbyte integration package.% pip install -qU langchain-airbyteNote: Currently, the airbyte library does not support Pydantic v2.\nPlease downgrade to Pydantic v1 to use this package.Note: This package also currently requires Python 3.10+.Loading Documents‚ÄãBy default, the AirbyteLoader will load any structured data from a\nstream and output yaml-formatted documents.from langchain_airbyte import AirbyteLoaderloader = AirbyteLoader(    source=""source-faker"",    stream=""users"",    config={""count"": 10},)docs = loader.load()print(docs[0].page_content[:500])```yamlacademic_degree: PhDaddress:  city: Lauderdale Lakes  country_code: FI  postal_code: '75466'  province: New Jersey  state: Hawaii  street_name: Stoneyford  street_number: '1112'age: 44blood_type: ""O\u2212""created_at: '2004-04-02T13:05:27+00:00'email: bread2099+1@outlook.comgender: Fluidheight: '1.62'id: 1language: Belarusianname: Mosesnationality: Dutchoccupation: Track Workertelephone: 1-467-194-2318title: M.Sc.Tech.updated_at: '2024-02-27T16:41:01+00:00'weight: 6You can also specify a custom prompt template for formatting documents:from langchain_core.prompts import PromptTemplateloader_templated = AirbyteLoader(    source=""source-faker"",    stream=""users"",    config={""count"": 10},    template=PromptTemplate.from_template(        ""My name is {name} and I am {height} meters tall.""    ),)docs_templated = loader_templated.load()print(docs_templated[0].page_content)My name is Verdie and I am 1.73 meters tall.Lazy Loading Documents‚ÄãOne of the powerful features of AirbyteLoader is its ability to load\nlarge documents from upstream sources. When working with large datasets,\nthe default .load() behavior can be slow and memory-intensive. To\navoid this, you can use the .lazy_load() method to load documents in a\nmore memory-efficient manner.import timeloader = AirbyteLoader(    source=""source-faker"",    stream=""users"",    config={""count"": 3},    template=PromptTemplate.from_template(        ""My name is {name} and I am {height} meters tall.""    ),)start_time = time.time()my_iterator = loader.lazy_load()print(    f""Just calling lazy load is quick! This took {time.time() - start_time:.4f} seconds"")Just calling lazy load is quick! This took 0.0001 secondsAnd you can iterate over documents as they‚Äôre yielded:for doc in my_iterator:    print(doc.page_content)My name is Andera and I am 1.91 meters tall.My name is Jody and I am 1.85 meters tall.My name is Zonia and I am 1.53 meters tall.You can also lazy load documents in an async manner with\n.alazy_load():loader = AirbyteLoader(    source=""source-faker"",    stream=""users"",    config={""count"": 3},    template=PromptTemplate.from_template(        ""My name is {name} and I am {height} meters tall.""    ),)my_async_iterator = loader.alazy_load()async for doc in my_async_iterator:    print(doc.page_content)My name is Carmelina and I am 1.74 meters tall.My name is Ali and I am 1.90 meters tall.My name is Rochell and I am 1.83 meters tall.Configuration‚ÄãAirbyteLoader can be configured with the following options:source (str, required): The name of the Airbyte source to load\nfrom.stream (str, required): The name of the stream to load from\n(Airbyte sources can return multiple streams)config (dict, required): The configuration for the Airbyte sourcetemplate (PromptTemplate, optional): A custom prompt template for\nformatting documentsinclude_metadata (bool, optional, default True): Whether to\ninclude all fields as metadata in the output documentsThe majority of the configuration will be in config, and you can find\nthe specific configuration options in the ‚ÄúConfig field reference‚Äù for\neach source in the Airbyte\ndocumentation.",en,
https://python.langchain.com/docs/integrations/document_loaders/pyspark_dataframe,PySpark | ü¶úÔ∏èüîó Langchain,This notebook goes over how to load data from a,"PySparkThis notebook goes over how to load data from a\nPySpark DataFrame.%pip install --upgrade --quiet  pysparkfrom pyspark.sql import SparkSessionspark = SparkSession.builder.getOrCreate()Setting default log level to ""WARN"".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).23/05/31 14:08:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicabledf = spark.read.csv(""example_data/mlb_teams_2012.csv"", header=True)from langchain_community.document_loaders import PySparkDataFrameLoaderloader = PySparkDataFrameLoader(spark, df, page_content_column=""Team"")loader.load()[Stage 8:>                                                          (0 + 1) / 1][Document(page_content='Nationals', metadata={' ""Payroll (millions)""': '     81.34', ' ""Wins""': ' 98'}), Document(page_content='Reds', metadata={' ""Payroll (millions)""': '          82.20', ' ""Wins""': ' 97'}), Document(page_content='Yankees', metadata={' ""Payroll (millions)""': '      197.96', ' ""Wins""': ' 95'}), Document(page_content='Giants', metadata={' ""Payroll (millions)""': '       117.62', ' ""Wins""': ' 94'}), Document(page_content='Braves', metadata={' ""Payroll (millions)""': '        83.31', ' ""Wins""': ' 94'}), Document(page_content='Athletics', metadata={' ""Payroll (millions)""': '     55.37', ' ""Wins""': ' 94'}), Document(page_content='Rangers', metadata={' ""Payroll (millions)""': '      120.51', ' ""Wins""': ' 93'}), Document(page_content='Orioles', metadata={' ""Payroll (millions)""': '       81.43', ' ""Wins""': ' 93'}), Document(page_content='Rays', metadata={' ""Payroll (millions)""': '          64.17', ' ""Wins""': ' 90'}), Document(page_content='Angels', metadata={' ""Payroll (millions)""': '       154.49', ' ""Wins""': ' 89'}), Document(page_content='Tigers', metadata={' ""Payroll (millions)""': '       132.30', ' ""Wins""': ' 88'}), Document(page_content='Cardinals', metadata={' ""Payroll (millions)""': '    110.30', ' ""Wins""': ' 88'}), Document(page_content='Dodgers', metadata={' ""Payroll (millions)""': '       95.14', ' ""Wins""': ' 86'}), Document(page_content='White Sox', metadata={' ""Payroll (millions)""': '     96.92', ' ""Wins""': ' 85'}), Document(page_content='Brewers', metadata={' ""Payroll (millions)""': '       97.65', ' ""Wins""': ' 83'}), Document(page_content='Phillies', metadata={' ""Payroll (millions)""': '     174.54', ' ""Wins""': ' 81'}), Document(page_content='Diamondbacks', metadata={' ""Payroll (millions)""': '  74.28', ' ""Wins""': ' 81'}), Document(page_content='Pirates', metadata={' ""Payroll (millions)""': '       63.43', ' ""Wins""': ' 79'}), Document(page_content='Padres', metadata={' ""Payroll (millions)""': '        55.24', ' ""Wins""': ' 76'}), Document(page_content='Mariners', metadata={' ""Payroll (millions)""': '      81.97', ' ""Wins""': ' 75'}), Document(page_content='Mets', metadata={' ""Payroll (millions)""': '          93.35', ' ""Wins""': ' 74'}), Document(page_content='Blue Jays', metadata={' ""Payroll (millions)""': '     75.48', ' ""Wins""': ' 73'}), Document(page_content='Royals', metadata={' ""Payroll (millions)""': '        60.91', ' ""Wins""': ' 72'}), Document(page_content='Marlins', metadata={' ""Payroll (millions)""': '      118.07', ' ""Wins""': ' 69'}), Document(page_content='Red Sox', metadata={' ""Payroll (millions)""': '      173.18', ' ""Wins""': ' 69'}), Document(page_content='Indians', metadata={' ""Payroll (millions)""': '       78.43', ' ""Wins""': ' 68'}), Document(page_content='Twins', metadata={' ""Payroll (millions)""': '         94.08', ' ""Wins""': ' 66'}), Document(page_content='Rockies', metadata={' ""Payroll (millions)""': '       78.06', ' ""Wins""': ' 64'}), Document(page_content='Cubs', metadata={' ""Payroll (millions)""': '          88.19', ' ""Wins""': ' 61'}), Document(page_content='Astros', metadata={' ""Payroll (millions)""': '        60.65', ' ""Wins""': ' 55'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/google_cloud_sql_mysql,Google Cloud SQL for MySQL | ü¶úÔ∏èüîó Langchain,Cloud SQL is a fully managed,"Google Cloud SQL for MySQLCloud SQL is a fully managed\nrelational database service that offers high performance, seamless\nintegration, and impressive scalability. It offers\nMySQL,\nPostgreSQL, and SQL\nServer database engines.\nExtend your database application to build AI-powered experiences\nleveraging Cloud SQL‚Äôs Langchain integrations.This notebook goes over how to use Cloud SQL for\nMySQL to save, load and delete\nlangchain documents\nwith MySQLLoader and MySQLDocumentSaver.Learn more about the package on\nGitHub.Open In ColabBefore You Begin‚ÄãTo run this notebook, you will need to do the following:Create a Google Cloud\nProjectEnable the Cloud SQL Admin\nAPI.Create a Cloud SQL for MySQL\ninstanceCreate a Cloud SQL\ndatabaseAdd an IAM database user to the\ndatabase\n(Optional)After confirmed access to database in the runtime environment of this\nnotebook, filling the following values and run the cell before running\nexample scripts.# @markdown Please fill in the both the Google Cloud region and name of your Cloud SQL instance.REGION = ""us-central1""  # @param {type:""string""}INSTANCE = ""test-instance""  # @param {type:""string""}# @markdown Please specify a database and a table for demo purpose.DATABASE = ""test""  # @param {type:""string""}TABLE_NAME = ""test-default""  # @param {type:""string""}ü¶úüîó Library Installation‚ÄãThe integration lives in its own langchain-google-cloud-sql-mysql\npackage, so we need to install it.%pip install -upgrade --quiet langchain-google-cloud-sql-mysqlColab only: Uncomment the following cell to restart the kernel or\nuse the button to restart the kernel. For Vertex AI Workbench you can\nrestart the terminal using the button on top.# # Automatically restart kernel after installs so that your environment can access the new packages# import IPython# app = IPython.Application.instance()# app.kernel.do_shutdown(True)‚òÅ Set Your Google Cloud Project‚ÄãSet your Google Cloud project so that you can leverage Google Cloud\nresources within this notebook.If you don‚Äôt know your project ID, try the following:Run gcloud config list.Run gcloud projects list.See the support page: Locate the project\nID.# @markdown Please fill in the value below with your Google Cloud project ID and then run the cell.PROJECT_ID = ""my-project-id""  # @param {type:""string""}# Set the project id!gcloud config set project {PROJECT_ID}üîê Authentication‚ÄãAuthenticate to Google Cloud as the IAM user logged into this notebook\nin order to access your Google Cloud Project.If you are using Colab to run this notebook, use the cell below and\ncontinue.If you are using Vertex AI Workbench, check out the setup\ninstructions\nhere.from google.colab import authauth.authenticate_user()Basic Usage‚ÄãMySQLEngine Connection Pool‚ÄãBefore saving or loading documents from MySQL table, we need first\nconfigures a connection pool to Cloud SQL database. The MySQLEngine\nconfigures a connection pool to your Cloud SQL database, enabling\nsuccessful connections from your application and following industry best\npractices.To create a MySQLEngine using MySQLEngine.from_instance() you need\nto provide only 4 things:project_id : Project ID of the Google Cloud Project where the\nCloud SQL instance is located.region : Region where the Cloud SQL instance is located.instance : The name of the Cloud SQL instance.database : The name of the database to connect to on the Cloud SQL\ninstance.By default, IAM database\nauthentication\nwill be used as the method of database authentication. This library uses\nthe IAM principal belonging to the Application Default Credentials\n(ADC)\nsourced from the envionment.For more informatin on IAM database authentication please see:Configure an instance for IAM database\nauthenticationManage users with IAM database\nauthenticationOptionally, built-in database\nauthentication\nusing a username and password to access the Cloud SQL database can also\nbe used. Just provide the optional user and password arguments to\nMySQLEngine.from_instance():user : Database user to use for built-in database authentication\nand loginpassword : Database password to use for built-in database\nauthentication and login.from langchain_google_cloud_sql_mysql import MySQLEngineengine = MySQLEngine.from_instance(    project_id=PROJECT_ID, region=REGION, instance=INSTANCE, database=DATABASE)Initialize a table‚ÄãInitialize a table of default schema via\nMySQLEngine.init_document_table(<table_name>). Table Columns:page_content (type: text)langchain_metadata (type: JSON)overwrite_existing=True flag means the newly initialized table will\nreplace any existing table of the same name.engine.init_document_table(TABLE_NAME, overwrite_existing=True)Save documents‚ÄãSave langchain documents with\nMySQLDocumentSaver.add_documents(<documents>). To initialize\nMySQLDocumentSaver class you need to provide 2 things:engine - An instance of a MySQLEngine engine.table_name - The name of the table within the Cloud SQL database\nto store langchain documents.from langchain_core.documents import Documentfrom langchain_google_cloud_sql_mysql import MySQLDocumentSavertest_docs = [    Document(        page_content=""Apple Granny Smith 150 0.99 1"",        metadata={""fruit_id"": 1},    ),    Document(        page_content=""Banana Cavendish 200 0.59 0"",        metadata={""fruit_id"": 2},    ),    Document(        page_content=""Orange Navel 80 1.29 1"",        metadata={""fruit_id"": 3},    ),]saver = MySQLDocumentSaver(engine=engine, table_name=TABLE_NAME)saver.add_documents(test_docs)Load documents‚ÄãLoad langchain documents with MySQLLoader.load() or\nMySQLLoader.lazy_load(). lazy_load returns a generator that only\nqueries database during the iteration. To initialize MySQLLoader class\nyou need to provide:engine - An instance of a MySQLEngine engine.table_name - The name of the table within the Cloud SQL database\nto store langchain documents.from langchain_google_cloud_sql_mysql import MySQLLoaderloader = MySQLLoader(engine=engine, table_name=TABLE_NAME)docs = loader.lazy_load()for doc in docs:    print(""Loaded documents:"", doc)Load documents via query‚ÄãOther than loading documents from a table, we can also choose to load\ndocuments from a view generated from a SQL query. For example:from langchain_google_cloud_sql_mysql import MySQLLoaderloader = MySQLLoader(    engine=engine,    query=f""select * from `{TABLE_NAME}` where JSON_EXTRACT(langchain_metadata, '$.fruit_id') = 1;"",)onedoc = loader.load()onedocThe view generated from SQL query can have different schema than default\ntable. In such cases, the behavior of MySQLLoader is the same as loading\nfrom table with non-default schema. Please refer to section Load\ndocuments with customized document page content &\nmetadata.Delete documents‚ÄãDelete a list of langchain documents from MySQL table with\nMySQLDocumentSaver.delete(<documents>).For table with default schema (page_content, langchain_metadata), the\ndeletion criteria is:A row should be deleted if there exists a document in the list, such\nthatdocument.page_content equals row[page_content]document.metadata equals row[langchain_metadata]from langchain_google_cloud_sql_mysql import MySQLLoaderloader = MySQLLoader(engine=engine, table_name=TABLE_NAME)docs = loader.load()print(""Documents before delete:"", docs)saver.delete(onedoc)print(""Documents after delete:"", loader.load())Advanced Usage‚ÄãLoad documents with customized document page content & metadata‚ÄãFirst we prepare an example table with non-default schema, and populate\nit with some arbitrary data.import sqlalchemywith engine.connect() as conn:    conn.execute(sqlalchemy.text(f""DROP TABLE IF EXISTS `{TABLE_NAME}`""))    conn.commit()    conn.execute(        sqlalchemy.text(            f""""""            CREATE TABLE IF NOT EXISTS `{TABLE_NAME}`(                fruit_id INT AUTO_INCREMENT PRIMARY KEY,                fruit_name VARCHAR(100) NOT NULL,                variety VARCHAR(50),                quantity_in_stock INT NOT NULL,                price_per_unit DECIMAL(6,2) NOT NULL,                organic TINYINT(1) NOT NULL            )            """"""        )    )    conn.execute(        sqlalchemy.text(            f""""""            INSERT INTO `{TABLE_NAME}` (fruit_name, variety, quantity_in_stock, price_per_unit, organic)            VALUES                ('Apple', 'Granny Smith', 150, 0.99, 1),                ('Banana', 'Cavendish', 200, 0.59, 0),                ('Orange', 'Navel', 80, 1.29, 1);            """"""        )    )    conn.commit()If we still load langchain documents with default parameters of\nMySQLLoader from this example table, the page_content of loaded\ndocuments will be the first column of the table, and metadata will be\nconsisting of key-value pairs of all the other columns.loader = MySQLLoader(    engine=engine,    table_name=TABLE_NAME,)loader.load()We can specify the content and metadata we want to load by setting the\ncontent_columns and metadata_columns when initializing the\nMySQLLoader.content_columns: The columns to write into the page_content of\nthe document.metadata_columns: The columns to write into the metadata of the\ndocument.For example here, the values of columns in content_columns will be\njoined together into a space-separated string, as page_content of\nloaded documents, and metadata of loaded documents will only contain\nkey-value pairs of columns specified in metadata_columns.loader = MySQLLoader(    engine=engine,    table_name=TABLE_NAME,    content_columns=[        ""variety"",        ""quantity_in_stock"",        ""price_per_unit"",        ""organic"",    ],    metadata_columns=[""fruit_id"", ""fruit_name""],)loader.load()Save document with customized page content & metadata‚ÄãIn order to save langchain document into table with customized metadata\nfields. We need first create such a table via\nMySQLEngine.init_document_table(), and specify the list of\nmetadata_columns we want it to have. In this example, the created\ntable will have table columns:description (type: text): for storing fruit description.fruit_name (type text): for storing fruit name.organic (type tinyint(1)): to tell if the fruit is organic.other_metadata (type: JSON): for storing other metadata information\nof the fruit.We can use the following parameters with\nMySQLEngine.init_document_table() to create the table:table_name: The name of the table within the Cloud SQL database to\nstore langchain documents.metadata_columns: A list of sqlalchemy.Column indicating the\nlist of metadata columns we need.content_column: The name of column to store page_content of\nlangchain document. Default: page_content.metadata_json_column: The name of JSON column to store extra\nmetadata of langchain document. Default: langchain_metadata.engine.init_document_table(    TABLE_NAME,    metadata_columns=[        sqlalchemy.Column(            ""fruit_name"",            sqlalchemy.UnicodeText,            primary_key=False,            nullable=True,        ),        sqlalchemy.Column(            ""organic"",            sqlalchemy.Boolean,            primary_key=False,            nullable=True,        ),    ],    content_column=""description"",    metadata_json_column=""other_metadata"",    overwrite_existing=True,)Save documents with MySQLDocumentSaver.add_documents(<documents>). As\nyou can see in this example,document.page_content will be saved into description column.document.metadata.fruit_name will be saved into fruit_name\ncolumn.document.metadata.organic will be saved into organic column.document.metadata.fruit_id will be saved into other_metadata\ncolumn in JSON format.test_docs = [    Document(        page_content=""Granny Smith 150 0.99"",        metadata={""fruit_id"": 1, ""fruit_name"": ""Apple"", ""organic"": 1},    ),]saver = MySQLDocumentSaver(    engine=engine,    table_name=TABLE_NAME,    content_column=""description"",    metadata_json_column=""other_metadata"",)saver.add_documents(test_docs)with engine.connect() as conn:    result = conn.execute(sqlalchemy.text(f""select * from `{TABLE_NAME}`;""))    print(result.keys())    print(result.fetchall())Delete documents with customized page content & metadata‚ÄãWe can also delete documents from table with customized metadata columns\nvia MySQLDocumentSaver.delete(<documents>). The deletion criteria is:A row should be deleted if there exists a document in the list, such\nthatdocument.page_content equals row[page_content]For every metadata field k in document.metadatadocument.metadata[k] equals row[k] or document.metadata[k]\nequals row[langchain_metadata][k]There no extra metadata field presents in row but not in\ndocument.metadata.loader = MySQLLoader(engine=engine, table_name=TABLE_NAME)docs = loader.load()print(""Documents before delete:"", docs)saver.delete(docs)print(""Documents after delete:"", loader.load())",en,
https://python.langchain.com/docs/integrations/document_loaders/web_base,WebBaseLoader | ü¶úÔ∏èüîó Langchain,This covers how to use WebBaseLoader to load all text from HTML,"WebBaseLoaderThis covers how to use WebBaseLoader to load all text from HTML\nwebpages into a document format that we can use downstream. For more\ncustom logic for loading webpages look at some child class examples such\nas IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoaderfrom langchain_community.document_loaders import WebBaseLoaderloader = WebBaseLoader(""https://www.espn.com/"")To bypass SSL verification errors during fetching, you can set the\n‚Äúverify‚Äù option:loader.requests_kwargs = {‚Äòverify‚Äô:False}data = loader.load()data[Document(page_content=""\n\n\n\n\n\n\n\n\nESPN - Serving Sports Fans. Anytime. Anywhere.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        Skip to main content\n    \n\n        Skip to navigation\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<\n\n>\n\n\n\n\n\n\n\n\n\nMenuESPN\n\n\nSearch\n\n\n\nscores\n\n\n\nNFLNBANCAAMNCAAWNHLSoccer‚Ä¶MLBNCAAFGolfTennisSports BettingBoxingCFLNCAACricketF1HorseLLWSMMANASCARNBA G LeagueOlympic SportsRacingRN BBRN FBRugbyWNBAWorld Baseball ClassicWWEX GamesXFLMore ESPNFantasyListenWatchESPN+\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\nSUBSCRIBE NOW\n\n\n\n\n\nNHL: Select Games\n\n\n\n\n\n\n\nXFL\n\n\n\n\n\n\n\nMLB: Select Games\n\n\n\n\n\n\n\nNCAA Baseball\n\n\n\n\n\n\n\nNCAA Softball\n\n\n\n\n\n\n\nCricket: Select Matches\n\n\n\n\n\n\n\nMel Kiper's NFL Mock Draft 3.0\n\n\nQuick Links\n\n\n\n\nMen's Tournament Challenge\n\n\n\n\n\n\n\nWomen's Tournament Challenge\n\n\n\n\n\n\n\nNFL Draft Order\n\n\n\n\n\n\n\nHow To Watch NHL Games\n\n\n\n\n\n\n\nFantasy Baseball: Sign Up\n\n\n\n\n\n\n\nHow To Watch PGA TOUR\n\n\n\n\n\n\nFavorites\n\n\n\n\n\n\n      Manage Favorites\n      \n\n\n\nCustomize ESPNSign UpLog InESPN Sites\n\n\n\n\nESPN Deportes\n\n\n\n\n\n\n\nAndscape\n\n\n\n\n\n\n\nespnW\n\n\n\n\n\n\n\nESPNFC\n\n\n\n\n\n\n\nX Games\n\n\n\n\n\n\n\nSEC Network\n\n\nESPN Apps\n\n\n\n\nESPN\n\n\n\n\n\n\n\nESPN Fantasy\n\n\nFollow ESPN\n\n\n\n\nFacebook\n\n\n\n\n\n\n\nTwitter\n\n\n\n\n\n\n\nInstagram\n\n\n\n\n\n\n\nSnapchat\n\n\n\n\n\n\n\nYouTube\n\n\n\n\n\n\n\nThe ESPN Daily Podcast\n\n\nAre you ready for Opening Day? Here's your guide to MLB's offseason chaosWait, Jacob deGrom is on the Rangers now? Xander Bogaerts and Trea Turner signed where? And what about Carlos Correa? Yeah, you're going to need to read up before Opening Day.12hESPNIllustration by ESPNEverything you missed in the MLB offseason3h2:33World Series odds, win totals, props for every teamPlay fantasy baseball for free!TOP HEADLINESQB Jackson has requested trade from RavensSources: Texas hiring Terry as full-time coachJets GM: No rush on Rodgers; Lamar not optionLove to leave North Carolina, enter transfer portalBelichick to angsty Pats fans: See last 25 yearsEmbiid out, Harden due back vs. Jokic, NuggetsLynch: Purdy 'earned the right' to start for NinersMan Utd, Wrexham plan July friendly in San DiegoOn paper, Padres overtake DodgersLAMAR WANTS OUT OF BALTIMOREMarcus Spears identifies the two teams that need Lamar Jackson the most8h2:00Would Lamar sit out? Will Ravens draft a QB? Jackson trade request insightsLamar Jackson has asked Baltimore to trade him, but Ravens coach John Harbaugh hopes the QB will be back.3hJamison HensleyBallard, Colts will consider trading for QB JacksonJackson to Indy? Washington? Barnwell ranks the QB's trade fitsSNYDER'S TUMULTUOUS 24-YEAR RUNHow Washington‚Äôs NFL franchise sank on and off the field under owner Dan SnyderSnyder purchased one of the NFL's marquee franchises in 1999. Twenty-four years later, and with the team up for sale, he leaves a legacy of on-field futility and off-field scandal.13hJohn KeimESPNIOWA STAR STEPS UP AGAINJ-Will: Caitlin Clark is the biggest brand in college sports right now8h0:47'The better the opponent, the better she plays': Clark draws comparisons to TaurasiCaitlin Clark's performance on Sunday had longtime observers going back decades to find comparisons.16hKevin PeltonWOMEN'S ELITE EIGHT SCOREBOARDMONDAY'S GAMESCheck your bracket!NBA DRAFTHow top prospects fared on the road to the Final FourThe 2023 NCAA tournament is down to four teams, and ESPN's Jonathan Givony recaps the players who saw their NBA draft stock change.11hJonathan GivonyAndy Lyons/Getty ImagesTALKING BASKETBALLWhy AD needs to be more assertive with LeBron on the court10h1:33Why Perk won't blame Kyrie for Mavs' woes8h1:48WHERE EVERY TEAM STANDSNew NFL Power Rankings: Post-free-agency 1-32 poll, plus underrated offseason movesThe free agent frenzy has come and gone. Which teams have improved their 2023 outlook, and which teams have taken a hit?12hNFL Nation reportersIllustration by ESPNTHE BUCK STOPS WITH BELICHICKBruschi: Fair to criticize Bill Belichick for Patriots' struggles10h1:27 Top HeadlinesQB Jackson has requested trade from RavensSources: Texas hiring Terry as full-time coachJets GM: No rush on Rodgers; Lamar not optionLove to leave North Carolina, enter transfer portalBelichick to angsty Pats fans: See last 25 yearsEmbiid out, Harden due back vs. Jokic, NuggetsLynch: Purdy 'earned the right' to start for NinersMan Utd, Wrexham plan July friendly in San DiegoOn paper, Padres overtake DodgersFavorites FantasyManage FavoritesFantasy HomeCustomize ESPNSign UpLog InMarch Madness LiveESPNMarch Madness LiveWatch every men's NCAA tournament game live! ICYMI1:42Austin Peay's coach, pitcher and catcher all ejected after retaliation pitchAustin Peay's pitcher, catcher and coach were all ejected after a pitch was thrown at Liberty's Nathan Keeter, who earlier in the game hit a home run and celebrated while running down the third-base line. Men's Tournament ChallengeIllustration by ESPNMen's Tournament ChallengeCheck your bracket(s) in the 2023 Men's Tournament Challenge, which you can follow throughout the Big Dance. Women's Tournament ChallengeIllustration by ESPNWomen's Tournament ChallengeCheck your bracket(s) in the 2023 Women's Tournament Challenge, which you can follow throughout the Big Dance. Best of ESPN+AP Photo/Lynne SladkyFantasy Baseball ESPN+ Cheat Sheet: Sleepers, busts, rookies and closersYou've read their names all preseason long, it'd be a shame to forget them on draft day. The ESPN+ Cheat Sheet is one way to make sure that doesn't happen.Steph Chambers/Getty ImagesPassan's 2023 MLB season preview: Bold predictions and moreOpening Day is just over a week away -- and Jeff Passan has everything you need to know covered from every possible angle.Photo by Bob Kupbens/Icon Sportswire2023 NFL free agency: Best team fits for unsigned playersWhere could Ezekiel Elliott land? Let's match remaining free agents to teams and find fits for two trade candidates.Illustration by ESPN2023 NFL mock draft: Mel Kiper's first-round pick predictionsMel Kiper Jr. makes his predictions for Round 1 of the NFL draft, including projecting a trade in the top five. Trending NowAnne-Marie Sorvin-USA TODAY SBoston Bruins record tracker: Wins, points, milestonesThe B's are on pace for NHL records in wins and points, along with some individual superlatives as well. Follow along here with our updated tracker.Mandatory Credit: William Purnell-USA TODAY Sports2023 NFL full draft order: AFC, NFC team picks for all roundsStarting with the Carolina Panthers at No. 1 overall, here's the entire 2023 NFL draft broken down round by round. How to Watch on ESPN+Gregory Fisher/Icon Sportswire2023 NCAA men's hockey: Results, bracket, how to watchThe matchups in Tampa promise to be thrillers, featuring plenty of star power, high-octane offense and stellar defense.(AP Photo/Koji Sasahara, File)How to watch the PGA Tour, Masters, PGA Championship and FedEx Cup playoffs on ESPN, ESPN+Here's everything you need to know about how to watch the PGA Tour, Masters, PGA Championship and FedEx Cup playoffs on ESPN and ESPN+.Hailie Lynch/XFLHow to watch the XFL: 2023 schedule, teams, players, news, moreEvery XFL game will be streamed on ESPN+. Find out when and where else you can watch the eight teams compete. Sign up to play the #1 Fantasy Baseball GameReactivate A LeagueCreate A LeagueJoin a Public LeaguePractice With a Mock DraftSports BettingAP Photo/Mike KropfMarch Madness betting 2023: Bracket odds, lines, tips, moreThe 2023 NCAA tournament brackets have finally been released, and we have everything you need to know to make a bet on all of the March Madness games. Sign up to play the #1 Fantasy game!Create A LeagueJoin Public LeagueReactivateMock Draft Now\n\nESPN+\n\n\n\n\nNHL: Select Games\n\n\n\n\n\n\n\nXFL\n\n\n\n\n\n\n\nMLB: Select Games\n\n\n\n\n\n\n\nNCAA Baseball\n\n\n\n\n\n\n\nNCAA Softball\n\n\n\n\n\n\n\nCricket: Select Matches\n\n\n\n\n\n\n\nMel Kiper's NFL Mock Draft 3.0\n\n\nQuick Links\n\n\n\n\nMen's Tournament Challenge\n\n\n\n\n\n\n\nWomen's Tournament Challenge\n\n\n\n\n\n\n\nNFL Draft Order\n\n\n\n\n\n\n\nHow To Watch NHL Games\n\n\n\n\n\n\n\nFantasy Baseball: Sign Up\n\n\n\n\n\n\n\nHow To Watch PGA TOUR\n\n\nESPN Sites\n\n\n\n\nESPN Deportes\n\n\n\n\n\n\n\nAndscape\n\n\n\n\n\n\n\nespnW\n\n\n\n\n\n\n\nESPNFC\n\n\n\n\n\n\n\nX Games\n\n\n\n\n\n\n\nSEC Network\n\n\nESPN Apps\n\n\n\n\nESPN\n\n\n\n\n\n\n\nESPN Fantasy\n\n\nFollow ESPN\n\n\n\n\nFacebook\n\n\n\n\n\n\n\nTwitter\n\n\n\n\n\n\n\nInstagram\n\n\n\n\n\n\n\nSnapchat\n\n\n\n\n\n\n\nYouTube\n\n\n\n\n\n\n\nThe ESPN Daily Podcast\n\n\nTerms of UsePrivacy PolicyYour US State Privacy RightsChildren's Online Privacy PolicyInterest-Based AdsAbout Nielsen MeasurementDo Not Sell or Share My Personal InformationContact UsDisney Ad Sales SiteWork for ESPNCopyright: ¬© ESPN Enterprises, Inc. All rights reserved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"", lookup_str='', metadata={'source': 'https://www.espn.com/'}, lookup_index=0)]""""""# Use this piece of code for testing new custom BeautifulSoup parsersimport requestsfrom bs4 import BeautifulSouphtml_doc = requests.get(""{INSERT_NEW_URL_HERE}"")soup = BeautifulSoup(html_doc.text, 'html.parser')# Beautiful soup logic to be exported to langchain_community.document_loaders.webpage.py# Example: transcript = soup.select_one(""td[class='scrtext']"").text# BS4 documentation can be found here: https://www.crummy.com/software/BeautifulSoup/bs4/doc/""""""Loading multiple webpages‚ÄãYou can also load multiple webpages at once by passing in a list of urls\nto the loader. This will return a list of documents in the same order as\nthe urls passed in.loader = WebBaseLoader([""https://www.espn.com/"", ""https://google.com""])docs = loader.load()docs[Document(page_content=""\n\n\n\n\n\n\n\n\nESPN - Serving Sports Fans. Anytime. Anywhere.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        Skip to main content\n    \n\n        Skip to navigation\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<\n\n>\n\n\n\n\n\n\n\n\n\nMenuESPN\n\n\nSearch\n\n\n\nscores\n\n\n\nNFLNBANCAAMNCAAWNHLSoccer‚Ä¶MLBNCAAFGolfTennisSports BettingBoxingCFLNCAACricketF1HorseLLWSMMANASCARNBA G LeagueOlympic SportsRacingRN BBRN FBRugbyWNBAWorld Baseball ClassicWWEX GamesXFLMore ESPNFantasyListenWatchESPN+\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\nSUBSCRIBE NOW\n\n\n\n\n\nNHL: Select Games\n\n\n\n\n\n\n\nXFL\n\n\n\n\n\n\n\nMLB: Select Games\n\n\n\n\n\n\n\nNCAA Baseball\n\n\n\n\n\n\n\nNCAA Softball\n\n\n\n\n\n\n\nCricket: Select Matches\n\n\n\n\n\n\n\nMel Kiper's NFL Mock Draft 3.0\n\n\nQuick Links\n\n\n\n\nMen's Tournament Challenge\n\n\n\n\n\n\n\nWomen's Tournament Challenge\n\n\n\n\n\n\n\nNFL Draft Order\n\n\n\n\n\n\n\nHow To Watch NHL Games\n\n\n\n\n\n\n\nFantasy Baseball: Sign Up\n\n\n\n\n\n\n\nHow To Watch PGA TOUR\n\n\n\n\n\n\nFavorites\n\n\n\n\n\n\n      Manage Favorites\n      \n\n\n\nCustomize ESPNSign UpLog InESPN Sites\n\n\n\n\nESPN Deportes\n\n\n\n\n\n\n\nAndscape\n\n\n\n\n\n\n\nespnW\n\n\n\n\n\n\n\nESPNFC\n\n\n\n\n\n\n\nX Games\n\n\n\n\n\n\n\nSEC Network\n\n\nESPN Apps\n\n\n\n\nESPN\n\n\n\n\n\n\n\nESPN Fantasy\n\n\nFollow ESPN\n\n\n\n\nFacebook\n\n\n\n\n\n\n\nTwitter\n\n\n\n\n\n\n\nInstagram\n\n\n\n\n\n\n\nSnapchat\n\n\n\n\n\n\n\nYouTube\n\n\n\n\n\n\n\nThe ESPN Daily Podcast\n\n\nAre you ready for Opening Day? Here's your guide to MLB's offseason chaosWait, Jacob deGrom is on the Rangers now? Xander Bogaerts and Trea Turner signed where? And what about Carlos Correa? Yeah, you're going to need to read up before Opening Day.12hESPNIllustration by ESPNEverything you missed in the MLB offseason3h2:33World Series odds, win totals, props for every teamPlay fantasy baseball for free!TOP HEADLINESQB Jackson has requested trade from RavensSources: Texas hiring Terry as full-time coachJets GM: No rush on Rodgers; Lamar not optionLove to leave North Carolina, enter transfer portalBelichick to angsty Pats fans: See last 25 yearsEmbiid out, Harden due back vs. Jokic, NuggetsLynch: Purdy 'earned the right' to start for NinersMan Utd, Wrexham plan July friendly in San DiegoOn paper, Padres overtake DodgersLAMAR WANTS OUT OF BALTIMOREMarcus Spears identifies the two teams that need Lamar Jackson the most7h2:00Would Lamar sit out? Will Ravens draft a QB? Jackson trade request insightsLamar Jackson has asked Baltimore to trade him, but Ravens coach John Harbaugh hopes the QB will be back.3hJamison HensleyBallard, Colts will consider trading for QB JacksonJackson to Indy? Washington? Barnwell ranks the QB's trade fitsSNYDER'S TUMULTUOUS 24-YEAR RUNHow Washington‚Äôs NFL franchise sank on and off the field under owner Dan SnyderSnyder purchased one of the NFL's marquee franchises in 1999. Twenty-four years later, and with the team up for sale, he leaves a legacy of on-field futility and off-field scandal.13hJohn KeimESPNIOWA STAR STEPS UP AGAINJ-Will: Caitlin Clark is the biggest brand in college sports right now8h0:47'The better the opponent, the better she plays': Clark draws comparisons to TaurasiCaitlin Clark's performance on Sunday had longtime observers going back decades to find comparisons.16hKevin PeltonWOMEN'S ELITE EIGHT SCOREBOARDMONDAY'S GAMESCheck your bracket!NBA DRAFTHow top prospects fared on the road to the Final FourThe 2023 NCAA tournament is down to four teams, and ESPN's Jonathan Givony recaps the players who saw their NBA draft stock change.11hJonathan GivonyAndy Lyons/Getty ImagesTALKING BASKETBALLWhy AD needs to be more assertive with LeBron on the court9h1:33Why Perk won't blame Kyrie for Mavs' woes8h1:48WHERE EVERY TEAM STANDSNew NFL Power Rankings: Post-free-agency 1-32 poll, plus underrated offseason movesThe free agent frenzy has come and gone. Which teams have improved their 2023 outlook, and which teams have taken a hit?12hNFL Nation reportersIllustration by ESPNTHE BUCK STOPS WITH BELICHICKBruschi: Fair to criticize Bill Belichick for Patriots' struggles10h1:27 Top HeadlinesQB Jackson has requested trade from RavensSources: Texas hiring Terry as full-time coachJets GM: No rush on Rodgers; Lamar not optionLove to leave North Carolina, enter transfer portalBelichick to angsty Pats fans: See last 25 yearsEmbiid out, Harden due back vs. Jokic, NuggetsLynch: Purdy 'earned the right' to start for NinersMan Utd, Wrexham plan July friendly in San DiegoOn paper, Padres overtake DodgersFavorites FantasyManage FavoritesFantasy HomeCustomize ESPNSign UpLog InMarch Madness LiveESPNMarch Madness LiveWatch every men's NCAA tournament game live! ICYMI1:42Austin Peay's coach, pitcher and catcher all ejected after retaliation pitchAustin Peay's pitcher, catcher and coach were all ejected after a pitch was thrown at Liberty's Nathan Keeter, who earlier in the game hit a home run and celebrated while running down the third-base line. Men's Tournament ChallengeIllustration by ESPNMen's Tournament ChallengeCheck your bracket(s) in the 2023 Men's Tournament Challenge, which you can follow throughout the Big Dance. Women's Tournament ChallengeIllustration by ESPNWomen's Tournament ChallengeCheck your bracket(s) in the 2023 Women's Tournament Challenge, which you can follow throughout the Big Dance. Best of ESPN+AP Photo/Lynne SladkyFantasy Baseball ESPN+ Cheat Sheet: Sleepers, busts, rookies and closersYou've read their names all preseason long, it'd be a shame to forget them on draft day. The ESPN+ Cheat Sheet is one way to make sure that doesn't happen.Steph Chambers/Getty ImagesPassan's 2023 MLB season preview: Bold predictions and moreOpening Day is just over a week away -- and Jeff Passan has everything you need to know covered from every possible angle.Photo by Bob Kupbens/Icon Sportswire2023 NFL free agency: Best team fits for unsigned playersWhere could Ezekiel Elliott land? Let's match remaining free agents to teams and find fits for two trade candidates.Illustration by ESPN2023 NFL mock draft: Mel Kiper's first-round pick predictionsMel Kiper Jr. makes his predictions for Round 1 of the NFL draft, including projecting a trade in the top five. Trending NowAnne-Marie Sorvin-USA TODAY SBoston Bruins record tracker: Wins, points, milestonesThe B's are on pace for NHL records in wins and points, along with some individual superlatives as well. Follow along here with our updated tracker.Mandatory Credit: William Purnell-USA TODAY Sports2023 NFL full draft order: AFC, NFC team picks for all roundsStarting with the Carolina Panthers at No. 1 overall, here's the entire 2023 NFL draft broken down round by round. How to Watch on ESPN+Gregory Fisher/Icon Sportswire2023 NCAA men's hockey: Results, bracket, how to watchThe matchups in Tampa promise to be thrillers, featuring plenty of star power, high-octane offense and stellar defense.(AP Photo/Koji Sasahara, File)How to watch the PGA Tour, Masters, PGA Championship and FedEx Cup playoffs on ESPN, ESPN+Here's everything you need to know about how to watch the PGA Tour, Masters, PGA Championship and FedEx Cup playoffs on ESPN and ESPN+.Hailie Lynch/XFLHow to watch the XFL: 2023 schedule, teams, players, news, moreEvery XFL game will be streamed on ESPN+. Find out when and where else you can watch the eight teams compete. Sign up to play the #1 Fantasy Baseball GameReactivate A LeagueCreate A LeagueJoin a Public LeaguePractice With a Mock DraftSports BettingAP Photo/Mike KropfMarch Madness betting 2023: Bracket odds, lines, tips, moreThe 2023 NCAA tournament brackets have finally been released, and we have everything you need to know to make a bet on all of the March Madness games. Sign up to play the #1 Fantasy game!Create A LeagueJoin Public LeagueReactivateMock Draft Now\n\nESPN+\n\n\n\n\nNHL: Select Games\n\n\n\n\n\n\n\nXFL\n\n\n\n\n\n\n\nMLB: Select Games\n\n\n\n\n\n\n\nNCAA Baseball\n\n\n\n\n\n\n\nNCAA Softball\n\n\n\n\n\n\n\nCricket: Select Matches\n\n\n\n\n\n\n\nMel Kiper's NFL Mock Draft 3.0\n\n\nQuick Links\n\n\n\n\nMen's Tournament Challenge\n\n\n\n\n\n\n\nWomen's Tournament Challenge\n\n\n\n\n\n\n\nNFL Draft Order\n\n\n\n\n\n\n\nHow To Watch NHL Games\n\n\n\n\n\n\n\nFantasy Baseball: Sign Up\n\n\n\n\n\n\n\nHow To Watch PGA TOUR\n\n\nESPN Sites\n\n\n\n\nESPN Deportes\n\n\n\n\n\n\n\nAndscape\n\n\n\n\n\n\n\nespnW\n\n\n\n\n\n\n\nESPNFC\n\n\n\n\n\n\n\nX Games\n\n\n\n\n\n\n\nSEC Network\n\n\nESPN Apps\n\n\n\n\nESPN\n\n\n\n\n\n\n\nESPN Fantasy\n\n\nFollow ESPN\n\n\n\n\nFacebook\n\n\n\n\n\n\n\nTwitter\n\n\n\n\n\n\n\nInstagram\n\n\n\n\n\n\n\nSnapchat\n\n\n\n\n\n\n\nYouTube\n\n\n\n\n\n\n\nThe ESPN Daily Podcast\n\n\nTerms of UsePrivacy PolicyYour US State Privacy RightsChildren's Online Privacy PolicyInterest-Based AdsAbout Nielsen MeasurementDo Not Sell or Share My Personal InformationContact UsDisney Ad Sales SiteWork for ESPNCopyright: ¬© ESPN Enterprises, Inc. All rights reserved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"", lookup_str='', metadata={'source': 'https://www.espn.com/'}, lookup_index=0), Document(page_content='GoogleSearch Images Maps Play YouTube News Gmail Drive More ¬ªWeb History | Settings | Sign in\xa0Advanced searchAdvertisingBusiness SolutionsAbout Google¬© 2023 - Privacy - Terms   ', lookup_str='', metadata={'source': 'https://google.com'}, lookup_index=0)]Load multiple urls concurrently‚ÄãYou can speed up the scraping process by scraping and parsing multiple\nurls concurrently.There are reasonable limits to concurrent requests, defaulting to 2 per\nsecond. If you aren‚Äôt concerned about being a good citizen, or you\ncontrol the server you are scraping and don‚Äôt care about load, you can\nchange the requests_per_second parameter to increase the max\nconcurrent requests. Note, while this will speed up the scraping\nprocess, but may cause the server to block you. Be careful!%pip install --upgrade --quiet  nest_asyncio# fixes a bug with asyncio and jupyterimport nest_asyncionest_asyncio.apply()Requirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)loader = WebBaseLoader([""https://www.espn.com/"", ""https://google.com""])loader.requests_per_second = 1docs = loader.aload()docs[Document(page_content=""\n\n\n\n\n\n\n\n\nESPN - Serving Sports Fans. Anytime. Anywhere.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        Skip to main content\n    \n\n        Skip to navigation\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<\n\n>\n\n\n\n\n\n\n\n\n\nMenuESPN\n\n\nSearch\n\n\n\nscores\n\n\n\nNFLNBANCAAMNCAAWNHLSoccer‚Ä¶MLBNCAAFGolfTennisSports BettingBoxingCFLNCAACricketF1HorseLLWSMMANASCARNBA G LeagueOlympic SportsRacingRN BBRN FBRugbyWNBAWorld Baseball ClassicWWEX GamesXFLMore ESPNFantasyListenWatchESPN+\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\nSUBSCRIBE NOW\n\n\n\n\n\nNHL: Select Games\n\n\n\n\n\n\n\nXFL\n\n\n\n\n\n\n\nMLB: Select Games\n\n\n\n\n\n\n\nNCAA Baseball\n\n\n\n\n\n\n\nNCAA Softball\n\n\n\n\n\n\n\nCricket: Select Matches\n\n\n\n\n\n\n\nMel Kiper's NFL Mock Draft 3.0\n\n\nQuick Links\n\n\n\n\nMen's Tournament Challenge\n\n\n\n\n\n\n\nWomen's Tournament Challenge\n\n\n\n\n\n\n\nNFL Draft Order\n\n\n\n\n\n\n\nHow To Watch NHL Games\n\n\n\n\n\n\n\nFantasy Baseball: Sign Up\n\n\n\n\n\n\n\nHow To Watch PGA TOUR\n\n\n\n\n\n\nFavorites\n\n\n\n\n\n\n      Manage Favorites\n      \n\n\n\nCustomize ESPNSign UpLog InESPN Sites\n\n\n\n\nESPN Deportes\n\n\n\n\n\n\n\nAndscape\n\n\n\n\n\n\n\nespnW\n\n\n\n\n\n\n\nESPNFC\n\n\n\n\n\n\n\nX Games\n\n\n\n\n\n\n\nSEC Network\n\n\nESPN Apps\n\n\n\n\nESPN\n\n\n\n\n\n\n\nESPN Fantasy\n\n\nFollow ESPN\n\n\n\n\nFacebook\n\n\n\n\n\n\n\nTwitter\n\n\n\n\n\n\n\nInstagram\n\n\n\n\n\n\n\nSnapchat\n\n\n\n\n\n\n\nYouTube\n\n\n\n\n\n\n\nThe ESPN Daily Podcast\n\n\nAre you ready for Opening Day? Here's your guide to MLB's offseason chaosWait, Jacob deGrom is on the Rangers now? Xander Bogaerts and Trea Turner signed where? And what about Carlos Correa? Yeah, you're going to need to read up before Opening Day.12hESPNIllustration by ESPNEverything you missed in the MLB offseason3h2:33World Series odds, win totals, props for every teamPlay fantasy baseball for free!TOP HEADLINESQB Jackson has requested trade from RavensSources: Texas hiring Terry as full-time coachJets GM: No rush on Rodgers; Lamar not optionLove to leave North Carolina, enter transfer portalBelichick to angsty Pats fans: See last 25 yearsEmbiid out, Harden due back vs. Jokic, NuggetsLynch: Purdy 'earned the right' to start for NinersMan Utd, Wrexham plan July friendly in San DiegoOn paper, Padres overtake DodgersLAMAR WANTS OUT OF BALTIMOREMarcus Spears identifies the two teams that need Lamar Jackson the most7h2:00Would Lamar sit out? Will Ravens draft a QB? Jackson trade request insightsLamar Jackson has asked Baltimore to trade him, but Ravens coach John Harbaugh hopes the QB will be back.3hJamison HensleyBallard, Colts will consider trading for QB JacksonJackson to Indy? Washington? Barnwell ranks the QB's trade fitsSNYDER'S TUMULTUOUS 24-YEAR RUNHow Washington‚Äôs NFL franchise sank on and off the field under owner Dan SnyderSnyder purchased one of the NFL's marquee franchises in 1999. Twenty-four years later, and with the team up for sale, he leaves a legacy of on-field futility and off-field scandal.13hJohn KeimESPNIOWA STAR STEPS UP AGAINJ-Will: Caitlin Clark is the biggest brand in college sports right now8h0:47'The better the opponent, the better she plays': Clark draws comparisons to TaurasiCaitlin Clark's performance on Sunday had longtime observers going back decades to find comparisons.16hKevin PeltonWOMEN'S ELITE EIGHT SCOREBOARDMONDAY'S GAMESCheck your bracket!NBA DRAFTHow top prospects fared on the road to the Final FourThe 2023 NCAA tournament is down to four teams, and ESPN's Jonathan Givony recaps the players who saw their NBA draft stock change.11hJonathan GivonyAndy Lyons/Getty ImagesTALKING BASKETBALLWhy AD needs to be more assertive with LeBron on the court9h1:33Why Perk won't blame Kyrie for Mavs' woes8h1:48WHERE EVERY TEAM STANDSNew NFL Power Rankings: Post-free-agency 1-32 poll, plus underrated offseason movesThe free agent frenzy has come and gone. Which teams have improved their 2023 outlook, and which teams have taken a hit?12hNFL Nation reportersIllustration by ESPNTHE BUCK STOPS WITH BELICHICKBruschi: Fair to criticize Bill Belichick for Patriots' struggles10h1:27 Top HeadlinesQB Jackson has requested trade from RavensSources: Texas hiring Terry as full-time coachJets GM: No rush on Rodgers; Lamar not optionLove to leave North Carolina, enter transfer portalBelichick to angsty Pats fans: See last 25 yearsEmbiid out, Harden due back vs. Jokic, NuggetsLynch: Purdy 'earned the right' to start for NinersMan Utd, Wrexham plan July friendly in San DiegoOn paper, Padres overtake DodgersFavorites FantasyManage FavoritesFantasy HomeCustomize ESPNSign UpLog InMarch Madness LiveESPNMarch Madness LiveWatch every men's NCAA tournament game live! ICYMI1:42Austin Peay's coach, pitcher and catcher all ejected after retaliation pitchAustin Peay's pitcher, catcher and coach were all ejected after a pitch was thrown at Liberty's Nathan Keeter, who earlier in the game hit a home run and celebrated while running down the third-base line. Men's Tournament ChallengeIllustration by ESPNMen's Tournament ChallengeCheck your bracket(s) in the 2023 Men's Tournament Challenge, which you can follow throughout the Big Dance. Women's Tournament ChallengeIllustration by ESPNWomen's Tournament ChallengeCheck your bracket(s) in the 2023 Women's Tournament Challenge, which you can follow throughout the Big Dance. Best of ESPN+AP Photo/Lynne SladkyFantasy Baseball ESPN+ Cheat Sheet: Sleepers, busts, rookies and closersYou've read their names all preseason long, it'd be a shame to forget them on draft day. The ESPN+ Cheat Sheet is one way to make sure that doesn't happen.Steph Chambers/Getty ImagesPassan's 2023 MLB season preview: Bold predictions and moreOpening Day is just over a week away -- and Jeff Passan has everything you need to know covered from every possible angle.Photo by Bob Kupbens/Icon Sportswire2023 NFL free agency: Best team fits for unsigned playersWhere could Ezekiel Elliott land? Let's match remaining free agents to teams and find fits for two trade candidates.Illustration by ESPN2023 NFL mock draft: Mel Kiper's first-round pick predictionsMel Kiper Jr. makes his predictions for Round 1 of the NFL draft, including projecting a trade in the top five. Trending NowAnne-Marie Sorvin-USA TODAY SBoston Bruins record tracker: Wins, points, milestonesThe B's are on pace for NHL records in wins and points, along with some individual superlatives as well. Follow along here with our updated tracker.Mandatory Credit: William Purnell-USA TODAY Sports2023 NFL full draft order: AFC, NFC team picks for all roundsStarting with the Carolina Panthers at No. 1 overall, here's the entire 2023 NFL draft broken down round by round. How to Watch on ESPN+Gregory Fisher/Icon Sportswire2023 NCAA men's hockey: Results, bracket, how to watchThe matchups in Tampa promise to be thrillers, featuring plenty of star power, high-octane offense and stellar defense.(AP Photo/Koji Sasahara, File)How to watch the PGA Tour, Masters, PGA Championship and FedEx Cup playoffs on ESPN, ESPN+Here's everything you need to know about how to watch the PGA Tour, Masters, PGA Championship and FedEx Cup playoffs on ESPN and ESPN+.Hailie Lynch/XFLHow to watch the XFL: 2023 schedule, teams, players, news, moreEvery XFL game will be streamed on ESPN+. Find out when and where else you can watch the eight teams compete. Sign up to play the #1 Fantasy Baseball GameReactivate A LeagueCreate A LeagueJoin a Public LeaguePractice With a Mock DraftSports BettingAP Photo/Mike KropfMarch Madness betting 2023: Bracket odds, lines, tips, moreThe 2023 NCAA tournament brackets have finally been released, and we have everything you need to know to make a bet on all of the March Madness games. Sign up to play the #1 Fantasy game!Create A LeagueJoin Public LeagueReactivateMock Draft Now\n\nESPN+\n\n\n\n\nNHL: Select Games\n\n\n\n\n\n\n\nXFL\n\n\n\n\n\n\n\nMLB: Select Games\n\n\n\n\n\n\n\nNCAA Baseball\n\n\n\n\n\n\n\nNCAA Softball\n\n\n\n\n\n\n\nCricket: Select Matches\n\n\n\n\n\n\n\nMel Kiper's NFL Mock Draft 3.0\n\n\nQuick Links\n\n\n\n\nMen's Tournament Challenge\n\n\n\n\n\n\n\nWomen's Tournament Challenge\n\n\n\n\n\n\n\nNFL Draft Order\n\n\n\n\n\n\n\nHow To Watch NHL Games\n\n\n\n\n\n\n\nFantasy Baseball: Sign Up\n\n\n\n\n\n\n\nHow To Watch PGA TOUR\n\n\nESPN Sites\n\n\n\n\nESPN Deportes\n\n\n\n\n\n\n\nAndscape\n\n\n\n\n\n\n\nespnW\n\n\n\n\n\n\n\nESPNFC\n\n\n\n\n\n\n\nX Games\n\n\n\n\n\n\n\nSEC Network\n\n\nESPN Apps\n\n\n\n\nESPN\n\n\n\n\n\n\n\nESPN Fantasy\n\n\nFollow ESPN\n\n\n\n\nFacebook\n\n\n\n\n\n\n\nTwitter\n\n\n\n\n\n\n\nInstagram\n\n\n\n\n\n\n\nSnapchat\n\n\n\n\n\n\n\nYouTube\n\n\n\n\n\n\n\nThe ESPN Daily Podcast\n\n\nTerms of UsePrivacy PolicyYour US State Privacy RightsChildren's Online Privacy PolicyInterest-Based AdsAbout Nielsen MeasurementDo Not Sell or Share My Personal InformationContact UsDisney Ad Sales SiteWork for ESPNCopyright: ¬© ESPN Enterprises, Inc. All rights reserved.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"", lookup_str='', metadata={'source': 'https://www.espn.com/'}, lookup_index=0), Document(page_content='GoogleSearch Images Maps Play YouTube News Gmail Drive More ¬ªWeb History | Settings | Sign in\xa0Advanced searchAdvertisingBusiness SolutionsAbout Google¬© 2023 - Privacy - Terms   ', lookup_str='', metadata={'source': 'https://google.com'}, lookup_index=0)]Loading a xml file, or using a different BeautifulSoup parser‚ÄãYou can also look at SitemapLoader for an example of how to load a\nsitemap file, which is an example of using this feature.loader = WebBaseLoader(    ""https://www.govinfo.gov/content/pkg/CFR-2018-title10-vol3/xml/CFR-2018-title10-vol3-sec431-86.xml"")loader.default_parser = ""xml""docs = loader.load()docs[Document(page_content='\n\n10\nEnergy\n3\n2018-01-01\n2018-01-01\nfalse\nUniform test method for the measurement of energy efficiency of commercial packaged boilers.\n√Ç¬ß 431.86\nSection √Ç¬ß 431.86\n\nEnergy\nDEPARTMENT OF ENERGY\nENERGY CONSERVATION\nENERGY EFFICIENCY PROGRAM FOR CERTAIN COMMERCIAL AND INDUSTRIAL EQUIPMENT\nCommercial Packaged Boilers\nTest Procedures\n\n\n\n\n¬ß\u2009431.86\nUniform test method for the measurement of energy efficiency of commercial packaged boilers.\n(a) Scope. This section provides test procedures, pursuant to the Energy Policy and Conservation Act (EPCA), as amended, which must be followed for measuring the combustion efficiency and/or thermal efficiency of a gas- or oil-fired commercial packaged boiler.\n(b) Testing and Calculations. Determine the thermal efficiency or combustion efficiency of commercial packaged boilers by conducting the appropriate test procedure(s) indicated in Table 1 of this section.\n\nTable 1‚ÄîTest Requirements for Commercial Packaged Boiler Equipment Classes\n\nEquipment category\nSubcategory\nCertified rated inputBtu/h\n\nStandards efficiency metric(¬ß\u2009431.87)\n\nTest procedure(corresponding to\nstandards efficiency\nmetric required\nby ¬ß\u2009431.87)\n\n\n\nHot Water\nGas-fired\n‚â•300,000 and ‚â§2,500,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\nHot Water\nGas-fired\n>2,500,000\nCombustion Efficiency\nAppendix A, Section 3.\n\n\nHot Water\nOil-fired\n‚â•300,000 and ‚â§2,500,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\nHot Water\nOil-fired\n>2,500,000\nCombustion Efficiency\nAppendix A, Section 3.\n\n\nSteam\nGas-fired (all*)\n‚â•300,000 and ‚â§2,500,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\nSteam\nGas-fired (all*)\n>2,500,000 and ‚â§5,000,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\n\u2003\n\n>5,000,000\nThermal Efficiency\nAppendix A, Section 2.OR\nAppendix A, Section 3 with Section 2.4.3.2.\n\n\n\nSteam\nOil-fired\n‚â•300,000 and ‚â§2,500,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\nSteam\nOil-fired\n>2,500,000 and ‚â§5,000,000\nThermal Efficiency\nAppendix A, Section 2.\n\n\n\u2003\n\n>5,000,000\nThermal Efficiency\nAppendix A, Section 2.OR\nAppendix A, Section 3. with Section 2.4.3.2.\n\n\n\n*\u2009Equipment classes for commercial packaged boilers as of July 22, 2009 (74 FR 36355) distinguish between gas-fired natural draft and all other gas-fired (except natural draft).\n\n(c) Field Tests. The field test provisions of appendix A may be used only to test a unit of commercial packaged boiler with rated input greater than 5,000,000 Btu/h.\n[81 FR 89305, Dec. 9, 2016]\n\n\nEnergy Efficiency Standards\n\n', lookup_str='', metadata={'source': 'https://www.govinfo.gov/content/pkg/CFR-2018-title10-vol3/xml/CFR-2018-title10-vol3-sec431-86.xml'}, lookup_index=0)]Using proxies‚ÄãSometimes you might need to use proxies to get around IP blocks. You can\npass in a dictionary of proxies to the loader (and requests\nunderneath) to use them.loader = WebBaseLoader(    ""https://www.walmart.com/search?q=parrots"",    proxies={        ""http"": ""http://{username}:{password}:@proxy.service.com:6666/"",        ""https"": ""https://{username}:{password}:@proxy.service.com:6666/"",    },)docs = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/google_speech_to_text,Google Speech-to-Text Audio Transcripts | ü¶úÔ∏èüîó Langchain,The GoogleSpeechToTextLoader allows to transcribe audio files with the,"Google Speech-to-Text Audio TranscriptsThe GoogleSpeechToTextLoader allows to transcribe audio files with the\nGoogle Cloud Speech-to-Text\nAPI and loads the transcribed\ntext into documents.To use it, you should have the google-cloud-speech python package\ninstalled, and a Google Cloud project with the Speech-to-Text API\nenabled.Bringing the power of large models to Google Cloud‚Äôs Speech\nAPIInstallation & setup‚ÄãFirst, you need to install the google-cloud-speech python package.You can find more info about it on the Speech-to-Text client\nlibraries\npage.Follow the quickstart\nguide\nin the Google Cloud documentation to create a project and enable the\nAPI.%pip install --upgrade --quiet  google-cloud-speechExample‚ÄãThe GoogleSpeechToTextLoader must include the project_id and\nfile_path arguments. Audio files can be specified as a Google Cloud\nStorage URI (gs://...) or a local file path.Only synchronous requests are supported by the loader, which has a\nlimit of 60 seconds or\n10MB\nper audio file.from langchain_community.document_loaders import GoogleSpeechToTextLoaderproject_id = ""<PROJECT_ID>""file_path = ""gs://cloud-samples-data/speech/audio.flac""# or a local file path: file_path = ""./audio.wav""loader = GoogleSpeechToTextLoader(project_id=project_id, file_path=file_path)docs = loader.load()Note: Calling loader.load() blocks until the transcription is\nfinished.The transcribed text is available in the page_content:docs[0].page_content""How old is the Brooklyn Bridge?""The metadata contains the full JSON response with more meta\ninformation:docs[0].metadata{  'language_code': 'en-US',  'result_end_offset': datetime.timedelta(seconds=1)}Recognition Config‚ÄãYou can specify the config argument to use different speech\nrecognition models and enable specific features.Refer to the Speech-to-Text recognizers\ndocumentation\nand the\nRecognizeRequest\nAPI reference for information on how to set a custom configuation.If you don‚Äôt specify a config, the following options will be selected\nautomatically:Model: Chirp Universal Speech\nModelLanguage: en-USAudio Encoding: Automatically DetectedAutomatic Punctuation: Enabledfrom google.cloud.speech_v2 import (    AutoDetectDecodingConfig,    RecognitionConfig,    RecognitionFeatures,)from langchain_community.document_loaders import GoogleSpeechToTextLoaderproject_id = ""<PROJECT_ID>""location = ""global""recognizer_id = ""<RECOGNIZER_ID>""file_path = ""./audio.wav""config = RecognitionConfig(    auto_decoding_config=AutoDetectDecodingConfig(),    language_codes=[""en-US""],    model=""long"",    features=RecognitionFeatures(        enable_automatic_punctuation=False,        profanity_filter=True,        enable_spoken_punctuation=True,        enable_spoken_emojis=True,    ),)loader = GoogleSpeechToTextLoader(    project_id=project_id,    location=location,    recognizer_id=recognizer_id,    file_path=file_path,    config=config,)",en,
https://python.langchain.com/docs/integrations/document_loaders/tidb,TiDB | ü¶úÔ∏èüîó Langchain,"TiDB Cloud, is a comprehensive","TiDBTiDB Cloud, is a comprehensive\nDatabase-as-a-Service (DBaaS) solution, that provides dedicated and\nserverless options. TiDB Serverless is now integrating a built-in\nvector search into the MySQL landscape. With this enhancement, you can\nseamlessly develop AI applications using TiDB Serverless without the\nneed for a new database or additional technical stacks. Be among the\nfirst to experience it by joining the waitlist for the private beta at\nhttps://tidb.cloud/ai.This notebook introduces how to use TiDBLoader to load data from TiDB\nin langchain.Prerequisites‚ÄãBefore using the TiDBLoader, we will install the following\ndependencies:%pip install --upgrade --quiet langchainThen, we will configure the connection to a TiDB. In this notebook, we\nwill follow the standard connection method provided by TiDB Cloud to\nestablish a secure and efficient database connection.import getpass# copy from tidb cloud consoleÔºåreplace it with your owntidb_connection_string_template = ""mysql+pymysql://<USER>:<PASSWORD>@<HOST>:4000/<DB>?ssl_ca=/etc/ssl/cert.pem&ssl_verify_cert=true&ssl_verify_identity=true""tidb_password = getpass.getpass(""Input your TiDB password:"")tidb_connection_string = tidb_connection_string_template.replace(    ""<PASSWORD>"", tidb_password)Load Data from TiDB‚ÄãHere‚Äôs a breakdown of some key arguments you can use to customize the\nbehavior of the TiDBLoader:query (str): This is the SQL query to be executed against the TiDB\ndatabase. The query should select the data you want to load into\nyour Document objects. For instance, you might use a query like\n""SELECT * FROM my_table"" to fetch all data from my_table.page_content_columns (Optional[List[str]]): Specifies the list\nof column names whose values should be included in the\npage_content of each Document object. If set to None (the\ndefault), all columns returned by the query are included in\npage_content. This allows you to tailor the content of each\ndocument based on specific columns of your data.metadata_columns (Optional[List[str]]): Specifies the list of\ncolumn names whose values should be included in the metadata of\neach Document object. By default, this list is empty, meaning no\nmetadata will be included unless explicitly specified. This is\nuseful for including additional information about each document that\ndoesn‚Äôt form part of the main content but is still valuable for\nprocessing or analysis.from sqlalchemy import Column, Integer, MetaData, String, Table, create_engine# Connect to the databaseengine = create_engine(tidb_connection_string)metadata = MetaData()table_name = ""test_tidb_loader""# Create a tabletest_table = Table(    table_name,    metadata,    Column(""id"", Integer, primary_key=True),    Column(""name"", String(255)),    Column(""description"", String(255)),)metadata.create_all(engine)with engine.connect() as connection:    transaction = connection.begin()    try:        connection.execute(            test_table.insert(),            [                {""name"": ""Item 1"", ""description"": ""Description of Item 1""},                {""name"": ""Item 2"", ""description"": ""Description of Item 2""},                {""name"": ""Item 3"", ""description"": ""Description of Item 3""},            ],        )        transaction.commit()    except:        transaction.rollback()        raisefrom langchain_community.document_loaders import TiDBLoader# Setup TiDBLoader to retrieve dataloader = TiDBLoader(    connection_string=tidb_connection_string,    query=f""SELECT * FROM {table_name};"",    page_content_columns=[""name"", ""description""],    metadata_columns=[""id""],)# Load datadocuments = loader.load()# Display the loaded documentsfor doc in documents:    print(""-"" * 30)    print(f""content: {doc.page_content}\nmetada: {doc.metadata}"")------------------------------content: name: Item 1description: Description of Item 1metada: {'id': 1}------------------------------content: name: Item 2description: Description of Item 2metada: {'id': 2}------------------------------content: name: Item 3description: Description of Item 3metada: {'id': 3}test_table.drop(bind=engine)",en,
https://python.langchain.com/docs/integrations/document_loaders/toml,TOML | ü¶úÔ∏èüîó Langchain,TOML is a file format for,"TOMLTOML is a file format for\nconfiguration files. It is intended to be easy to read and write, and\nis designed to map unambiguously to a dictionary. Its specification is\nopen-source. TOML is implemented in many programming languages. The\nname TOML is an acronym for ‚ÄúTom‚Äôs Obvious, Minimal Language‚Äù\nreferring to its creator, Tom Preston-Werner.If you need to load Toml files, use the TomlLoader.from langchain_community.document_loaders import TomlLoaderloader = TomlLoader(""example_data/fake_rule.toml"")rule = loader.load()rule[Document(page_content='{""internal"": {""creation_date"": ""2023-05-01"", ""updated_date"": ""2022-05-01"", ""release"": [""release_type""], ""min_endpoint_version"": ""some_semantic_version"", ""os_list"": [""operating_system_list""]}, ""rule"": {""uuid"": ""some_uuid"", ""name"": ""Fake Rule Name"", ""description"": ""Fake description of rule"", ""query"": ""process where process.name : \\""somequery\\""\\n"", ""threat"": [{""framework"": ""MITRE ATT&CK"", ""tactic"": {""name"": ""Execution"", ""id"": ""TA0002"", ""reference"": ""https://attack.mitre.org/tactics/TA0002/""}}]}}', metadata={'source': 'example_data/fake_rule.toml'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/cassandra,Cassandra | ü¶úÔ∏èüîó Langchain,"Cassandra is a NoSQL, row-oriented,","CassandraCassandra is a NoSQL, row-oriented,\nhighly scalable and highly available database.Starting with version 5.0,\nthe database ships with vector search\ncapabilities.Overview‚ÄãThe Cassandra Document Loader returns a list of Langchain Documents from\na Cassandra database.You must either provide a CQL query or a table name to retrieve the\ndocuments. The Loader takes the following parameters:table: (Optional) The table to load the data from.session: (Optional) The cassandra driver session. If not provided,\nthe cassio resolved session will be used.keyspace: (Optional) The keyspace of the table. If not provided, the\ncassio resolved keyspace will be used.query: (Optional) The query used to load the data.page_content_mapper: (Optional) a function to convert a row to\nstring page content. The default converts the row to JSON.metadata_mapper: (Optional) a function to convert a row to metadata\ndict.query_parameters: (Optional) The query parameters used when calling\nsession.execute .query_timeout: (Optional) The query timeout used when calling\nsession.execute .query_custom_payload: (Optional) The query custom_payload used when\ncalling session.execute.query_execution_profile: (Optional) The query execution_profile used\nwhen calling session.execute.query_host: (Optional) The query host used when calling\nsession.execute.query_execute_as: (Optional) The query execute_as used when calling\nsession.execute.Load documents with the Document Loader‚Äãfrom langchain_community.document_loaders import CassandraLoaderInit from a cassandra driver Session‚ÄãYou need to create a cassandra.cluster.Session object, as described in\nthe Cassandra driver\ndocumentation.\nThe details vary (e.g.¬†with network settings and authentication), but\nthis might be something like:from cassandra.cluster import Clustercluster = Cluster()session = cluster.connect()You need to provide the name of an existing keyspace of the Cassandra\ninstance:CASSANDRA_KEYSPACE = input(""CASSANDRA_KEYSPACE = "")Creating the document loader:loader = CassandraLoader(    table=""movie_reviews"",    session=session,    keyspace=CASSANDRA_KEYSPACE,)docs = loader.load()docs[0]Document(page_content='Row(_id=\'659bdffa16cbc4586b11a423\', title=\'Dangerous Men\', reviewtext=\'""Dangerous Men,""  the picture\\\'s production notes inform, took 26 years to reach the big screen. After having seen it, I wonder: What was the rush?\')', metadata={'table': 'movie_reviews', 'keyspace': 'default_keyspace'})Init from cassio‚ÄãIt‚Äôs also possible to use cassio to configure the session and keyspace.import cassiocassio.init(contact_points=""127.0.0.1"", keyspace=CASSANDRA_KEYSPACE)loader = CassandraLoader(    table=""movie_reviews"",)docs = loader.load()Attribution statement‚ÄãApache Cassandra, Cassandra and Apache are either registered\ntrademarks or trademarks of the Apache Software\nFoundation in the United States and/or other\ncountries.",en,
https://python.langchain.com/docs/integrations/document_loaders/huawei_obs_directory,Huawei OBS Directory | ü¶úÔ∏èüîó Langchain,The following code demonstrates how to load objects from the Huawei OBS,"Huawei OBS DirectoryThe following code demonstrates how to load objects from the Huawei OBS\n(Object Storage Service) as documents.# Install the required package# pip install esdk-obs-pythonfrom langchain_community.document_loaders import OBSDirectoryLoaderendpoint = ""your-endpoint""# Configure your access credentials\nconfig = {""ak"": ""your-access-key"", ""sk"": ""your-secret-key""}loader = OBSDirectoryLoader(""your-bucket-name"", endpoint=endpoint, config=config)loader.load()Specify a Prefix for Loading‚ÄãIf you want to load objects with a specific prefix from the bucket, you\ncan use the following code:loader = OBSDirectoryLoader(    ""your-bucket-name"", endpoint=endpoint, config=config, prefix=""test_prefix"")loader.load()Get Authentication Information from ECS‚ÄãIf your langchain is deployed on Huawei Cloud ECS and Agency is set\nup,\nthe loader can directly get the security token from ECS without needing\naccess key and secret key.config = {""get_token_from_ecs"": True}loader = OBSDirectoryLoader(""your-bucket-name"", endpoint=endpoint, config=config)loader.load()Use a Public Bucket‚ÄãIf your bucket‚Äôs bucket policy allows anonymous access (anonymous users\nhave listBucket and GetObject permissions), you can directly load\nthe objects without configuring the config parameter.loader = OBSDirectoryLoader(""your-bucket-name"", endpoint=endpoint)loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/google_bigtable,Google Bigtable | ü¶úÔ∏èüîó Langchain,Bigtable is a key-value and,"Google BigtableBigtable is a key-value and\nwide-column store, ideal for fast access to structured,\nsemi-structured, or unstructured data. Extend your database\napplication to build AI-powered experiences leveraging Bigtable‚Äôs\nLangchain integrations.This notebook goes over how to use\nBigtable to save, load and delete\nlangchain documents\nwith BigtableLoader and BigtableSaver.Learn more about the package on\nGitHub.Open In ColabBefore You Begin‚ÄãTo run this notebook, you will need to do the following:Create a Google Cloud\nProjectEnable the Bigtable\nAPICreate a Bigtable\ninstanceCreate a Bigtable\ntableCreate Bigtable access\ncredentialsAfter confirmed access to database in the runtime environment of this\nnotebook, filling the following values and run the cell before running\nexample scripts.# @markdown Please specify an instance and a table for demo purpose.INSTANCE_ID = ""my_instance""  # @param {type:""string""}TABLE_ID = ""my_table""  # @param {type:""string""}ü¶úüîó Library Installation‚ÄãThe integration lives in its own langchain-google-bigtable package, so\nwe need to install it.%pip install -upgrade --quiet langchain-google-bigtableColab only: Uncomment the following cell to restart the kernel or\nuse the button to restart the kernel. For Vertex AI Workbench you can\nrestart the terminal using the button on top.# # Automatically restart kernel after installs so that your environment can access the new packages# import IPython# app = IPython.Application.instance()# app.kernel.do_shutdown(True)‚òÅ Set Your Google Cloud Project‚ÄãSet your Google Cloud project so that you can leverage Google Cloud\nresources within this notebook.If you don‚Äôt know your project ID, try the following:Run gcloud config list.Run gcloud projects list.See the support page: Locate the project\nID.# @markdown Please fill in the value below with your Google Cloud project ID and then run the cell.PROJECT_ID = ""my-project-id""  # @param {type:""string""}# Set the project id!gcloud config set project {PROJECT_ID}üîê Authentication‚ÄãAuthenticate to Google Cloud as the IAM user logged into this notebook\nin order to access your Google Cloud Project.If you are using Colab to run this notebook, use the cell below and\ncontinue.If you are using Vertex AI Workbench, check out the setup\ninstructions\nhere.from google.colab import authauth.authenticate_user()Basic Usage‚ÄãUsing the saver‚ÄãSave langchain documents with\nBigtableSaver.add_documents(<documents>). To initialize\nBigtableSaver class you need to provide 2 things:instance_id - An instance of Bigtable.table_id - The name of the table within the Bigtable to store\nlangchain documents.from langchain_core.documents import Documentfrom langchain_google_bigtable import BigtableSavertest_docs = [    Document(        page_content=""Apple Granny Smith 150 0.99 1"",        metadata={""fruit_id"": 1},    ),    Document(        page_content=""Banana Cavendish 200 0.59 0"",        metadata={""fruit_id"": 2},    ),    Document(        page_content=""Orange Navel 80 1.29 1"",        metadata={""fruit_id"": 3},    ),]saver = BigtableSaver(    instance_id=INSTANCE_ID,    table_id=TABLE_ID,)saver.add_documents(test_docs)Querying for Documents from Bigtable‚ÄãFor more details on connecting to a Bigtable table, please check the\nPython SDK\ndocumentation.Load documents from table‚ÄãLoad langchain documents with BigtableLoader.load() or\nBigtableLoader.lazy_load(). lazy_load returns a generator that only\nqueries database during the iteration. To initialize BigtableLoader\nclass you need to provide:instance_id - An instance of Bigtable.table_id - The name of the table within the Bigtable to store\nlangchain documents.from langchain_google_bigtable import BigtableLoaderloader = BigtableLoader(    instance_id=INSTANCE_ID,    table_id=TABLE_ID,)for doc in loader.lazy_load():    print(doc)    breakDelete documents‚ÄãDelete a list of langchain documents from Bigtable table with\nBigtableSaver.delete(<documents>).from langchain_google_bigtable import BigtableSaverdocs = loader.load()print(""Documents before delete: "", docs)onedoc = test_docs[0]saver.delete([onedoc])print(""Documents after delete: "", loader.load())Advanced Usage‚ÄãLimiting the returned rows‚ÄãThere are two ways to limit the returned rows:Using a\nfilterUsing a\nrow_setimport google.cloud.bigtable.row_filters as row_filtersfilter_loader = BigtableLoader(    INSTANCE_ID, TABLE_ID, filter=row_filters.ColumnQualifierRegexFilter(b""os_build""))from google.cloud.bigtable.row_set import RowSetrow_set = RowSet()row_set.add_row_range_from_keys(    start_key=""phone#4c410523#20190501"", end_key=""phone#4c410523#201906201"")row_set_loader = BigtableLoader(    INSTANCE_ID,    TABLE_ID,    row_set=row_set,)Custom client‚ÄãThe client created by default is the default client, using only\nadmin=True option. To use a non-default, a custom\nclient\ncan be passed to the constructor.from google.cloud import bigtablecustom_client_loader = BigtableLoader(    INSTANCE_ID,    TABLE_ID,    client=bigtable.Client(...),)Custom content‚ÄãThe BigtableLoader assumes there is a column family called langchain,\nthat has a column called content, that contains values encoded in\nUTF-8. These defaults can be changed like so:from langchain_google_bigtable import Encodingcustom_content_loader = BigtableLoader(    INSTANCE_ID,    TABLE_ID,    content_encoding=Encoding.ASCII,    content_column_family=""my_content_family"",    content_column_name=""my_content_column_name"",)Metadata mapping‚ÄãBy default, the metadata map on the Document object will contain a\nsingle key, rowkey, with the value of the row‚Äôs rowkey value. To add\nmore items to that map, use metadata_mapping.import jsonfrom langchain_google_bigtable import MetadataMappingmetadata_mapping_loader = BigtableLoader(    INSTANCE_ID,    TABLE_ID,    metadata_mappings=[        MetadataMapping(            column_family=""my_int_family"",            column_name=""my_int_column"",            metadata_key=""key_in_metadata_map"",            encoding=Encoding.INT_BIG_ENDIAN,        ),        MetadataMapping(            column_family=""my_custom_family"",            column_name=""my_custom_column"",            metadata_key=""custom_key"",            encoding=Encoding.CUSTOM,            custom_decoding_func=lambda input: json.loads(input.decode()),            custom_encoding_func=lambda input: str.encode(json.dumps(input)),        ),    ],)Metadata as JSON‚ÄãIf there is a column in Bigtable that contains a JSON string that you\nwould like to have added to the output document metadata, it is possible\nto add the following parameters to BigtableLoader. Note, the default\nvalue for metadata_as_json_encoding is UTF-8.metadata_as_json_loader = BigtableLoader(    INSTANCE_ID,    TABLE_ID,    metadata_as_json_encoding=Encoding.ASCII,    metadata_as_json_family=""my_metadata_as_json_family"",    metadata_as_json_name=""my_metadata_as_json_column_name"",)Customize BigtableSaver‚ÄãThe BigtableSaver is also customizable similar to BigtableLoader.saver = BigtableSaver(    INSTANCE_ID,    TABLE_ID,    client=bigtable.Client(...),    content_encoding=Encoding.ASCII,    content_column_family=""my_content_family"",    content_column_name=""my_content_column_name"",    metadata_mappings=[        MetadataMapping(            column_family=""my_int_family"",            column_name=""my_int_column"",            metadata_key=""key_in_metadata_map"",            encoding=Encoding.INT_BIG_ENDIAN,        ),        MetadataMapping(            column_family=""my_custom_family"",            column_name=""my_custom_column"",            metadata_key=""custom_key"",            encoding=Encoding.CUSTOM,            custom_decoding_func=lambda input: json.loads(input.decode()),            custom_encoding_func=lambda input: str.encode(json.dumps(input)),        ),    ],    metadata_as_json_encoding=Encoding.ASCII,    metadata_as_json_family=""my_metadata_as_json_family"",    metadata_as_json_name=""my_metadata_as_json_column_name"",)",en,
https://python.langchain.com/docs/integrations/document_loaders/bilibili,BiliBili | ü¶úÔ∏èüîó Langchain,Bilibili is one of the most beloved,"BiliBiliBilibili is one of the most beloved\nlong-form video sites in China.This loader utilizes the\nbilibili-api to fetch the\ntext transcript from Bilibili.With this BiliBiliLoader, users can easily obtain the transcript of\ntheir desired video content on the platform.%pip install --upgrade --quiet  bilibili-api-pythonfrom langchain_community.document_loaders import BiliBiliLoaderloader = BiliBiliLoader([""https://www.bilibili.com/video/BV1xt411o7Xu/""])loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/geopandas,Geopandas | ü¶úÔ∏èüîó Langchain,Geopandas is an,"GeopandasGeopandas is an\nopen-source project to make working with geospatial data in python\neasier.GeoPandas extends the datatypes used by pandas to allow spatial\noperations on geometric types.Geometric operations are performed by shapely. Geopandas further depends\non fiona for file access and matplotlib for plotting.LLM applications (chat, QA) that utilize geospatial data are an\ninteresting area for exploration.%pip install --upgrade --quiet  sodapy%pip install --upgrade --quiet  pandas%pip install --upgrade --quiet  geopandasimport astimport geopandas as gpdimport pandas as pdfrom langchain_community.document_loaders import OpenCityDataLoaderCreate a GeoPandas dataframe from\nOpen City Data\nas an example input.# Load Open City Datadataset = ""tmnf-yvry""  # San Francisco crime dataloader = OpenCityDataLoader(city_id=""data.sfgov.org"", dataset_id=dataset, limit=5000)docs = loader.load()# Convert list of dictionaries to DataFramedf = pd.DataFrame([ast.literal_eval(d.page_content) for d in docs])# Extract latitude and longitudedf[""Latitude""] = df[""location""].apply(lambda loc: loc[""coordinates""][1])df[""Longitude""] = df[""location""].apply(lambda loc: loc[""coordinates""][0])# Create geopandas DFgdf = gpd.GeoDataFrame(    df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude), crs=""EPSG:4326"")# Only keep valid longitudes and latitudes for San Franciscogdf = gdf[    (gdf[""Longitude""] >= -123.173825)    & (gdf[""Longitude""] <= -122.281780)    & (gdf[""Latitude""] >= 37.623983)    & (gdf[""Latitude""] <= 37.929824)]Visualization of the sample of SF crime data.import matplotlib.pyplot as plt# Load San Francisco map datasf = gpd.read_file(""https://data.sfgov.org/resource/3psu-pn9h.geojson"")# Plot the San Francisco map and the pointsfig, ax = plt.subplots(figsize=(10, 10))sf.plot(ax=ax, color=""white"", edgecolor=""black"")gdf.plot(ax=ax, color=""red"", markersize=5)plt.show()Load GeoPandas dataframe as a Document for downstream processing\n(embedding, chat, etc).The geometry will be the default page_content columns, and all other\ncolumns are placed in metadata.But, we can specify the page_content_column.from langchain_community.document_loaders import GeoDataFrameLoaderloader = GeoDataFrameLoader(data_frame=gdf, page_content_column=""geometry"")docs = loader.load()docs[0]Document(page_content='POINT (-122.420084075249 37.7083109744362)', metadata={'pdid': '4133422003074', 'incidntnum': '041334220', 'incident_code': '03074', 'category': 'ROBBERY', 'descript': 'ROBBERY, BODILY FORCE', 'dayofweek': 'Monday', 'date': '2004-11-22T00:00:00.000', 'time': '17:50', 'pddistrict': 'INGLESIDE', 'resolution': 'NONE', 'address': 'GENEVA AV / SANTOS ST', 'x': '-122.420084075249', 'y': '37.7083109744362', 'location': {'type': 'Point', 'coordinates': [-122.420084075249, 37.7083109744362]}, ':@computed_region_26cr_cadq': '9', ':@computed_region_rxqg_mtj9': '8', ':@computed_region_bh8s_q3mv': '309', ':@computed_region_6qbp_sg9q': nan, ':@computed_region_qgnn_b9vv': nan, ':@computed_region_ajp5_b2md': nan, ':@computed_region_yftq_j783': nan, ':@computed_region_p5aj_wyqh': nan, ':@computed_region_fyvs_ahh9': nan, ':@computed_region_6pnf_4xz7': nan, ':@computed_region_jwn9_ihcz': nan, ':@computed_region_9dfj_4gjx': nan, ':@computed_region_4isq_27mq': nan, ':@computed_region_pigm_ib2e': nan, ':@computed_region_9jxd_iqea': nan, ':@computed_region_6ezc_tdp2': nan, ':@computed_region_h4ep_8xdi': nan, ':@computed_region_n4xg_c4py': nan, ':@computed_region_fcz8_est8': nan, ':@computed_region_nqbw_i6c3': nan, ':@computed_region_2dwj_jsy4': nan, 'Latitude': 37.7083109744362, 'Longitude': -122.420084075249})",en,
https://python.langchain.com/docs/integrations/document_loaders/mhtml,mhtml | ü¶úÔ∏èüîó Langchain,MHTML is a is used both for emails but also for archived webpages.,"mhtmlMHTML is a is used both for emails but also for archived webpages.\nMHTML, sometimes referred as MHT, stands for MIME HTML is a single file\nin which entire webpage is archived. When one saves a webpage as MHTML\nformat, this file extension will contain HTML code, images, audio files,\nflash animation etc.from langchain_community.document_loaders import MHTMLLoader# Create a new loader object for the MHTML fileloader = MHTMLLoader(    file_path=""../../../../../../tests/integration_tests/examples/example.mht"")# Load the document from the filedocuments = loader.load()# Print the documents to see the resultsfor doc in documents:    print(doc)page_content='LangChain\nLANG CHAIN ü¶úÔ∏èüîóOfficial Home Page\xa0\n\n\n\n\n\n\n\nIntegrations\n\n\n\nFeatures\n\n\n\n\nBlog\n\n\n\nConceptual Guide\n\n\n\n\nPython Repo\n\n\nJavaScript Repo\n\n\n\nPython Documentation \n\n\nJavaScript Documentation\n\n\n\n\nPython ChatLangChain \n\n\nJavaScript ChatLangChain\n\n\n\n\nDiscord \n\n\nTwitter\n\n\n\n\nIf you have any comments about our WEB page, you can \nwrite us at the address shown above.  However, due to \nthe limited number of personnel in our corporate office, we are unable to \nprovide a direct response.\n\nCopyright ¬© 2023-2023 LangChain Inc.\n\n\n' metadata={'source': '../../../../../../tests/integration_tests/examples/example.mht', 'title': 'LangChain'}",en,
https://python.langchain.com/docs/integrations/document_loaders/astradb,AstraDB | ü¶úÔ∏èüîó Langchain,DataStax Astra DB,"AstraDBDataStax Astra DB\nis a serverless vector-capable database built on Cassandra and made\nconveniently available through an easy-to-use JSON API.Overview‚ÄãThe AstraDB Document Loader returns a list of Langchain Documents from\nan AstraDB database.The Loader takes the following parameters:api_endpoint: AstraDB API endpoint. Looks like\nhttps://01234567-89ab-cdef-0123-456789abcdef-us-east1.apps.astra.datastax.comtoken: AstraDB token. Looks like AstraCS:6gBhNmsk135....collection_name : AstraDB collection namenamespace: (Optional) AstraDB namespacefilter_criteria: (Optional) Filter used in the find queryprojection: (Optional) Projection used in the find queryfind_options: (Optional) Options used in the find querynb_prefetched: (Optional) Number of documents pre-fetched by the\nloaderextraction_function: (Optional) A function to convert the AstraDB\ndocument to the LangChain page_content string. Defaults to\njson.dumpsThe following metadata is set to the LangChain Documents metadata\noutput:{    metadata : {        ""namespace"": ""..."",         ""api_endpoint"": ""..."",         ""collection"": ""...""    }}Load documents with the Document Loader‚Äãfrom langchain_community.document_loaders import AstraDBLoaderfrom getpass import getpassASTRA_DB_API_ENDPOINT = input(""ASTRA_DB_API_ENDPOINT = "")ASTRA_DB_APPLICATION_TOKEN = getpass(""ASTRA_DB_APPLICATION_TOKEN = "")loader = AstraDBLoader(    api_endpoint=ASTRA_DB_API_ENDPOINT,    token=ASTRA_DB_APPLICATION_TOKEN,    collection_name=""movie_reviews"",    projection={""title"": 1, ""reviewtext"": 1},    find_options={""limit"": 10},)docs = loader.load()docs[0]Document(page_content='{""_id"": ""659bdffa16cbc4586b11a423"", ""title"": ""Dangerous Men"", ""reviewtext"": ""\\""Dangerous Men,\\"" the picture\'s production notes inform, took 26 years to reach the big screen. After having seen it, I wonder: What was the rush?""}', metadata={'namespace': 'default_keyspace', 'api_endpoint': 'https://01234567-89ab-cdef-0123-456789abcdef-us-east1.apps.astra.datastax.com', 'collection': 'movie_reviews'})",en,
https://python.langchain.com/docs/integrations/document_loaders/recursive_url,Recursive URL | ü¶úÔ∏èüîó Langchain,We may want to process load all URLs under a root directory.,"Recursive URLWe may want to process load all URLs under a root directory.For example, let‚Äôs look at the Python 3.9\nDocument.This has many interesting child pages that we may want to read in bulk.Of course, the WebBaseLoader can load a list of pages.But, the challenge is traversing the tree of child pages and actually\nassembling that list!We do this using the RecursiveUrlLoader.This also gives us the flexibility to exclude some children, customize\nthe extractor, and more.Parametersurl: str, the target url to crawl.exclude_dirs: Optional[str], webpage directories to exclude.use_async: Optional[bool], wether to use async requests, using\nasync requests is usually faster in large tasks. However, async will\ndisable the lazy loading feature(the function still works, but it is\nnot lazy). By default, it is set to False.extractor: Optional[Callable[[str], str]], a function to\nextract the text of the document from the webpage, by default it\nreturns the page as it is. It is recommended to use tools like\ngoose3 and beautifulsoup to extract the text. By default, it just\nreturns the page as it is.max_depth: Optional[int] = None, the maximum depth to crawl. By\ndefault, it is set to 2. If you need to crawl the whole website, set\nit to a number that is large enough would simply do the job.timeout: Optional[int] = None, the timeout for each request, in\nthe unit of seconds. By default, it is set to 10.prevent_outside: Optional[bool] = None, whether to prevent\ncrawling outside the root url. By default, it is set to True.from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoaderLet‚Äôs try a simple example.from bs4 import BeautifulSoup as Soupurl = ""https://docs.python.org/3.9/""loader = RecursiveUrlLoader(    url=url, max_depth=2, extractor=lambda x: Soup(x, ""html.parser"").text)docs = loader.load()docs[0].page_content[:50]'\n\n\n\n\nPython Frequently Asked Questions ‚Äî Python 3.'docs[-1].metadata{'source': 'https://docs.python.org/3.9/library/index.html', 'title': 'The Python Standard Library ‚Äî Python 3.9.17 documentation', 'language': None}However, since it‚Äôs hard to perform a perfect filter, you may still see\nsome irrelevant results in the results. You can perform a filter on the\nreturned documents by yourself, if it‚Äôs needed. Most of the time, the\nreturned results are good enough.Testing on LangChain docs.url = ""https://js.langchain.com/docs/modules/memory/integrations/""loader = RecursiveUrlLoader(url=url)docs = loader.load()len(docs)8",en,
https://python.langchain.com/docs/integrations/document_loaders/azure_blob_storage_file,Azure Blob Storage File | ü¶úÔ∏èüîó Langchain,[Azure,"Azure Blob Storage FileAzure\nFiles\noffers fully managed file shares in the cloud that are accessible via\nthe industry standard Server Message Block (SMB) protocol, Network\nFile System (NFS) protocol, and Azure Files REST API.This covers how to load document objects from a Azure Files.%pip install --upgrade --quiet  azure-storage-blobfrom langchain_community.document_loaders import AzureBlobStorageFileLoaderloader = AzureBlobStorageFileLoader(    conn_str=""<connection string>"",    container=""<container name>"",    blob_name=""<blob name>"",)loader.load()[Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmpxvave6wl/fake.docx'}, lookup_index=0)]",en,
https://python.langchain.com/docs/integrations/document_loaders/polars_dataframe,Polars DataFrame | ü¶úÔ∏èüîó Langchain,This notebook goes over how to load data from a,"Polars DataFrameThis notebook goes over how to load data from a\npolars DataFrame.%pip install --upgrade --quiet  polarsimport polars as pldf = pl.read_csv(""example_data/mlb_teams_2012.csv"")df.head()\nshape: (5, 3)Team""Payroll (millions)""""Wins""strf64i64""Nationals""81.3498""Reds""82.297""Yankees""197.9695""Giants""117.6294""Braves""83.3194from langchain_community.document_loaders import PolarsDataFrameLoaderloader = PolarsDataFrameLoader(df, page_content_column=""Team"")loader.load()[Document(page_content='Nationals', metadata={' ""Payroll (millions)""': 81.34, ' ""Wins""': 98}), Document(page_content='Reds', metadata={' ""Payroll (millions)""': 82.2, ' ""Wins""': 97}), Document(page_content='Yankees', metadata={' ""Payroll (millions)""': 197.96, ' ""Wins""': 95}), Document(page_content='Giants', metadata={' ""Payroll (millions)""': 117.62, ' ""Wins""': 94}), Document(page_content='Braves', metadata={' ""Payroll (millions)""': 83.31, ' ""Wins""': 94}), Document(page_content='Athletics', metadata={' ""Payroll (millions)""': 55.37, ' ""Wins""': 94}), Document(page_content='Rangers', metadata={' ""Payroll (millions)""': 120.51, ' ""Wins""': 93}), Document(page_content='Orioles', metadata={' ""Payroll (millions)""': 81.43, ' ""Wins""': 93}), Document(page_content='Rays', metadata={' ""Payroll (millions)""': 64.17, ' ""Wins""': 90}), Document(page_content='Angels', metadata={' ""Payroll (millions)""': 154.49, ' ""Wins""': 89}), Document(page_content='Tigers', metadata={' ""Payroll (millions)""': 132.3, ' ""Wins""': 88}), Document(page_content='Cardinals', metadata={' ""Payroll (millions)""': 110.3, ' ""Wins""': 88}), Document(page_content='Dodgers', metadata={' ""Payroll (millions)""': 95.14, ' ""Wins""': 86}), Document(page_content='White Sox', metadata={' ""Payroll (millions)""': 96.92, ' ""Wins""': 85}), Document(page_content='Brewers', metadata={' ""Payroll (millions)""': 97.65, ' ""Wins""': 83}), Document(page_content='Phillies', metadata={' ""Payroll (millions)""': 174.54, ' ""Wins""': 81}), Document(page_content='Diamondbacks', metadata={' ""Payroll (millions)""': 74.28, ' ""Wins""': 81}), Document(page_content='Pirates', metadata={' ""Payroll (millions)""': 63.43, ' ""Wins""': 79}), Document(page_content='Padres', metadata={' ""Payroll (millions)""': 55.24, ' ""Wins""': 76}), Document(page_content='Mariners', metadata={' ""Payroll (millions)""': 81.97, ' ""Wins""': 75}), Document(page_content='Mets', metadata={' ""Payroll (millions)""': 93.35, ' ""Wins""': 74}), Document(page_content='Blue Jays', metadata={' ""Payroll (millions)""': 75.48, ' ""Wins""': 73}), Document(page_content='Royals', metadata={' ""Payroll (millions)""': 60.91, ' ""Wins""': 72}), Document(page_content='Marlins', metadata={' ""Payroll (millions)""': 118.07, ' ""Wins""': 69}), Document(page_content='Red Sox', metadata={' ""Payroll (millions)""': 173.18, ' ""Wins""': 69}), Document(page_content='Indians', metadata={' ""Payroll (millions)""': 78.43, ' ""Wins""': 68}), Document(page_content='Twins', metadata={' ""Payroll (millions)""': 94.08, ' ""Wins""': 66}), Document(page_content='Rockies', metadata={' ""Payroll (millions)""': 78.06, ' ""Wins""': 64}), Document(page_content='Cubs', metadata={' ""Payroll (millions)""': 88.19, ' ""Wins""': 61}), Document(page_content='Astros', metadata={' ""Payroll (millions)""': 60.65, ' ""Wins""': 55})]# Use lazy load for larger table, which won't read the full table into memoryfor i in loader.lazy_load():    print(i)page_content='Nationals' metadata={' ""Payroll (millions)""': 81.34, ' ""Wins""': 98}page_content='Reds' metadata={' ""Payroll (millions)""': 82.2, ' ""Wins""': 97}page_content='Yankees' metadata={' ""Payroll (millions)""': 197.96, ' ""Wins""': 95}page_content='Giants' metadata={' ""Payroll (millions)""': 117.62, ' ""Wins""': 94}page_content='Braves' metadata={' ""Payroll (millions)""': 83.31, ' ""Wins""': 94}page_content='Athletics' metadata={' ""Payroll (millions)""': 55.37, ' ""Wins""': 94}page_content='Rangers' metadata={' ""Payroll (millions)""': 120.51, ' ""Wins""': 93}page_content='Orioles' metadata={' ""Payroll (millions)""': 81.43, ' ""Wins""': 93}page_content='Rays' metadata={' ""Payroll (millions)""': 64.17, ' ""Wins""': 90}page_content='Angels' metadata={' ""Payroll (millions)""': 154.49, ' ""Wins""': 89}page_content='Tigers' metadata={' ""Payroll (millions)""': 132.3, ' ""Wins""': 88}page_content='Cardinals' metadata={' ""Payroll (millions)""': 110.3, ' ""Wins""': 88}page_content='Dodgers' metadata={' ""Payroll (millions)""': 95.14, ' ""Wins""': 86}page_content='White Sox' metadata={' ""Payroll (millions)""': 96.92, ' ""Wins""': 85}page_content='Brewers' metadata={' ""Payroll (millions)""': 97.65, ' ""Wins""': 83}page_content='Phillies' metadata={' ""Payroll (millions)""': 174.54, ' ""Wins""': 81}page_content='Diamondbacks' metadata={' ""Payroll (millions)""': 74.28, ' ""Wins""': 81}page_content='Pirates' metadata={' ""Payroll (millions)""': 63.43, ' ""Wins""': 79}page_content='Padres' metadata={' ""Payroll (millions)""': 55.24, ' ""Wins""': 76}page_content='Mariners' metadata={' ""Payroll (millions)""': 81.97, ' ""Wins""': 75}page_content='Mets' metadata={' ""Payroll (millions)""': 93.35, ' ""Wins""': 74}page_content='Blue Jays' metadata={' ""Payroll (millions)""': 75.48, ' ""Wins""': 73}page_content='Royals' metadata={' ""Payroll (millions)""': 60.91, ' ""Wins""': 72}page_content='Marlins' metadata={' ""Payroll (millions)""': 118.07, ' ""Wins""': 69}page_content='Red Sox' metadata={' ""Payroll (millions)""': 173.18, ' ""Wins""': 69}page_content='Indians' metadata={' ""Payroll (millions)""': 78.43, ' ""Wins""': 68}page_content='Twins' metadata={' ""Payroll (millions)""': 94.08, ' ""Wins""': 66}page_content='Rockies' metadata={' ""Payroll (millions)""': 78.06, ' ""Wins""': 64}page_content='Cubs' metadata={' ""Payroll (millions)""': 88.19, ' ""Wins""': 61}page_content='Astros' metadata={' ""Payroll (millions)""': 60.65, ' ""Wins""': 55}",en,
https://python.langchain.com/docs/integrations/document_loaders/lakefs,lakeFS | ü¶úÔ∏èüîó Langchain,lakeFS provides scalable version control,"lakeFSlakeFS provides scalable version control\nover the data lake, and uses Git-like semantics to create and access\nthose versions.This notebooks covers how to load document objects from a lakeFS path\n(whether it‚Äôs an object or a prefix).Initializing the lakeFS loader‚ÄãReplace ENDPOINT, LAKEFS_ACCESS_KEY, and LAKEFS_SECRET_KEY values\nwith your own.from langchain_community.document_loaders import LakeFSLoaderENDPOINT = """"LAKEFS_ACCESS_KEY = """"LAKEFS_SECRET_KEY = """"lakefs_loader = LakeFSLoader(    lakefs_access_key=LAKEFS_ACCESS_KEY,    lakefs_secret_key=LAKEFS_SECRET_KEY,    lakefs_endpoint=ENDPOINT,)Specifying a path‚ÄãYou can specify a prefix or a complete object path to control which\nfiles to load.Specify the repository, reference (branch, commit id, or tag), and path\nin the corresponding REPO, REF, and PATH to load the documents\nfrom:REPO = """"REF = """"PATH = """"lakefs_loader.set_repo(REPO)lakefs_loader.set_ref(REF)lakefs_loader.set_path(PATH)docs = lakefs_loader.load()docs",en,
https://python.langchain.com/docs/integrations/document_loaders/stripe,Stripe | ü¶úÔ∏èüîó Langchain,Stripe is an Irish-American financial,"StripeStripe is an Irish-American financial\nservices and software as a service (SaaS) company. It offers\npayment-processing software and application programming interfaces for\ne-commerce websites and mobile applications.This notebook covers how to load data from the Stripe REST API into a\nformat that can be ingested into LangChain, along with example usage for\nvectorization.from langchain.indexes import VectorstoreIndexCreatorfrom langchain_community.document_loaders import StripeLoaderThe Stripe API requires an access token, which can be found inside of\nthe Stripe dashboard.This document loader also requires a resource option which defines\nwhat data you want to load.Following resources are available:balance_transations\nDocumentationcharges Documentationcustomers Documentationevents Documentationrefunds Documentationdisputes Documentationstripe_loader = StripeLoader(""charges"")# Create a vectorstore retriever from the loader# see https://python.langchain.com/en/latest/modules/data_connection/getting_started.html for more detailsindex = VectorstoreIndexCreator().from_loaders([stripe_loader])stripe_doc_retriever = index.vectorstore.as_retriever()",en,
https://python.langchain.com/docs/integrations/document_loaders/wikipedia,Wikipedia | ü¶úÔ∏èüîó Langchain,Wikipedia is a multilingual free online,"WikipediaWikipedia is a multilingual free online\nencyclopedia written and maintained by a community of volunteers,\nknown as Wikipedians, through open collaboration and using a\nwiki-based editing system called MediaWiki. Wikipedia is the largest\nand most-read reference work in history.This notebook shows how to load wiki pages from wikipedia.org into the\nDocument format that we use downstream.Installation‚ÄãFirst, you need to install wikipedia python package.%pip install --upgrade --quiet  wikipediaExamples‚ÄãWikipediaLoader has these arguments: - query: free text which used\nto find documents in Wikipedia - optional lang: default=‚Äúen‚Äù. Use it\nto search in a specific language part of Wikipedia - optional\nload_max_docs: default=100. Use it to limit number of downloaded\ndocuments. It takes time to download all 100 documents, so use a small\nnumber for experiments. There is a hard limit of 300 for now. - optional\nload_all_available_meta: default=False. By default only the most\nimportant fields downloaded: Published (date when document was\npublished/last updated), title, Summary. If True, other fields also\ndownloaded.from langchain_community.document_loaders import WikipediaLoaderdocs = WikipediaLoader(query=""HUNTER X HUNTER"", load_max_docs=2).load()len(docs)docs[0].metadata  # meta-information of the Documentdocs[0].page_content[:400]  # a content of the Document",en,
https://python.langchain.com/docs/integrations/document_loaders/reddit,Reddit | ü¶úÔ∏èüîó Langchain,Reddit is an American social news,"RedditReddit is an American social news\naggregation, content rating, and discussion website.This loader fetches the text from the Posts of Subreddits or Reddit\nusers, using the praw Python package.Make a Reddit Application and\ninitialize the loader with with your Reddit API credentials.from langchain_community.document_loaders import RedditPostsLoader%pip install --upgrade --quiet  praw# load using 'subreddit' modeloader = RedditPostsLoader(    client_id=""YOUR CLIENT ID"",    client_secret=""YOUR CLIENT SECRET"",    user_agent=""extractor by u/Master_Ocelot8179"",    categories=[""new"", ""hot""],  # List of categories to load posts from    mode=""subreddit"",    search_queries=[        ""investing"",        ""wallstreetbets"",    ],  # List of subreddits to load posts from    number_posts=20,  # Default value is 10)# # or load using 'username' mode# loader = RedditPostsLoader(#     client_id=""YOUR CLIENT ID"",#     client_secret=""YOUR CLIENT SECRET"",#     user_agent=""extractor by u/Master_Ocelot8179"",#     categories=['new', 'hot'],#     mode = 'username',#     search_queries=['ga3far', 'Master_Ocelot8179'],         # List of usernames to load posts from#     number_posts=20#     )# Note: Categories can be only of following value - ""controversial"" ""hot"" ""new"" ""rising"" ""top""documents = loader.load()documents[:5][Document(page_content='Hello, I am not looking for investment advice. I will apply my own due diligence. However, I am interested if anyone knows as a UK resident how fees and exchange rate differences would impact performance?\n\nI am planning to create a pie of index funds (perhaps UK, US, europe) or find a fund with a good track record of long term growth at low rates. \n\nDoes anyone have any ideas?', metadata={'post_subreddit': 'r/investing', 'post_category': 'new', 'post_title': 'Long term retirement funds fees/exchange rate query', 'post_score': 1, 'post_id': '130pa6m', 'post_url': 'https://www.reddit.com/r/investing/comments/130pa6m/long_term_retirement_funds_feesexchange_rate_query/', 'post_author': Redditor(name='Badmanshiz')}), Document(page_content='I much prefer the Roth IRA and would rather rollover my 401k to that every year instead of keeping it in the limited 401k options. But if I rollover, will I be able to continue contributing to my 401k? Or will that close my account? I realize that there are tax implications of doing this but I still think it is the better option.', metadata={'post_subreddit': 'r/investing', 'post_category': 'new', 'post_title': 'Is it possible to rollover my 401k every year?', 'post_score': 3, 'post_id': '130ja0h', 'post_url': 'https://www.reddit.com/r/investing/comments/130ja0h/is_it_possible_to_rollover_my_401k_every_year/', 'post_author': Redditor(name='AnCap_Catholic')}), Document(page_content='Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn\'t warrant a self post?  Feel free to post here! \n\nIf your question is ""I have $10,000, what do I do?"" or other ""advice for my personal situation"" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nIf you are new to investing - please refer to Wiki - [Getting Started](https://www.reddit.com/r/investing/wiki/index/gettingstarted/)\n\nThe reading list in the wiki has a list of books ranging from light reading to advanced topics depending on your knowledge level. Link here - [Reading List](https://www.reddit.com/r/investing/wiki/readinglist)\n\nCheck the resources in the sidebar.\n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered investment adviser if you need professional support before making any financial decisions!', metadata={'post_subreddit': 'r/investing', 'post_category': 'new', 'post_title': 'Daily General Discussion and Advice Thread - April 27, 2023', 'post_score': 5, 'post_id': '130eszz', 'post_url': 'https://www.reddit.com/r/investing/comments/130eszz/daily_general_discussion_and_advice_thread_april/', 'post_author': Redditor(name='AutoModerator')}), Document(page_content=""Based on recent news about salt battery advancements and the overall issues of lithium, I was wondering what would be feasible ways to invest into non-lithium based battery technologies? CATL is of course a choice, but the selection of brokers I currently have in my disposal don't provide HK stocks at all."", metadata={'post_subreddit': 'r/investing', 'post_category': 'new', 'post_title': 'Investing in non-lithium battery technologies?', 'post_score': 2, 'post_id': '130d6qp', 'post_url': 'https://www.reddit.com/r/investing/comments/130d6qp/investing_in_nonlithium_battery_technologies/', 'post_author': Redditor(name='-manabreak')}), Document(page_content='Hello everyone,\n\nI would really like to invest in an ETF that follows spy or another big index, as I think this form of investment suits me best. \n\nThe problem is, that I live in Denmark where ETFs and funds are taxed annually on unrealised gains at quite a steep rate. This means that an ETF growing say 10% per year will only grow about 6%, which really ruins the long term effects of compounding interest.\n\nHowever stocks are only taxed on realised gains which is why they look more interesting to hold long term.\n\nI do not like the lack of diversification this brings, as I am looking to spend tonnes of time picking the right long term stocks.\n\nIt would be ideal to find a few stocks that over the long term somewhat follows the indexes. Does anyone have suggestions?\n\nI have looked at Nasdaq Inc. which quite closely follows Nasdaq 100. \n\nI really appreciate any help.', metadata={'post_subreddit': 'r/investing', 'post_category': 'new', 'post_title': 'Stocks that track an index', 'post_score': 7, 'post_id': '130auvj', 'post_url': 'https://www.reddit.com/r/investing/comments/130auvj/stocks_that_track_an_index/', 'post_author': Redditor(name='LeAlbertP')})]",en,
https://python.langchain.com/docs/integrations/document_loaders/gutenberg,Gutenberg | ü¶úÔ∏èüîó Langchain,Project Gutenberg is an online,"GutenbergProject Gutenberg is an online\nlibrary of free eBooks.This notebook covers how to load links to Gutenberg e-books into a\ndocument format that we can use downstream.from langchain_community.document_loaders import GutenbergLoaderloader = GutenbergLoader(""https://www.gutenberg.org/cache/epub/69972/pg69972.txt"")data = loader.load()data[0].page_content[:300]'The Project Gutenberg eBook of The changed brides, by Emma Dorothy\r\n\n\nEliza Nevitte Southworth\r\n\n\n\r\n\n\nThis eBook is for the use of anyone anywhere in the United States and\r\n\n\nmost other parts of the world at no cost and with almost no restrictions\r\n\n\nwhatsoever. You may copy it, give it away or re-u'data[0].metadata{'source': 'https://www.gutenberg.org/cache/epub/69972/pg69972.txt'}",en,
https://python.langchain.com/docs/integrations/document_loaders/assemblyai,AssemblyAI Audio Transcripts | ü¶úÔ∏èüîó Langchain,The AssemblyAIAudioTranscriptLoader allows to transcribe audio files,"AssemblyAI Audio TranscriptsThe AssemblyAIAudioTranscriptLoader allows to transcribe audio files\nwith the AssemblyAI API and loads the\ntranscribed text into documents.To use it, you should have the assemblyai python package installed,\nand the environment variable ASSEMBLYAI_API_KEY set with your API key.\nAlternatively, the API key can also be passed as an argument.More info about AssemblyAI:WebsiteGet a Free API keyAssemblyAI API DocsInstallation‚ÄãFirst, you need to install the assemblyai python package.You can find more info about it inside the assemblyai-python-sdk GitHub\nrepo.%pip install --upgrade --quiet  assemblyaiExample‚ÄãThe AssemblyAIAudioTranscriptLoader needs at least the file_path\nargument. Audio files can be specified as an URL or a local file path.from langchain_community.document_loaders import AssemblyAIAudioTranscriptLoaderaudio_file = ""https://storage.googleapis.com/aai-docs-samples/nbc.mp3""# or a local file path: audio_file = ""./nbc.mp3""loader = AssemblyAIAudioTranscriptLoader(file_path=audio_file)docs = loader.load()Note: Calling loader.load() blocks until the transcription is\nfinished.The transcribed text is available in the page_content:docs[0].page_content""Load time, a new president and new congressional makeup. Same old ...""The metadata contains the full JSON response with more meta\ninformation:docs[0].metadata{'language_code': <LanguageCode.en_us: 'en_us'>, 'audio_url': 'https://storage.googleapis.com/aai-docs-samples/nbc.mp3', 'punctuate': True, 'format_text': True,  ...}Transcript Formats‚ÄãYou can specify the transcript_format argument for different formats.Depending on the format, one or more documents are returned. These are\nthe different TranscriptFormat options:TEXT: One document with the transcription textSENTENCES: Multiple documents, splits the transcription by each\nsentencePARAGRAPHS: Multiple documents, splits the transcription by each\nparagraphSUBTITLES_SRT: One document with the transcript exported in SRT\nsubtitles formatSUBTITLES_VTT: One document with the transcript exported in VTT\nsubtitles formatfrom langchain_community.document_loaders.assemblyai import TranscriptFormatloader = AssemblyAIAudioTranscriptLoader(    file_path=""./your_file.mp3"",    transcript_format=TranscriptFormat.SENTENCES,)docs = loader.load()Transcription Config‚ÄãYou can also specify the config argument to use different audio\nintelligence models.Visit the AssemblyAI API\nDocumentation to get an overview of\nall available models!import assemblyai as aaiconfig = aai.TranscriptionConfig(    speaker_labels=True, auto_chapters=True, entity_detection=True)loader = AssemblyAIAudioTranscriptLoader(file_path=""./your_file.mp3"", config=config)Pass the API Key as argument‚ÄãNext to setting the API key as environment variable\nASSEMBLYAI_API_KEY, it is also possible to pass it as argument.loader = AssemblyAIAudioTranscriptLoader(    file_path=""./your_file.mp3"", api_key=""YOUR_KEY"")",en,
https://python.langchain.com/docs/integrations/document_loaders/microsoft_onenote,Microsoft OneNote | ü¶úÔ∏èüîó Langchain,This notebook covers how to load documents from OneNote.,"Microsoft OneNoteThis notebook covers how to load documents from OneNote.Prerequisites‚ÄãRegister an application with the Microsoft identity\nplatform\ninstructions.When registration finishes, the Azure portal displays the app\nregistration‚Äôs Overview pane. You see the Application (client) ID.\nAlso called the client ID, this value uniquely identifies your\napplication in the Microsoft identity platform.During the steps you will be following at item 1, you can set\nthe redirect URI as http://localhost:8000/callbackDuring the steps you will be following at item 1, generate a new\npassword (client_secret) under¬†Application Secrets¬†section.Follow the instructions at this\ndocument\nto add the following SCOPES (Notes.Read) to your application.You need to install the msal and bs4 packages using the commands\npip install msal and pip install beautifulsoup4.At the end of the steps you must have the following values:CLIENT_IDCLIENT_SECRETüßë Instructions for ingesting your documents from OneNote‚Äãüîë Authentication‚ÄãBy default, the OneNoteLoader expects that the values of CLIENT_ID\nand CLIENT_SECRET must be stored as environment variables named\nMS_GRAPH_CLIENT_ID and MS_GRAPH_CLIENT_SECRET respectively. You\ncould pass those environment variables through a .env file at the root\nof your application or using the following command in your script.os.environ['MS_GRAPH_CLIENT_ID'] = ""YOUR CLIENT ID""os.environ['MS_GRAPH_CLIENT_SECRET'] = ""YOUR CLIENT SECRET""This loader uses an authentication called on behalf of a\nuser.\nIt is a 2 step authentication with user consent. When you instantiate\nthe loader, it will call will print a url that the user must visit to\ngive consent to the app on the required permissions. The user must then\nvisit this url and give consent to the application. Then the user must\ncopy the resulting page url and paste it back on the console. The method\nwill then return True if the login attempt was successful.from langchain_community.document_loaders.onenote import OneNoteLoaderloader = OneNoteLoader(notebook_name=""NOTEBOOK NAME"", section_name=""SECTION NAME"", page_title=""PAGE TITLE"")Once the authentication has been done, the loader will store a token\n(onenote_graph_token.txt) at ~/.credentials/ folder. This token\ncould be used later to authenticate without the copy/paste steps\nexplained earlier. To use this token for authentication, you need to\nchange the auth_with_token parameter to True in the instantiation of\nthe loader.from langchain_community.document_loaders.onenote import OneNoteLoaderloader = OneNoteLoader(notebook_name=""NOTEBOOK NAME"", section_name=""SECTION NAME"", page_title=""PAGE TITLE"", auth_with_token=True)Alternatively, you can also pass the token directly to the loader. This\nis useful when you want to authenticate with a token that was generated\nby another application. For instance, you can use the Microsoft Graph\nExplorer to\ngenerate a token and then pass it to the loader.from langchain_community.document_loaders.onenote import OneNoteLoaderloader = OneNoteLoader(notebook_name=""NOTEBOOK NAME"", section_name=""SECTION NAME"", page_title=""PAGE TITLE"", access_token=""TOKEN"")üóÇÔ∏è Documents loader‚Äãüìë Loading pages from a OneNote Notebook‚ÄãOneNoteLoader can load pages from OneNote notebooks stored in\nOneDrive. You can specify any combination of notebook_name,\nsection_name, page_title to filter for pages under a specific\nnotebook, under a specific section, or with a specific title\nrespectively. For instance, you want to load all pages that are stored\nunder a section called Recipes within any of your notebooks OneDrive.from langchain_community.document_loaders.onenote import OneNoteLoaderloader = OneNoteLoader(section_name=""Recipes"", auth_with_token=True)documents = loader.load()üìë Loading pages from a list of Page IDs‚ÄãAnother possibility is to provide a list of object_ids for each page\nyou want to load. For that, you will need to query the Microsoft Graph\nAPI to find\nall the documents ID that you are interested in. This\nlink\nprovides a list of endpoints that will be helpful to retrieve the\ndocuments ID.For instance, to retrieve information about all pages that are stored in\nyour notebooks, you need make a request to:\nhttps://graph.microsoft.com/v1.0/me/onenote/pages. Once you have the\nlist of IDs that you are interested in, then you can instantiate the\nloader with the following parameters.from langchain_community.document_loaders.onenote import OneNoteLoaderloader = OneNoteLoader(object_ids=[""ID_1"", ""ID_2""], auth_with_token=True)documents = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/vsdx,Vsdx | ü¶úÔ∏èüîó Langchain,A visio file (with,"VsdxA visio file (with\nextension .vsdx) is associated with Microsoft Visio, a diagram\ncreation software. It stores information about the structure, layout,\nand graphical elements of a diagram. This format facilitates the\ncreation and sharing of visualizations in areas such as business,\nengineering, and computer science.A Visio file can contain multiple pages. Some of them may serve as the\nbackground for others, and this can occur across multiple layers. This\nloader extracts the textual content from each page and its\nassociated pages, enabling the extraction of all visible text from each\npage, similar to what an OCR algorithm would do.WARNING : Only Visio files with the .vsdx extension are\ncompatible with this loader. Files with extensions such as .vsd, ‚Ä¶ are\nnot compatible because they cannot be converted to compressed XML.from langchain_community.document_loaders import VsdxLoaderloader = VsdxLoader(file_path=""./example_data/fake.vsdx"")documents = loader.load()Display loaded documentsfor i, doc in enumerate(documents):    print(f""\n------ Page {doc.metadata['page']} ------"")    print(f""Title page : {doc.metadata['page_name']}"")    print(f""Source : {doc.metadata['source']}"")    print(""\n==> CONTENT <== "")    print(doc.page_content)------ Page 0 ------Title page : SummarySource : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a titleBest Caption of the worlThis is an arrowThis is EarthThis is a bounded arrow------ Page 1 ------Title page : GlossarySource : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a title------ Page 2 ------Title page : blanket pageSource : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a titleThis file is a vsdx fileFirst textSecond textThird text------ Page 3 ------Title page : BLABLABLASource : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a titleAnother RED arrow wowArrow with point but redGreen lineUserCaptionsRed arrow magic !Something whiteSomething RedThis a a completly useless diagramm, cool !!But this is for example !This diagramm is a base of many pages in this file. But it is editable in file \""BG WITH CONTENT\""This is a page with something...WAW I have learned something !This is a page with something...WAW I have learned something !X2------ Page 4 ------Title page : What a page !!Source : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a titleSomething whiteSomething RedThis a a completly useless diagramm, cool !!But this is for example !This diagramm is a base of many pages in this file. But it is editable in file \""BG WITH CONTENT\""Another RED arrow wowArrow with point but redGreen lineUserCaptionsRed arrow magic !------ Page 5 ------Title page : next page after previous oneSource : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a titleAnother RED arrow wowArrow with point but redGreen lineUserCaptionsRed arrow magic !Something whiteSomething RedThis a a completly useless diagramm, cool !!But this is for example !This diagramm is a base of many pages in this file. But it is editable in file \""BG WITH CONTENT\""Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-\u00a0incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit involuptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa*qui officia deserunt mollit anim id est laborum.------ Page 6 ------Title page : Connector PageSource : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a titleSomething whiteSomething RedThis a a completly useless diagramm, cool !!But this is for example !This diagramm is a base of many pages in this file. But it is editable in file \""BG WITH CONTENT\""------ Page 7 ------Title page : Useful ‚Üî Useless pageSource : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a titleSomething whiteSomething RedThis a a completly useless diagramm, cool !!But this is for example !This diagramm is a base of many pages in this file. But it is editable in file \""BG WITH CONTENT\""Title of this document : BLABLABLA------ Page 8 ------Title page : Alone pageSource : ./example_data/fake.vsdx==> CONTENT <== Black cloudUnidirectional traffic primary pathUnidirectional traffic backup pathEncapsulationUserCaptionsBidirectional trafficAlone, sadTest of another pageThis is a \""bannier\""Tests of some exotics characters :\u00a0\u00e3\u00e4\u00e5\u0101\u0103 \u00fc\u2554\u00a0 \u00a0\u00bc \u00c7 \u25d8\u25cb\u2642\u266b\u2640\u00ee\u2665This is ethernetLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.This is an empty caseLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0-\u00a0 incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa *qui officia deserunt mollit anim id est laborum.------ Page 9 ------Title page : BGSource : ./example_data/fake.vsdx==> CONTENT <== Best Caption of the worlThis is an arrowThis is EarthThis is a bounded arrowCreated byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a title------ Page 10 ------Title page : BG  + caption1Source : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a titleAnother RED arrow wowArrow with point but redGreen lineUserCaptionsRed arrow magic !Something whiteSomething RedThis a a completly useless diagramm, cool !!But this is for example !This diagramm is a base of many pages in this file. But it is editable in file \""BG WITH CONTENT\""Useful\u2194 Useless page\u00a0Tests of some exotics characters :\u00a0\u00e3\u00e4\u00e5\u0101\u0103 \u00fc\u2554\u00a0\u00a0\u00bc \u00c7 \u25d8\u25cb\u2642\u266b\u2640\u00ee\u2665------ Page 11 ------Title page : BG+Source : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a title------ Page 12 ------Title page : BG WITH CONTENTSource : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a titleLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. - Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.This is a page with a lot of text------ Page 13 ------Title page : 2nd caption with ____________________________________________________________________ contentSource : ./example_data/fake.vsdx==> CONTENT <== Created byCreated theModified byModified theVersionTitleFlorian MOREL2024-01-14FLORIAN MorelToday0.0.0.0.0.1This is a titleAnother RED arrow wowArrow with point but redGreen lineUserCaptionsRed arrow magic !Something whiteSomething RedThis a a completly useless diagramm, cool !!But this is for example !This diagramm is a base of many pages in this file. But it is editable in file \""BG WITH CONTENT\""Only connectors on this page. This is the CoNNeCtor page",en,
https://python.langchain.com/docs/integrations/document_loaders/telegram,Telegram | ü¶úÔ∏èüîó Langchain,Telegram Messenger is a globally,"TelegramTelegram Messenger is a globally\naccessible freemium, cross-platform, encrypted, cloud-based and\ncentralized instant messaging service. The application also provides\noptional end-to-end encrypted chats and video calling, VoIP, file\nsharing and several other features.This notebook covers how to load data from Telegram into a format that\ncan be ingested into LangChain.from langchain_community.document_loaders import (    TelegramChatApiLoader,    TelegramChatFileLoader,)loader = TelegramChatFileLoader(""example_data/telegram.json"")loader.load()[Document(page_content=""Henry on 2020-01-01T00:00:02: It's 2020...\n\nHenry on 2020-01-01T00:00:04: Fireworks!\n\nGrace √∞≈∏¬ß¬§ √∞≈∏\x8d‚Äô on 2020-01-01T00:00:05: You're a minute late!\n\n"", metadata={'source': 'example_data/telegram.json'})]TelegramChatApiLoader loads data directly from any specified chat from\nTelegram. In order to export the data, you will need to authenticate\nyour Telegram account.You can get the API_HASH and API_ID from\nhttps://my.telegram.org/auth?to=appschat_entity ‚Äì recommended to be the\nentity\nof a channel.loader = TelegramChatApiLoader(    chat_entity=""<CHAT_URL>"",  # recommended to use Entity here    api_hash=""<API HASH >"",    api_id=""<API_ID>"",    username="""",  # needed only for caching the session.)loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/couchbase,Couchbase | ü¶úÔ∏èüîó Langchain,Couchbase is an award-winning distributed NoSQL,"CouchbaseCouchbase is an award-winning distributed NoSQL\ncloud database that delivers unmatched versatility, performance,\nscalability, and financial value for all of your cloud, mobile, AI, and\nedge computing applications.Installation‚Äã%pip install --upgrade --quiet  couchbaseQuerying for Documents from Couchbase‚ÄãFor more details on connecting to a Couchbase cluster, please check the\nPython SDK\ndocumentation.For help with querying for documents using SQL++ (SQL for JSON), please\ncheck the\ndocumentation.from langchain_community.document_loaders.couchbase import CouchbaseLoaderconnection_string = ""couchbase://localhost""  # valid Couchbase connection stringdb_username = (    ""Administrator""  # valid database user with read access to the bucket being queried)db_password = ""Password""  # password for the database user# query is a valid SQL++ queryquery = """"""    SELECT h.* FROM `travel-sample`.inventory.hotel h         WHERE h.country = 'United States'        LIMIT 1        """"""Create the Loader‚Äãloader = CouchbaseLoader(    connection_string,    db_username,    db_password,    query,)You can fetch the documents by calling the load method of the loader.\nIt will return a list with all the documents. If you want to avoid this\nblocking call, you can call lazy_load method that returns an Iterator.docs = loader.load()print(docs)[Document(page_content='address: 8301 Hollister Ave\nalias: None\ncheckin: 12PM\ncheckout: 4PM\ncity: Santa Barbara\ncountry: United States\ndescription: Located on 78 acres of oceanfront property, this resort is an upscale experience that caters to luxury travelers. There are 354 guest rooms in 19 separate villas, each in a Spanish style. Property amenities include saline infinity pools, a private beach, clay tennis courts, a 42,000 foot spa and fitness center, and nature trails through the adjoining wetland and forest. The onsite Miro restaurant provides great views of the coast with excellent food and service. With all that said, you pay for the experience, and this resort is not for the budget traveler.  In addition to quoted rates there is a $25 per day resort fee that includes a bottle of wine in your room, two bottles of water, access to fitness center and spa, and internet access.\ndirections: None\nemail: None\nfax: None\nfree_breakfast: True\nfree_internet: False\nfree_parking: False\ngeo: {\'accuracy\': \'ROOFTOP\', \'lat\': 34.43429, \'lon\': -119.92137}\nid: 10180\nname: Bacara Resort & Spa\npets_ok: False\nphone: None\nprice: $300-$1000+\npublic_likes: [\'Arnoldo Towne\', \'Olaf Turcotte\', \'Ruben Volkman\', \'Adella Aufderhar\', \'Elwyn Franecki\']\nreviews: [{\'author\': \'Delmer Cole\', \'content\': ""Jane and Joyce make every effort to see to your personal needs and comfort. The rooms take one back in time to the original styles and designs of the 1800\'s. A real connection to local residents, the 905 is a regular tour stop and the oldest hotel in the French Quarter. My wife and I prefer to stay in the first floor rooms where there is a sitting room with TV, bedroom, bath and kitchen. The kitchen has a stove and refrigerator, sink, coffeemaker, etc. Plus there is a streetside private entrance (very good security system) and a covered balcony area with seating so you can watch passersby. Quaint, cozy, and most of all: ORIGINAL. No plastic remods. Feels like my great Grandmother\'s place. While there are more luxurious places to stay, if you want the real flavor and eclectic style of N.O. you have to stay here. It just FEELS like New Orleans. The location is one block towards the river from Bourbon Street and smack dab in the middle of everything. Royal street is one of the nicest residential streets in the Quarter and you can walk back to your room and get some peace and quiet whenever you like. The French Quarter is always busy so we bring a small fan to turn on to make some white noise so we can sleep more soundly. Works great. You might not need it at the 905 but it\'s a necessity it if you stay on or near Bourbon Street, which is very loud all the time. Parking tips: You can park right in front to unload and it\'s only a couple blocks to the secure riverfront parking area. Plus there are several public parking lots nearby. My strategy is to get there early, unload, and drive around for a while near the hotel. It\'s not too hard to find a parking place but be careful about where it is. Stay away from corner spots since streets are narrow and delivery trucks don\'t have the room to turn and they will hit your car. Take note of the signs. Tuesday and Thursday they clean the streets and you can\'t park in many areas when they do or they will tow your car. Once you find a spot don\'t move it since everything is walking distance. If you find a good spot and get a ticket it will cost $20, which is cheaper than the daily rate at most parking garages. Even if you don\'t get a ticket make sure to go online to N.O. traffic ticket site to check your license number for violations. Some local kids think it\'s funny to take your ticket and throw it away since the fine doubles every month it\'s not paid. You don\'t know you got a ticket but your fine is getting bigger. We\'ve been coming to the French Quarter for years and have stayed at many of the local hotels. The 905 Royal is our favorite."", \'date\': \'2013-12-05 09:27:07 +0300\', \'ratings\': {\'Cleanliness\': 5, \'Location\': 5, \'Overall\': 5, \'Rooms\': 5, \'Service\': 5, \'Sleep Quality\': 5, \'Value\': 5}}, {\'author\': \'Orval Lebsack\', \'content\': \'I stayed there with a friend for a girls trip around St. Patricks Day. This was my third time to NOLA, my first at Chateau Lemoyne. The location is excellent....very easy walking distance to everything, without the chaos of staying right on Bourbon Street. Even though its a Holiday Inn, it still has the historical feel and look of NOLA. The pool looked nice too, even though we never used it. The staff was friendly and helpful. Chateau Lemoyne would be hard to top, considering the price.\', \'date\': \'2013-10-26 15:01:39 +0300\', \'ratings\': {\'Cleanliness\': 5, \'Location\': 5, \'Overall\': 4, \'Rooms\': 4, \'Service\': 4, \'Sleep Quality\': 5, \'Value\': 4}}, {\'author\': \'Hildegard Larkin\', \'content\': \'This hotel is a safe bet for a value stay in French Quarter. Close enough to all sites and action but just out of the real loud & noisy streets. Check in is quick and friendly and room ( king side balcony) while dated was good size and clean. Small balcony with table & chairs is a nice option for evening drink & passing sites below. Down side is no mimi bar fridge ( they are available upon request on a first come basis apparently, so book one when you make initial reservation if necessary) Bathroom is adequate with ok shower pressure and housekeeping is quick and efficient. TIP; forget paying high price for conducted local tours, just take the red trams to end of line and back and then next day the green tram to cross town garden district and zoo and museums. cost for each ride $2.00 each way!! fantastic. Tip: If you stay during hot weather make sure you top up on ice early as later guests can ""run the machine dry"" for short time. Overall experience met expectations and would recommend for value stay.\', \'date\': \'2012-01-01 18:48:30 +0300\', \'ratings\': {\'Cleanliness\': 4, \'Location\': 4, \'Overall\': 4, \'Rooms\': 3, \'Service\': 4, \'Sleep Quality\': 3, \'Value\': 4}}, {\'author\': \'Uriah Rohan\', \'content\': \'The Chateau Le Moyne Holiday Inn is in a perfect location in the French Quarter, a block away from the craziness on Bourbon St. We got a fantastic deal on Priceline and were expecting a standard room for the price. The pleasant hotel clerk upgraded our room much to our delight, without us asking and the concierge also went above an beyond to assist us with information and suggestions for places to dine and possessed an ""can do"" attitude. Nice pool area to cool off in during the midday NOLA heat. It is definitely a three star establishment, not super luxurious but the beds were comfy and the location superb! If you can get a deal on Priceline, etc, it\\\'s a great value.\', \'date\': \'2014-08-04 15:17:49 +0300\', \'ratings\': {\'Cleanliness\': 4, \'Location\': 5, \'Overall\': 4, \'Rooms\': 3, \'Service\': 5, \'Sleep Quality\': 4, \'Value\': 4}}]\nstate: California\ntitle: Goleta\ntollfree: None\ntype: hotel\nurl: http://www.bacararesort.com/\nvacancy: True')]docs_iterator = loader.lazy_load()for doc in docs_iterator:    print(doc)    breakpage_content='address: 8301 Hollister Ave\nalias: None\ncheckin: 12PM\ncheckout: 4PM\ncity: Santa Barbara\ncountry: United States\ndescription: Located on 78 acres of oceanfront property, this resort is an upscale experience that caters to luxury travelers. There are 354 guest rooms in 19 separate villas, each in a Spanish style. Property amenities include saline infinity pools, a private beach, clay tennis courts, a 42,000 foot spa and fitness center, and nature trails through the adjoining wetland and forest. The onsite Miro restaurant provides great views of the coast with excellent food and service. With all that said, you pay for the experience, and this resort is not for the budget traveler.  In addition to quoted rates there is a $25 per day resort fee that includes a bottle of wine in your room, two bottles of water, access to fitness center and spa, and internet access.\ndirections: None\nemail: None\nfax: None\nfree_breakfast: True\nfree_internet: False\nfree_parking: False\ngeo: {\'accuracy\': \'ROOFTOP\', \'lat\': 34.43429, \'lon\': -119.92137}\nid: 10180\nname: Bacara Resort & Spa\npets_ok: False\nphone: None\nprice: $300-$1000+\npublic_likes: [\'Arnoldo Towne\', \'Olaf Turcotte\', \'Ruben Volkman\', \'Adella Aufderhar\', \'Elwyn Franecki\']\nreviews: [{\'author\': \'Delmer Cole\', \'content\': ""Jane and Joyce make every effort to see to your personal needs and comfort. The rooms take one back in time to the original styles and designs of the 1800\'s. A real connection to local residents, the 905 is a regular tour stop and the oldest hotel in the French Quarter. My wife and I prefer to stay in the first floor rooms where there is a sitting room with TV, bedroom, bath and kitchen. The kitchen has a stove and refrigerator, sink, coffeemaker, etc. Plus there is a streetside private entrance (very good security system) and a covered balcony area with seating so you can watch passersby. Quaint, cozy, and most of all: ORIGINAL. No plastic remods. Feels like my great Grandmother\'s place. While there are more luxurious places to stay, if you want the real flavor and eclectic style of N.O. you have to stay here. It just FEELS like New Orleans. The location is one block towards the river from Bourbon Street and smack dab in the middle of everything. Royal street is one of the nicest residential streets in the Quarter and you can walk back to your room and get some peace and quiet whenever you like. The French Quarter is always busy so we bring a small fan to turn on to make some white noise so we can sleep more soundly. Works great. You might not need it at the 905 but it\'s a necessity it if you stay on or near Bourbon Street, which is very loud all the time. Parking tips: You can park right in front to unload and it\'s only a couple blocks to the secure riverfront parking area. Plus there are several public parking lots nearby. My strategy is to get there early, unload, and drive around for a while near the hotel. It\'s not too hard to find a parking place but be careful about where it is. Stay away from corner spots since streets are narrow and delivery trucks don\'t have the room to turn and they will hit your car. Take note of the signs. Tuesday and Thursday they clean the streets and you can\'t park in many areas when they do or they will tow your car. Once you find a spot don\'t move it since everything is walking distance. If you find a good spot and get a ticket it will cost $20, which is cheaper than the daily rate at most parking garages. Even if you don\'t get a ticket make sure to go online to N.O. traffic ticket site to check your license number for violations. Some local kids think it\'s funny to take your ticket and throw it away since the fine doubles every month it\'s not paid. You don\'t know you got a ticket but your fine is getting bigger. We\'ve been coming to the French Quarter for years and have stayed at many of the local hotels. The 905 Royal is our favorite."", \'date\': \'2013-12-05 09:27:07 +0300\', \'ratings\': {\'Cleanliness\': 5, \'Location\': 5, \'Overall\': 5, \'Rooms\': 5, \'Service\': 5, \'Sleep Quality\': 5, \'Value\': 5}}, {\'author\': \'Orval Lebsack\', \'content\': \'I stayed there with a friend for a girls trip around St. Patricks Day. This was my third time to NOLA, my first at Chateau Lemoyne. The location is excellent....very easy walking distance to everything, without the chaos of staying right on Bourbon Street. Even though its a Holiday Inn, it still has the historical feel and look of NOLA. The pool looked nice too, even though we never used it. The staff was friendly and helpful. Chateau Lemoyne would be hard to top, considering the price.\', \'date\': \'2013-10-26 15:01:39 +0300\', \'ratings\': {\'Cleanliness\': 5, \'Location\': 5, \'Overall\': 4, \'Rooms\': 4, \'Service\': 4, \'Sleep Quality\': 5, \'Value\': 4}}, {\'author\': \'Hildegard Larkin\', \'content\': \'This hotel is a safe bet for a value stay in French Quarter. Close enough to all sites and action but just out of the real loud & noisy streets. Check in is quick and friendly and room ( king side balcony) while dated was good size and clean. Small balcony with table & chairs is a nice option for evening drink & passing sites below. Down side is no mimi bar fridge ( they are available upon request on a first come basis apparently, so book one when you make initial reservation if necessary) Bathroom is adequate with ok shower pressure and housekeeping is quick and efficient. TIP; forget paying high price for conducted local tours, just take the red trams to end of line and back and then next day the green tram to cross town garden district and zoo and museums. cost for each ride $2.00 each way!! fantastic. Tip: If you stay during hot weather make sure you top up on ice early as later guests can ""run the machine dry"" for short time. Overall experience met expectations and would recommend for value stay.\', \'date\': \'2012-01-01 18:48:30 +0300\', \'ratings\': {\'Cleanliness\': 4, \'Location\': 4, \'Overall\': 4, \'Rooms\': 3, \'Service\': 4, \'Sleep Quality\': 3, \'Value\': 4}}, {\'author\': \'Uriah Rohan\', \'content\': \'The Chateau Le Moyne Holiday Inn is in a perfect location in the French Quarter, a block away from the craziness on Bourbon St. We got a fantastic deal on Priceline and were expecting a standard room for the price. The pleasant hotel clerk upgraded our room much to our delight, without us asking and the concierge also went above an beyond to assist us with information and suggestions for places to dine and possessed an ""can do"" attitude. Nice pool area to cool off in during the midday NOLA heat. It is definitely a three star establishment, not super luxurious but the beds were comfy and the location superb! If you can get a deal on Priceline, etc, it\\\'s a great value.\', \'date\': \'2014-08-04 15:17:49 +0300\', \'ratings\': {\'Cleanliness\': 4, \'Location\': 5, \'Overall\': 4, \'Rooms\': 3, \'Service\': 5, \'Sleep Quality\': 4, \'Value\': 4}}]\nstate: California\ntitle: Goleta\ntollfree: None\ntype: hotel\nurl: http://www.bacararesort.com/\nvacancy: True'Specifying Fields with Content and Metadata‚ÄãThe fields that are part of the Document content can be specified using\nthe page_content_fields parameter. The metadata fields for the\nDocument can be specified using the metadata_fields parameter.loader_with_selected_fields = CouchbaseLoader(    connection_string,    db_username,    db_password,    query,    page_content_fields=[        ""address"",        ""name"",        ""city"",        ""phone"",        ""country"",        ""geo"",        ""description"",        ""reviews"",    ],    metadata_fields=[""id""],)docs_with_selected_fields = loader_with_selected_fields.load()print(docs_with_selected_fields)[Document(page_content='address: 8301 Hollister Ave\ncity: Santa Barbara\ncountry: United States\ndescription: Located on 78 acres of oceanfront property, this resort is an upscale experience that caters to luxury travelers. There are 354 guest rooms in 19 separate villas, each in a Spanish style. Property amenities include saline infinity pools, a private beach, clay tennis courts, a 42,000 foot spa and fitness center, and nature trails through the adjoining wetland and forest. The onsite Miro restaurant provides great views of the coast with excellent food and service. With all that said, you pay for the experience, and this resort is not for the budget traveler.  In addition to quoted rates there is a $25 per day resort fee that includes a bottle of wine in your room, two bottles of water, access to fitness center and spa, and internet access.\ngeo: {\'accuracy\': \'ROOFTOP\', \'lat\': 34.43429, \'lon\': -119.92137}\nname: Bacara Resort & Spa\nphone: None\nreviews: [{\'author\': \'Delmer Cole\', \'content\': ""Jane and Joyce make every effort to see to your personal needs and comfort. The rooms take one back in time to the original styles and designs of the 1800\'s. A real connection to local residents, the 905 is a regular tour stop and the oldest hotel in the French Quarter. My wife and I prefer to stay in the first floor rooms where there is a sitting room with TV, bedroom, bath and kitchen. The kitchen has a stove and refrigerator, sink, coffeemaker, etc. Plus there is a streetside private entrance (very good security system) and a covered balcony area with seating so you can watch passersby. Quaint, cozy, and most of all: ORIGINAL. No plastic remods. Feels like my great Grandmother\'s place. While there are more luxurious places to stay, if you want the real flavor and eclectic style of N.O. you have to stay here. It just FEELS like New Orleans. The location is one block towards the river from Bourbon Street and smack dab in the middle of everything. Royal street is one of the nicest residential streets in the Quarter and you can walk back to your room and get some peace and quiet whenever you like. The French Quarter is always busy so we bring a small fan to turn on to make some white noise so we can sleep more soundly. Works great. You might not need it at the 905 but it\'s a necessity it if you stay on or near Bourbon Street, which is very loud all the time. Parking tips: You can park right in front to unload and it\'s only a couple blocks to the secure riverfront parking area. Plus there are several public parking lots nearby. My strategy is to get there early, unload, and drive around for a while near the hotel. It\'s not too hard to find a parking place but be careful about where it is. Stay away from corner spots since streets are narrow and delivery trucks don\'t have the room to turn and they will hit your car. Take note of the signs. Tuesday and Thursday they clean the streets and you can\'t park in many areas when they do or they will tow your car. Once you find a spot don\'t move it since everything is walking distance. If you find a good spot and get a ticket it will cost $20, which is cheaper than the daily rate at most parking garages. Even if you don\'t get a ticket make sure to go online to N.O. traffic ticket site to check your license number for violations. Some local kids think it\'s funny to take your ticket and throw it away since the fine doubles every month it\'s not paid. You don\'t know you got a ticket but your fine is getting bigger. We\'ve been coming to the French Quarter for years and have stayed at many of the local hotels. The 905 Royal is our favorite."", \'date\': \'2013-12-05 09:27:07 +0300\', \'ratings\': {\'Cleanliness\': 5, \'Location\': 5, \'Overall\': 5, \'Rooms\': 5, \'Service\': 5, \'Sleep Quality\': 5, \'Value\': 5}}, {\'author\': \'Orval Lebsack\', \'content\': \'I stayed there with a friend for a girls trip around St. Patricks Day. This was my third time to NOLA, my first at Chateau Lemoyne. The location is excellent....very easy walking distance to everything, without the chaos of staying right on Bourbon Street. Even though its a Holiday Inn, it still has the historical feel and look of NOLA. The pool looked nice too, even though we never used it. The staff was friendly and helpful. Chateau Lemoyne would be hard to top, considering the price.\', \'date\': \'2013-10-26 15:01:39 +0300\', \'ratings\': {\'Cleanliness\': 5, \'Location\': 5, \'Overall\': 4, \'Rooms\': 4, \'Service\': 4, \'Sleep Quality\': 5, \'Value\': 4}}, {\'author\': \'Hildegard Larkin\', \'content\': \'This hotel is a safe bet for a value stay in French Quarter. Close enough to all sites and action but just out of the real loud & noisy streets. Check in is quick and friendly and room ( king side balcony) while dated was good size and clean. Small balcony with table & chairs is a nice option for evening drink & passing sites below. Down side is no mimi bar fridge ( they are available upon request on a first come basis apparently, so book one when you make initial reservation if necessary) Bathroom is adequate with ok shower pressure and housekeeping is quick and efficient. TIP; forget paying high price for conducted local tours, just take the red trams to end of line and back and then next day the green tram to cross town garden district and zoo and museums. cost for each ride $2.00 each way!! fantastic. Tip: If you stay during hot weather make sure you top up on ice early as later guests can ""run the machine dry"" for short time. Overall experience met expectations and would recommend for value stay.\', \'date\': \'2012-01-01 18:48:30 +0300\', \'ratings\': {\'Cleanliness\': 4, \'Location\': 4, \'Overall\': 4, \'Rooms\': 3, \'Service\': 4, \'Sleep Quality\': 3, \'Value\': 4}}, {\'author\': \'Uriah Rohan\', \'content\': \'The Chateau Le Moyne Holiday Inn is in a perfect location in the French Quarter, a block away from the craziness on Bourbon St. We got a fantastic deal on Priceline and were expecting a standard room for the price. The pleasant hotel clerk upgraded our room much to our delight, without us asking and the concierge also went above an beyond to assist us with information and suggestions for places to dine and possessed an ""can do"" attitude. Nice pool area to cool off in during the midday NOLA heat. It is definitely a three star establishment, not super luxurious but the beds were comfy and the location superb! If you can get a deal on Priceline, etc, it\\\'s a great value.\', \'date\': \'2014-08-04 15:17:49 +0300\', \'ratings\': {\'Cleanliness\': 4, \'Location\': 5, \'Overall\': 4, \'Rooms\': 3, \'Service\': 5, \'Sleep Quality\': 4, \'Value\': 4}}]', metadata={'id': 10180})]",en,
https://python.langchain.com/docs/integrations/document_loaders/brave_search,Brave Search | ü¶úÔ∏èüîó Langchain,Brave Search is a search,"Brave SearchBrave Search is a search\nengine developed by Brave Software. - Brave Search uses its own web\nindex. As of May 2022, it covered over 10 billion pages and was used\nto serve 92% of search results without relying on any third-parties,\nwith the remainder being retrieved server-side from the Bing API or\n(on an opt-in basis) client-side from Google. According to Brave, the\nindex was kept ‚Äúintentionally smaller than that of Google or Bing‚Äù in\norder to help avoid spam and other low-quality content, with the\ndisadvantage that ‚ÄúBrave Search is not yet as good as Google in\nrecovering long-tail queries.‚Äù - Brave Search Premium: As of April\n2023 Brave Search is an ad-free website, but it will eventually switch\nto a new model that will include ads and premium users will get an\nad-free experience. User data including IP addresses won‚Äôt be\ncollected from its users by default. A premium account will be\nrequired for opt-in data-collection.Installation and Setup‚ÄãTo get access to the Brave Search API, you need to create an account\nand get an API key.api_key = ""...""from langchain_community.document_loaders import BraveSearchLoaderExample‚Äãloader = BraveSearchLoader(    query=""obama middle name"", api_key=api_key, search_kwargs={""count"": 3})docs = loader.load()len(docs)3[doc.metadata for doc in docs][{'title': ""Obama's Middle Name -- My Last Name -- is 'Hussein.' So?"",  'link': 'https://www.cair.com/cair_in_the_news/obamas-middle-name-my-last-name-is-hussein-so/'}, {'title': ""What's up with Obama's middle name? - Quora"",  'link': 'https://www.quora.com/Whats-up-with-Obamas-middle-name'}, {'title': 'Barack Obama | Biography, Parents, Education, Presidency, Books, ...',  'link': 'https://www.britannica.com/biography/Barack-Obama'}][doc.page_content for doc in docs]['I wasn‚Äôt sure whether to laugh or cry a few days back listening to radio talk show host Bill Cunningham repeatedly scream Barack <strong>Obama</strong>‚Äô<strong>s</strong> <strong>middle</strong> <strong>name</strong> ‚Äî my last <strong>name</strong> ‚Äî as if he had anti-Muslim Tourette‚Äôs. ‚ÄúHussein,‚Äù Cunningham hissed like he was beckoning Satan when shouting the ...', 'Answer (1 of 15): A better question would be, ‚ÄúWhat‚Äôs up with <strong>Obama</strong>‚Äôs first <strong>name</strong>?‚Äù President Barack Hussein <strong>Obama</strong>‚Äôs father‚Äôs <strong>name</strong> was Barack Hussein <strong>Obama</strong>. He was <strong>named</strong> after his father. Hussein, <strong>Obama</strong>‚Äô<strong>s</strong> <strong>middle</strong> <strong>name</strong>, is a very common Arabic <strong>name</strong>, meaning &quot;good,&quot; &quot;handsome,&quot; or ...', 'Barack <strong>Obama</strong>, in full Barack Hussein <strong>Obama</strong> II, (born August 4, 1961, Honolulu, Hawaii, U.S.), 44th president of the United States (2009‚Äì17) and the first African American to hold the office. Before winning the presidency, <strong>Obama</strong> represented Illinois in the U.S.']",en,
https://python.langchain.com/docs/integrations/document_loaders/tencent_cos_directory,Tencent COS Directory | ü¶úÔ∏èüîó Langchain,[Tencent Cloud Object Storage,"Tencent COS DirectoryTencent Cloud Object Storage\n(COS) is a distributed\nstorage service that enables you to store any amount of data from\nanywhere via HTTP/HTTPS protocols. COS has no restrictions on data\nstructure or format. It also has no bucket size limit and partition\nmanagement, making it suitable for virtually any use case, such as\ndata delivery, data processing, and data lakes. COS provides a\nweb-based console, multi-language SDKs and APIs, command line tool,\nand graphical tools. It works well with Amazon S3 APIs, allowing you\nto quickly access community tools and plugins.This covers how to load document objects from a Tencent COS Directory.%pip install --upgrade --quiet  cos-python-sdk-v5from langchain_community.document_loaders import TencentCOSDirectoryLoaderfrom qcloud_cos import CosConfigconf = CosConfig(    Region=""your cos region"",    SecretId=""your cos secret_id"",    SecretKey=""your cos secret_key"",)loader = TencentCOSDirectoryLoader(conf=conf, bucket=""you_cos_bucket"")loader.load()Specifying a prefix‚ÄãYou can also specify a prefix for more finegrained control over what\nfiles to load.loader = TencentCOSDirectoryLoader(conf=conf, bucket=""you_cos_bucket"", prefix=""fake"")loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/tensorflow_datasets,TensorFlow Datasets | ü¶úÔ∏èüîó Langchain,TensorFlow Datasets is a,"TensorFlow DatasetsTensorFlow Datasets is a\ncollection of datasets ready to use, with TensorFlow or other Python\nML frameworks, such as Jax. All datasets are exposed as\ntf.data.Datasets,\nenabling easy-to-use and high-performance input pipelines. To get\nstarted see the guide\nand the list of\ndatasets.This notebook shows how to load TensorFlow Datasets into a Document\nformat that we can use downstream.Installation‚ÄãYou need to install tensorflow and tensorflow-datasets python\npackages.%pip install --upgrade --quiet  tensorflow%pip install --upgrade --quiet  tensorflow-datasetsExample‚ÄãAs an example, we use the mlqa/en\ndataset.MLQA (Multilingual Question Answering Dataset) is a benchmark\ndataset for evaluating multilingual question answering performance.\nThe dataset consists of 7 languages: Arabic, German, Spanish, English,\nHindi, Vietnamese, Chinese.Homepage: https://github.com/facebookresearch/MLQASource code: tfds.datasets.mlqa.BuilderDownload size: 72.21 MiB# Feature structure of `mlqa/en` dataset:FeaturesDict(    {        ""answers"": Sequence(            {                ""answer_start"": int32,                ""text"": Text(shape=(), dtype=string),            }        ),        ""context"": Text(shape=(), dtype=string),        ""id"": string,        ""question"": Text(shape=(), dtype=string),        ""title"": Text(shape=(), dtype=string),    })import tensorflow as tfimport tensorflow_datasets as tfds# try directly access this dataset:ds = tfds.load(""mlqa/en"", split=""test"")ds = ds.take(1)  # Only take a single exampleds<_TakeDataset element_spec={'answers': {'answer_start': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'text': TensorSpec(shape=(None,), dtype=tf.string, name=None)}, 'context': TensorSpec(shape=(), dtype=tf.string, name=None), 'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'question': TensorSpec(shape=(), dtype=tf.string, name=None), 'title': TensorSpec(shape=(), dtype=tf.string, name=None)}>Now we have to create a custom function to convert dataset sample into a\nDocument.This is a requirement. There is no standard format for the TF datasets\nthat‚Äôs why we need to make a custom transformation function.Let‚Äôs use context field as the Document.page_content and place other\nfields in the Document.metadata.from langchain_core.documents import Documentdef decode_to_str(item: tf.Tensor) -> str:    return item.numpy().decode(""utf-8"")def mlqaen_example_to_document(example: dict) -> Document:    return Document(        page_content=decode_to_str(example[""context""]),        metadata={            ""id"": decode_to_str(example[""id""]),            ""title"": decode_to_str(example[""title""]),            ""question"": decode_to_str(example[""question""]),            ""answer"": decode_to_str(example[""answers""][""text""][0]),        },    )for example in ds:    doc = mlqaen_example_to_document(example)    print(doc)    breakpage_content='After completing the journey around South America, on 23 February 2006, Queen Mary 2 met her namesake, the original RMS Queen Mary, which is permanently docked at Long Beach, California. Escorted by a flotilla of smaller ships, the two Queens exchanged a ""whistle salute"" which was heard throughout the city of Long Beach. Queen Mary 2 met the other serving Cunard liners Queen Victoria and Queen Elizabeth 2 on 13 January 2008 near the Statue of Liberty in New York City harbour, with a celebratory fireworks display; Queen Elizabeth 2 and Queen Victoria made a tandem crossing of the Atlantic for the meeting. This marked the first time three Cunard Queens have been present in the same location. Cunard stated this would be the last time these three ships would ever meet, due to Queen Elizabeth 2\'s impending retirement from service in late 2008. However this would prove not to be the case, as the three Queens met in Southampton on 22 April 2008. Queen Mary 2 rendezvoused with Queen Elizabeth 2  in Dubai on Saturday 21 March 2009, after the latter ship\'s retirement, while both ships were berthed at Port Rashid. With the withdrawal of Queen Elizabeth 2 from Cunard\'s fleet and its docking in Dubai, Queen Mary 2 became the only ocean liner left in active passenger service.' metadata={'id': '5116f7cccdbf614d60bcd23498274ffd7b1e4ec7', 'title': 'RMS Queen Mary 2', 'question': 'What year did Queen Mary 2 complete her journey around South America?', 'answer': '2006'}2023-08-03 14:27:08.482983: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.from langchain_community.document_loaders import TensorflowDatasetLoaderfrom langchain_core.documents import Documentloader = TensorflowDatasetLoader(    dataset_name=""mlqa/en"",    split_name=""test"",    load_max_docs=3,    sample_to_document_function=mlqaen_example_to_document,)TensorflowDatasetLoader has these parameters: - dataset_name: the\nname of the dataset to load - split_name: the name of the split to\nload. Defaults to ‚Äútrain‚Äù. - load_max_docs: a limit to the number of\nloaded documents. Defaults to 100. - sample_to_document_function: a\nfunction that converts a dataset sample to a Documentdocs = loader.load()len(docs)2023-08-03 14:27:22.998964: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.3docs[0].page_content'After completing the journey around South America, on 23 February 2006, Queen Mary 2 met her namesake, the original RMS Queen Mary, which is permanently docked at Long Beach, California. Escorted by a flotilla of smaller ships, the two Queens exchanged a ""whistle salute"" which was heard throughout the city of Long Beach. Queen Mary 2 met the other serving Cunard liners Queen Victoria and Queen Elizabeth 2 on 13 January 2008 near the Statue of Liberty in New York City harbour, with a celebratory fireworks display; Queen Elizabeth 2 and Queen Victoria made a tandem crossing of the Atlantic for the meeting. This marked the first time three Cunard Queens have been present in the same location. Cunard stated this would be the last time these three ships would ever meet, due to Queen Elizabeth 2\'s impending retirement from service in late 2008. However this would prove not to be the case, as the three Queens met in Southampton on 22 April 2008. Queen Mary 2 rendezvoused with Queen Elizabeth 2  in Dubai on Saturday 21 March 2009, after the latter ship\'s retirement, while both ships were berthed at Port Rashid. With the withdrawal of Queen Elizabeth 2 from Cunard\'s fleet and its docking in Dubai, Queen Mary 2 became the only ocean liner left in active passenger service.'docs[0].metadata{'id': '5116f7cccdbf614d60bcd23498274ffd7b1e4ec7', 'title': 'RMS Queen Mary 2', 'question': 'What year did Queen Mary 2 complete her journey around South America?', 'answer': '2006'}",en,
https://python.langchain.com/docs/integrations/document_loaders/confluence,Confluence | ü¶úÔ∏èüîó Langchain,Confluence is a wiki,"ConfluenceConfluence is a wiki\ncollaboration platform that saves and organizes all of the\nproject-related material. Confluence is a knowledge base that\nprimarily handles content management activities.A loader for Confluence pages.This currently supports username/api_key, Oauth2 login.\nAdditionally, on-prem installations also support token authentication.Specify a list page_id-s and/or space_key to load in the\ncorresponding pages into Document objects, if both are specified the\nunion of both sets will be returned.You can also specify a boolean include_attachments to include\nattachments, this is set to False by default, if set to True all\nattachments will be downloaded and ConfluenceReader will extract the\ntext from the attachments and add it to the Document object. Currently\nsupported attachment types are: PDF, PNG, JPEG/JPG, SVG, Word\nand Excel.Hint: space_key and page_id can both be found in the URL of a page\nin Confluence -\nhttps://yoursite.atlassian.com/wiki/spaces/<space_key>/pages/<page_id>Before using ConfluenceLoader make sure you have the latest version of\nthe atlassian-python-api package installed:%pip install --upgrade --quiet  atlassian-python-apiExamples‚ÄãUsername and Password or Username and API Token (Atlassian Cloud only)‚ÄãThis example authenticates using either a username and password or, if\nyou‚Äôre connecting to an Atlassian Cloud hosted version of Confluence, a\nusername and an API Token. You can generate an API token at:\nhttps://id.atlassian.com/manage-profile/security/api-tokens.The limit parameter specifies how many documents will be retrieved in\na single call, not how many documents will be retrieved in total. By\ndefault the code will return up to 1000 documents in 50 documents\nbatches. To control the total number of documents use the max_pages\nparameter. Plese note the maximum value for the limit parameter in the\natlassian-python-api package is currently 100.from langchain_community.document_loaders import ConfluenceLoaderloader = ConfluenceLoader(    url=""https://yoursite.atlassian.com/wiki"", username=""me"", api_key=""12345"")documents = loader.load(space_key=""SPACE"", include_attachments=True, limit=50)Personal Access Token (Server/On-Prem only)‚ÄãThis method is valid for the Data Center/Server on-prem edition only.\nFor more information on how to generate a Personal Access Token (PAT)\ncheck the official Confluence documentation at:\nhttps://confluence.atlassian.com/enterprise/using-personal-access-tokens-1026032365.html.\nWhen using a PAT you provide only the token value, you cannot provide a\nusername. Please note that ConfluenceLoader will run under the\npermissions of the user that generated the PAT and will only be able to\nload documents for which said user has access to.from langchain_community.document_loaders import ConfluenceLoaderloader = ConfluenceLoader(url=""https://yoursite.atlassian.com/wiki"", token=""12345"")documents = loader.load(    space_key=""SPACE"", include_attachments=True, limit=50, max_pages=50)",en,
https://python.langchain.com/docs/integrations/document_loaders/nuclia,Nuclia | ü¶úÔ∏èüîó Langchain,Nuclia automatically indexes your unstructured,"NucliaNuclia automatically indexes your unstructured\ndata from any internal and external source, providing optimized search\nresults and generative answers. It can handle video and audio\ntranscription, image content extraction, and document parsing.The Nuclia Understanding API supports the processing of unstructured\ndata, including text, web pages, documents, and audio/video contents.\nIt extracts all texts wherever they are (using speech-to-text or OCR\nwhen needed), it also extracts metadata, embedded files (like images\nin a PDF), and web links. If machine learning is enabled, it\nidentifies entities, provides a summary of the content and generates\nembeddings for all the sentences.Setup‚ÄãTo use the Nuclia Understanding API, you need to have a Nuclia\naccount. You can create one for free at https://nuclia.cloud, and then\ncreate a NUA\nkey.%pip install --upgrade --quiet  protobuf%pip install --upgrade --quiet  nucliadb-protosimport osos.environ[""NUCLIA_ZONE""] = ""<YOUR_ZONE>""  # e.g. europe-1os.environ[""NUCLIA_NUA_KEY""] = ""<YOUR_API_KEY>""Example‚ÄãTo use the Nuclia document loader, you need to instantiate a\nNucliaUnderstandingAPI tool:from langchain_community.tools.nuclia import NucliaUnderstandingAPInua = NucliaUnderstandingAPI(enable_ml=False)from langchain_community.document_loaders.nuclia import NucliaLoaderloader = NucliaLoader(""./interview.mp4"", nua)You can now call the load the document in a loop until you get the\ndocument.import timepending = Truewhile pending:    time.sleep(15)    docs = loader.load()    if len(docs) > 0:        print(docs[0].page_content)        print(docs[0].metadata)        pending = False    else:        print(""waiting..."")Retrieved information‚ÄãNuclia returns the following information:file metadataextracted textnested text (like text in an embedded image)paragraphs and sentences splitting (defined by the position of their\nfirst and last characters, plus start time and end time for a video\nor audio file)linksa thumbnailembedded filesNote:Generated files (thumbnail, extracted embedded files, etc.) are provided\nas a token. You can download them with the /processing/download\nendpoint.Also at any level, if an attribute exceeds a certain size, it will be\nput in a downloadable file and will be replaced in the document by a\nfile pointer. This will consist of {""file"": {""uri"": ""JWT_TOKEN""}}. The\nrule is that if the size of the message is greater than 1000000\ncharacters, the biggest parts will be moved to downloadable files.\nFirst, the compression process will target vectors. If that is not\nenough, it will target large field metadata, and finally it will target\nextracted text.",en,
https://python.langchain.com/docs/integrations/document_loaders/youtube_transcript,YouTube transcripts | ü¶úÔ∏èüîó Langchain,YouTube is an online video sharing and,"YouTube transcriptsYouTube is an online video sharing and\nsocial media platform created by Google.This notebook covers how to load documents from YouTube transcripts.from langchain_community.document_loaders import YoutubeLoader%pip install --upgrade --quiet  youtube-transcript-apiloader = YoutubeLoader.from_youtube_url(    ""https://www.youtube.com/watch?v=QsYGlZkevEg"", add_video_info=False)loader.load()Add video info‚Äã%pip install --upgrade --quiet  pytubeloader = YoutubeLoader.from_youtube_url(    ""https://www.youtube.com/watch?v=QsYGlZkevEg"", add_video_info=True)loader.load()Add language preferences‚ÄãLanguage param : It‚Äôs a list of language codes in a descending priority,\nen by default.translation param : It‚Äôs a translate preference, you can translate\navailable transcript to your preferred language.loader = YoutubeLoader.from_youtube_url(    ""https://www.youtube.com/watch?v=QsYGlZkevEg"",    add_video_info=True,    language=[""en"", ""id""],    translation=""en"",)loader.load()YouTube loader from Google Cloud‚ÄãPrerequisites‚ÄãCreate a Google Cloud project or use an existing projectEnable the Youtube\nApiAuthorize credentials for desktop\napppip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib youtube-transcript-apiüßë Instructions for ingesting your Google Docs data‚ÄãBy default, the GoogleDriveLoader expects the credentials.json file\nto be ~/.credentials/credentials.json, but this is configurable using\nthe credentials_file keyword argument. Same thing with token.json.\nNote that token.json will be created automatically the first time you\nuse the loader.GoogleApiYoutubeLoader can load from a list of Google Docs document\nids or a folder id. You can obtain your folder and document id from the\nURL: Note depending on your set up, the service_account_path needs to\nbe set up. See\nhere for\nmore details.# Init the GoogleApiClientfrom pathlib import Pathfrom langchain_community.document_loaders import GoogleApiClient, GoogleApiYoutubeLoadergoogle_api_client = GoogleApiClient(credentials_path=Path(""your_path_creds.json""))# Use a Channelyoutube_loader_channel = GoogleApiYoutubeLoader(    google_api_client=google_api_client,    channel_name=""Reducible"",    captions_language=""en"",)# Use Youtube Idsyoutube_loader_ids = GoogleApiYoutubeLoader(    google_api_client=google_api_client, video_ids=[""TrdevFK_am4""], add_video_info=True)# returns a list of Documentsyoutube_loader_channel.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/obsidian,Obsidian | ü¶úÔ∏èüîó Langchain,Obsidian is a powerful and extensible,"ObsidianObsidian is a powerful and extensible\nknowledge base that works on top of your local folder of plain text\nfiles.This notebook covers how to load documents from an Obsidian database.Since Obsidian is just stored on disk as a folder of Markdown files,\nthe loader just takes a path to this directory.Obsidian files also sometimes contain\nmetadata\nwhich is a YAML block at the top of the file. These values will be added\nto the document‚Äôs metadata. (ObsidianLoader can also be passed a\ncollect_metadata=False argument to disable this behavior.)from langchain_community.document_loaders import ObsidianLoaderloader = ObsidianLoader(""<path-to-obsidian>"")docs = loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/arcgis,ArcGIS | ü¶úÔ∏èüîó Langchain,This notebook demonstrates the use of the,"ArcGISThis notebook demonstrates the use of the\nlangchain_community.document_loaders.ArcGISLoader class.You will need to install the ArcGIS API for Python arcgis and,\noptionally, bs4.BeautifulSoup.You can use an arcgis.gis.GIS object for authenticated data loading,\nor leave it blank to access public data.from langchain_community.document_loaders import ArcGISLoaderURL = ""https://maps1.vcgov.org/arcgis/rest/services/Beaches/MapServer/7""loader = ArcGISLoader(URL)docs = loader.load()Let‚Äôs measure loader latency.%%timedocs = loader.load()CPU times: user 2.37 ms, sys: 5.83 ms, total: 8.19 msWall time: 1.05 sdocs[0].metadata{'accessed': '2023-09-13T19:58:32.546576+00:00Z', 'name': 'Beach Ramps', 'url': 'https://maps1.vcgov.org/arcgis/rest/services/Beaches/MapServer/7', 'layer_description': '(Not Provided)', 'item_description': '(Not Provided)', 'layer_properties': {   ""currentVersion"": 10.81,   ""id"": 7,   ""name"": ""Beach Ramps"",   ""type"": ""Feature Layer"",   ""description"": """",   ""geometryType"": ""esriGeometryPoint"",   ""sourceSpatialReference"": {     ""wkid"": 2881,     ""latestWkid"": 2881   },   ""copyrightText"": """",   ""parentLayer"": null,   ""subLayers"": [],   ""minScale"": 750000,   ""maxScale"": 0,   ""drawingInfo"": {     ""renderer"": {       ""type"": ""simple"",       ""symbol"": {         ""type"": ""esriPMS"",         ""url"": ""9bb2e5ca499bb68aa3ee0d4e1ecc3849"",         ""imageData"": ""iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAAXNSR0IB2cksfwAAAAlwSFlzAAAOxAAADsQBlSsOGwAAAJJJREFUOI3NkDEKg0AQRZ9kkSnSGBshR7DJqdJYeg7BMpcS0uQWQsqoCLExkcUJzGqT38zw2fcY1rEzbp7vjXz0EXC7gBxs1ABcG/8CYkCcDqwyLqsV+RlV0I/w7PzuJBArr1VB20H58Ls6h+xoFITkTwWpQJX7XSIBAnFwVj7MLAjJV/AC6G3QoAmK+74Lom04THTBEp/HCSc6AAAAAElFTkSuQmCC"",         ""contentType"": ""image/png"",         ""width"": 12,         ""height"": 12,         ""angle"": 0,         ""xoffset"": 0,         ""yoffset"": 0       },       ""label"": """",       ""description"": """"     },     ""transparency"": 0,     ""labelingInfo"": null   },   ""defaultVisibility"": true,   ""extent"": {     ""xmin"": -81.09480168806815,     ""ymin"": 28.858349245353473,     ""xmax"": -80.77512908572814,     ""ymax"": 29.41078388840041,     ""spatialReference"": {       ""wkid"": 4326,       ""latestWkid"": 4326     }   },   ""hasAttachments"": false,   ""htmlPopupType"": ""esriServerHTMLPopupTypeNone"",   ""displayField"": ""AccessName"",   ""typeIdField"": null,   ""subtypeFieldName"": null,   ""subtypeField"": null,   ""defaultSubtypeCode"": null,   ""fields"": [     {       ""name"": ""OBJECTID"",       ""type"": ""esriFieldTypeOID"",       ""alias"": ""OBJECTID"",       ""domain"": null     },     {       ""name"": ""Shape"",       ""type"": ""esriFieldTypeGeometry"",       ""alias"": ""Shape"",       ""domain"": null     },     {       ""name"": ""AccessName"",       ""type"": ""esriFieldTypeString"",       ""alias"": ""AccessName"",       ""length"": 40,       ""domain"": null     },     {       ""name"": ""AccessID"",       ""type"": ""esriFieldTypeString"",       ""alias"": ""AccessID"",       ""length"": 50,       ""domain"": null     },     {       ""name"": ""AccessType"",       ""type"": ""esriFieldTypeString"",       ""alias"": ""AccessType"",       ""length"": 25,       ""domain"": null     },     {       ""name"": ""GeneralLoc"",       ""type"": ""esriFieldTypeString"",       ""alias"": ""GeneralLoc"",       ""length"": 100,       ""domain"": null     },     {       ""name"": ""MilePost"",       ""type"": ""esriFieldTypeDouble"",       ""alias"": ""MilePost"",       ""domain"": null     },     {       ""name"": ""City"",       ""type"": ""esriFieldTypeString"",       ""alias"": ""City"",       ""length"": 50,       ""domain"": null     },     {       ""name"": ""AccessStatus"",       ""type"": ""esriFieldTypeString"",       ""alias"": ""AccessStatus"",       ""length"": 50,       ""domain"": null     },     {       ""name"": ""Entry_Date_Time"",       ""type"": ""esriFieldTypeDate"",       ""alias"": ""Entry_Date_Time"",       ""length"": 8,       ""domain"": null     },     {       ""name"": ""DrivingZone"",       ""type"": ""esriFieldTypeString"",       ""alias"": ""DrivingZone"",       ""length"": 50,       ""domain"": null     }   ],   ""geometryField"": {     ""name"": ""Shape"",     ""type"": ""esriFieldTypeGeometry"",     ""alias"": ""Shape""   },   ""indexes"": null,   ""subtypes"": [],   ""relationships"": [],   ""canModifyLayer"": true,   ""canScaleSymbols"": false,   ""hasLabels"": false,   ""capabilities"": ""Map,Query,Data"",   ""maxRecordCount"": 1000,   ""supportsStatistics"": true,   ""supportsAdvancedQueries"": true,   ""supportedQueryFormats"": ""JSON, geoJSON"",   ""isDataVersioned"": false,   ""ownershipBasedAccessControlForFeatures"": {     ""allowOthersToQuery"": true   },   ""useStandardizedQueries"": true,   ""advancedQueryCapabilities"": {     ""useStandardizedQueries"": true,     ""supportsStatistics"": true,     ""supportsHavingClause"": true,     ""supportsCountDistinct"": true,     ""supportsOrderBy"": true,     ""supportsDistinct"": true,     ""supportsPagination"": true,     ""supportsTrueCurve"": true,     ""supportsReturningQueryExtent"": true,     ""supportsQueryWithDistance"": true,     ""supportsSqlExpression"": true   },   ""supportsDatumTransformation"": true,   ""dateFieldsTimeReference"": null,   ""supportsCoordinatesQuantization"": true }}Retrieving Geometries‚ÄãIf you want to retrieve feature geometries, you may do so with the\nreturn_geometry keyword.Each document‚Äôs geometry will be stored in its metadata dictionary.loader_geom = ArcGISLoader(URL, return_geometry=True)%%timedocs = loader_geom.load()CPU times: user 9.6 ms, sys: 5.84 ms, total: 15.4 msWall time: 1.06 sdocs[0].metadata[""geometry""]{'x': -81.01508803280349, 'y': 29.24246579525828, 'spatialReference': {'wkid': 4326, 'latestWkid': 4326}}for doc in docs:    print(doc.page_content){""OBJECTID"": 4, ""AccessName"": ""UNIVERSITY BLVD"", ""AccessID"": ""DB-048"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""900 BLK N ATLANTIC AV"", ""MilePost"": 13.74, ""City"": ""DAYTONA BEACH"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694597536000, ""DrivingZone"": ""BOTH""}{""OBJECTID"": 18, ""AccessName"": ""BEACHWAY AV"", ""AccessID"": ""NS-106"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""1400 N ATLANTIC AV"", ""MilePost"": 1.57, ""City"": ""NEW SMYRNA BEACH"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694600478000, ""DrivingZone"": ""YES""}{""OBJECTID"": 24, ""AccessName"": ""27TH AV"", ""AccessID"": ""NS-141"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""3600 BLK S ATLANTIC AV"", ""MilePost"": 4.83, ""City"": ""NEW SMYRNA BEACH"", ""AccessStatus"": ""CLOSED FOR HIGH TIDE"", ""Entry_Date_Time"": 1694619363000, ""DrivingZone"": ""BOTH""}{""OBJECTID"": 26, ""AccessName"": ""SEABREEZE BLVD"", ""AccessID"": ""DB-051"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""500 BLK N ATLANTIC AV"", ""MilePost"": 14.24, ""City"": ""DAYTONA BEACH"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694597536000, ""DrivingZone"": ""BOTH""}{""OBJECTID"": 30, ""AccessName"": ""INTERNATIONAL SPEEDWAY BLVD"", ""AccessID"": ""DB-059"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""300 BLK S ATLANTIC AV"", ""MilePost"": 15.27, ""City"": ""DAYTONA BEACH"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694598638000, ""DrivingZone"": ""BOTH""}{""OBJECTID"": 33, ""AccessName"": ""GRANADA BLVD"", ""AccessID"": ""OB-030"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""20 BLK OCEAN SHORE BLVD"", ""MilePost"": 10.02, ""City"": ""ORMOND BEACH"", ""AccessStatus"": ""4X4 ONLY"", ""Entry_Date_Time"": 1694595424000, ""DrivingZone"": ""BOTH""}{""OBJECTID"": 39, ""AccessName"": ""BEACH ST"", ""AccessID"": ""PI-097"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""4890 BLK S ATLANTIC AV"", ""MilePost"": 25.85, ""City"": ""PONCE INLET"", ""AccessStatus"": ""4X4 ONLY"", ""Entry_Date_Time"": 1694596294000, ""DrivingZone"": ""BOTH""}{""OBJECTID"": 44, ""AccessName"": ""SILVER BEACH AV"", ""AccessID"": ""DB-064"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""1000 BLK S ATLANTIC AV"", ""MilePost"": 15.98, ""City"": ""DAYTONA BEACH"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694598638000, ""DrivingZone"": ""YES""}{""OBJECTID"": 45, ""AccessName"": ""BOTEFUHR AV"", ""AccessID"": ""DBS-067"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""1900 BLK S ATLANTIC AV"", ""MilePost"": 16.68, ""City"": ""DAYTONA BEACH SHORES"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694598638000, ""DrivingZone"": ""YES""}{""OBJECTID"": 46, ""AccessName"": ""MINERVA RD"", ""AccessID"": ""DBS-069"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""2300 BLK S ATLANTIC AV"", ""MilePost"": 17.52, ""City"": ""DAYTONA BEACH SHORES"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694598638000, ""DrivingZone"": ""YES""}{""OBJECTID"": 56, ""AccessName"": ""3RD AV"", ""AccessID"": ""NS-118"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""1200 BLK HILL ST"", ""MilePost"": 3.25, ""City"": ""NEW SMYRNA BEACH"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694600478000, ""DrivingZone"": ""YES""}{""OBJECTID"": 65, ""AccessName"": ""MILSAP RD"", ""AccessID"": ""OB-037"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""700 BLK S ATLANTIC AV"", ""MilePost"": 11.52, ""City"": ""ORMOND BEACH"", ""AccessStatus"": ""4X4 ONLY"", ""Entry_Date_Time"": 1694595749000, ""DrivingZone"": ""YES""}{""OBJECTID"": 72, ""AccessName"": ""ROCKEFELLER DR"", ""AccessID"": ""OB-034"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""400 BLK S ATLANTIC AV"", ""MilePost"": 10.9, ""City"": ""ORMOND BEACH"", ""AccessStatus"": ""CLOSED - SEASONAL"", ""Entry_Date_Time"": 1694591351000, ""DrivingZone"": ""YES""}{""OBJECTID"": 74, ""AccessName"": ""DUNLAWTON BLVD"", ""AccessID"": ""DBS-078"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""3400 BLK S ATLANTIC AV"", ""MilePost"": 20.61, ""City"": ""DAYTONA BEACH SHORES"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694601124000, ""DrivingZone"": ""YES""}{""OBJECTID"": 77, ""AccessName"": ""EMILIA AV"", ""AccessID"": ""DBS-082"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""3790 BLK S ATLANTIC AV"", ""MilePost"": 21.38, ""City"": ""DAYTONA BEACH SHORES"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694601124000, ""DrivingZone"": ""BOTH""}{""OBJECTID"": 84, ""AccessName"": ""VAN AV"", ""AccessID"": ""DBS-075"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""3100 BLK S ATLANTIC AV"", ""MilePost"": 19.6, ""City"": ""DAYTONA BEACH SHORES"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694601124000, ""DrivingZone"": ""YES""}{""OBJECTID"": 104, ""AccessName"": ""HARVARD DR"", ""AccessID"": ""OB-038"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""900 BLK S ATLANTIC AV"", ""MilePost"": 11.72, ""City"": ""ORMOND BEACH"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694597536000, ""DrivingZone"": ""YES""}{""OBJECTID"": 106, ""AccessName"": ""WILLIAMS AV"", ""AccessID"": ""DB-042"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""2200 BLK N ATLANTIC AV"", ""MilePost"": 12.5, ""City"": ""DAYTONA BEACH"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694597536000, ""DrivingZone"": ""YES""}{""OBJECTID"": 109, ""AccessName"": ""HARTFORD AV"", ""AccessID"": ""DB-043"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""1890 BLK N ATLANTIC AV"", ""MilePost"": 12.76, ""City"": ""DAYTONA BEACH"", ""AccessStatus"": ""CLOSED - SEASONAL"", ""Entry_Date_Time"": 1694591351000, ""DrivingZone"": ""YES""}{""OBJECTID"": 138, ""AccessName"": ""CRAWFORD RD"", ""AccessID"": ""NS-108"", ""AccessType"": ""OPEN VEHICLE RAMP - PASS"", ""GeneralLoc"": ""800 BLK N ATLANTIC AV"", ""MilePost"": 2.19, ""City"": ""NEW SMYRNA BEACH"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694600478000, ""DrivingZone"": ""YES""}{""OBJECTID"": 140, ""AccessName"": ""FLAGLER AV"", ""AccessID"": ""NS-110"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""500 BLK FLAGLER AV"", ""MilePost"": 2.57, ""City"": ""NEW SMYRNA BEACH"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694600478000, ""DrivingZone"": ""YES""}{""OBJECTID"": 144, ""AccessName"": ""CARDINAL DR"", ""AccessID"": ""OB-036"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""600 BLK S ATLANTIC AV"", ""MilePost"": 11.27, ""City"": ""ORMOND BEACH"", ""AccessStatus"": ""4X4 ONLY"", ""Entry_Date_Time"": 1694595749000, ""DrivingZone"": ""YES""}{""OBJECTID"": 174, ""AccessName"": ""EL PORTAL ST"", ""AccessID"": ""DBS-076"", ""AccessType"": ""OPEN VEHICLE RAMP"", ""GeneralLoc"": ""3200 BLK S ATLANTIC AV"", ""MilePost"": 20.04, ""City"": ""DAYTONA BEACH SHORES"", ""AccessStatus"": ""OPEN"", ""Entry_Date_Time"": 1694601124000, ""DrivingZone"": ""YES""}",en,
https://python.langchain.com/docs/integrations/document_loaders/google_alloydb,Google AlloyDB for PostgreSQL | ü¶úÔ∏èüîó Langchain,AlloyDB is a fully managed,"Google AlloyDB for PostgreSQLAlloyDB is a fully managed\nrelational database service that offers high performance, seamless\nintegration, and impressive scalability. AlloyDB is 100% compatible\nwith PostgreSQL. Extend your database application to build AI-powered\nexperiences leveraging AlloyDB‚Äôs Langchain integrations.This notebook goes over how to use AlloyDB for PostgreSQL to load\nDocuments with the AlloyDBLoader class.Learn more about the package on\nGitHub.Open In ColabBefore you begin‚ÄãTo run this notebook, you will need to do the following:Create a Google Cloud\nProjectEnable the AlloyDB\nAPICreate a AlloyDB cluster and\ninstance.Create a AlloyDB\ndatabase.Add a User to the\ndatabase.ü¶úüîó Library Installation‚ÄãInstall the integration library, langchain-google-alloydb-pg.%pip install --upgrade --quiet  langchain-google-alloydb-pgColab only: Uncomment the following cell to restart the kernel or\nuse the button to restart the kernel. For Vertex AI Workbench you can\nrestart the terminal using the button on top.# # Automatically restart kernel after installs so that your environment can access the new packages# import IPython# app = IPython.Application.instance()# app.kernel.do_shutdown(True)üîê Authentication‚ÄãAuthenticate to Google Cloud as the IAM user logged into this notebook\nin order to access your Google Cloud Project.If you are using Colab to run this notebook, use the cell below and\ncontinue.If you are using Vertex AI Workbench, check out the setup\ninstructions\nhere.from google.colab import authauth.authenticate_user()‚òÅ Set Your Google Cloud Project‚ÄãSet your Google Cloud project so that you can leverage Google Cloud\nresources within this notebook.If you don‚Äôt know your project ID, try the following:Run gcloud config list.Run gcloud projects list.See the support page: Locate the project\nID.# @title Project { display-mode: ""form"" }PROJECT_ID = ""gcp_project_id""  # @param {type:""string""}# Set the project id! gcloud config set project {PROJECT_ID}Basic Usage‚ÄãSet AlloyDB database variables‚ÄãFind your database values, in the AlloyDB Instances\npage.# @title Set Your Values Here { display-mode: ""form"" }REGION = ""us-central1""  # @param {type: ""string""}CLUSTER = ""my-cluster""  # @param {type: ""string""}INSTANCE = ""my-primary""  # @param {type: ""string""}DATABASE = ""my-database""  # @param {type: ""string""}TABLE_NAME = ""vector_store""  # @param {type: ""string""}AlloyDBEngine Connection Pool‚ÄãOne of the requirements and arguments to establish AlloyDB as a vector\nstore is a AlloyDBEngine object. The AlloyDBEngine configures a\nconnection pool to your AlloyDB database, enabling successful\nconnections from your application and following industry best practices.To create a AlloyDBEngine using AlloyDBEngine.from_instance() you\nneed to provide only 5 things:project_id : Project ID of the Google Cloud Project where the\nAlloyDB instance is located.region : Region where the AlloyDB instance is located.cluster: The name of the AlloyDB cluster.instance : The name of the AlloyDB instance.database : The name of the database to connect to on the AlloyDB\ninstance.By default, IAM database\nauthentication will\nbe used as the method of database authentication. This library uses the\nIAM principal belonging to the Application Default Credentials\n(ADC)\nsourced from the environment.Optionally, built-in database\nauthentication\nusing a username and password to access the AlloyDB database can also be\nused. Just provide the optional user and password arguments to\nAlloyDBEngine.from_instance():user : Database user to use for built-in database authentication\nand loginpassword : Database password to use for built-in database\nauthentication and login.Note: This tutorial demonstrates the async interface. All async\nmethods have corresponding sync methods.from langchain_google_alloydb_pg import AlloyDBEngineengine = await AlloyDBEngine.afrom_instance(    project_id=PROJECT_ID,    region=REGION,    cluster=CLUSTER,    instance=INSTANCE,    database=DATABASE,)Create AlloyDBLoader‚Äãfrom langchain_google_alloydb_pg import AlloyDBLoader# Creating a basic AlloyDBLoader objectloader = await AlloyDBLoader.create(engine, table_name=TABLE_NAME)Load Documents via default table‚ÄãThe loader returns a list of Documents from the table using the first\ncolumn as page_content and all other columns as metadata. The default\ntable will have the first column as page_content and the second column\nas metadata (JSON). Each row becomes a document.docs = await loader.aload()print(docs)Load documents via custom table/metadata or custom page content columns‚Äãloader = await AlloyDBLoader.create(    engine,    table_name=TABLE_NAME,    content_columns=[""product_name""],  # Optional    metadata_columns=[""id""],  # Optional)docs = await loader.aload()print(docs)Set page content format‚ÄãThe loader returns a list of Documents, with one document per row, with\npage content in specified string format, i.e.¬†text (space separated\nconcatenation), JSON, YAML, CSV, etc. JSON and YAML formats include\nheaders, while text and CSV do not include field headers.loader = AlloyDBLoader.create(    engine,    table_name=""products"",    content_columns=[""product_name"", ""description""],    format=""YAML"",)docs = await loader.aload()print(docs)",en,
https://python.langchain.com/docs/integrations/document_loaders/aws_s3_directory,AWS S3 Directory | ü¶úÔ∏èüîó Langchain,[Amazon Simple Storage Service (Amazon,"AWS S3 DirectoryAmazon Simple Storage Service (Amazon\nS3)\nis an object storage serviceAWS S3\nDirectoryThis covers how to load document objects from an AWS S3 Directory\nobject.%pip install --upgrade --quiet  boto3from langchain_community.document_loaders import S3DirectoryLoaderloader = S3DirectoryLoader(""testing-hwc"")loader.load()Specifying a prefix‚ÄãYou can also specify a prefix for more finegrained control over what\nfiles to load.loader = S3DirectoryLoader(""testing-hwc"", prefix=""fake"")loader.load()[Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': 's3://testing-hwc/fake.docx'}, lookup_index=0)]Configuring the AWS Boto3 client‚ÄãYou can configure the AWS\nBoto3\nclient by passing named arguments when creating the S3DirectoryLoader.\nThis is useful for instance when AWS credentials can‚Äôt be set as\nenvironment variables. See the list of\nparameters\nthat can be configured.loader = S3DirectoryLoader(    ""testing-hwc"", aws_access_key_id=""xxxx"", aws_secret_access_key=""yyyy"")loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/email,Email | ü¶úÔ∏èüîó Langchain,This notebook shows how to load email (.eml) or Microsoft Outlook,"EmailThis notebook shows how to load email (.eml) or Microsoft Outlook\n(.msg) files.Using Unstructured‚Äã%pip install --upgrade --quiet  unstructuredfrom langchain_community.document_loaders import UnstructuredEmailLoaderloader = UnstructuredEmailLoader(""example_data/fake-email.eml"")data = loader.load()data[Document(page_content='This is a test email to use for unit tests.\n\nImportant points:\n\nRoses are red\n\nViolets are blue', metadata={'source': 'example_data/fake-email.eml'})]Retain Elements‚ÄãUnder the hood, Unstructured creates different ‚Äúelements‚Äù for different\nchunks of text. By default we combine those together, but you can easily\nkeep that separation by specifying mode=""elements"".loader = UnstructuredEmailLoader(""example_data/fake-email.eml"", mode=""elements"")data = loader.load()data[0]Document(page_content='This is a test email to use for unit tests.', metadata={'source': 'example_data/fake-email.eml', 'filename': 'fake-email.eml', 'file_directory': 'example_data', 'date': '2022-12-16T17:04:16-05:00', 'filetype': 'message/rfc822', 'sent_from': ['Matthew Robinson <mrobinson@unstructured.io>'], 'sent_to': ['Matthew Robinson <mrobinson@unstructured.io>'], 'subject': 'Test Email', 'category': 'NarrativeText'})Processing Attachments‚ÄãYou can process attachments with UnstructuredEmailLoader by setting\nprocess_attachments=True in the constructor. By default, attachments\nwill be partitioned using the partition function from unstructured.\nYou can use a different partitioning function by passing the function to\nthe attachment_partitioner kwarg.loader = UnstructuredEmailLoader(    ""example_data/fake-email.eml"",    mode=""elements"",    process_attachments=True,)data = loader.load()data[0]Document(page_content='This is a test email to use for unit tests.', metadata={'source': 'example_data/fake-email.eml', 'filename': 'fake-email.eml', 'file_directory': 'example_data', 'date': '2022-12-16T17:04:16-05:00', 'filetype': 'message/rfc822', 'sent_from': ['Matthew Robinson <mrobinson@unstructured.io>'], 'sent_to': ['Matthew Robinson <mrobinson@unstructured.io>'], 'subject': 'Test Email', 'category': 'NarrativeText'})Using OutlookMessageLoader‚Äã%pip install --upgrade --quiet  extract_msgfrom langchain_community.document_loaders import OutlookMessageLoaderloader = OutlookMessageLoader(""example_data/fake-email.msg"")data = loader.load()data[0]Document(page_content='This is a test email to experiment with the MS Outlook MSG Extractor\r\n\r\n\r\n-- \r\n\r\n\r\nKind regards\r\n\r\n\r\n\r\n\r\nBrian Zhou\r\n\r\n', metadata={'subject': 'Test for TIF files', 'sender': 'Brian Zhou <brizhou@gmail.com>', 'date': 'Mon, 18 Nov 2013 16:26:24 +0800'})",en,
https://python.langchain.com/docs/integrations/document_loaders/snowflake,Snowflake | ü¶úÔ∏èüîó Langchain,This notebooks goes over how to load documents from Snowflake,"SnowflakeThis notebooks goes over how to load documents from Snowflake%pip install --upgrade --quiet  snowflake-connector-pythonimport settings as sfrom langchain_community.document_loaders import SnowflakeLoaderQUERY = ""select text, survey_id from CLOUD_DATA_SOLUTIONS.HAPPY_OR_NOT.OPEN_FEEDBACK limit 10""snowflake_loader = SnowflakeLoader(    query=QUERY,    user=s.SNOWFLAKE_USER,    password=s.SNOWFLAKE_PASS,    account=s.SNOWFLAKE_ACCOUNT,    warehouse=s.SNOWFLAKE_WAREHOUSE,    role=s.SNOWFLAKE_ROLE,    database=s.SNOWFLAKE_DATABASE,    schema=s.SNOWFLAKE_SCHEMA,)snowflake_documents = snowflake_loader.load()print(snowflake_documents)import settings as sfrom snowflakeLoader import SnowflakeLoaderQUERY = ""select text, survey_id as source from CLOUD_DATA_SOLUTIONS.HAPPY_OR_NOT.OPEN_FEEDBACK limit 10""snowflake_loader = SnowflakeLoader(    query=QUERY,    user=s.SNOWFLAKE_USER,    password=s.SNOWFLAKE_PASS,    account=s.SNOWFLAKE_ACCOUNT,    warehouse=s.SNOWFLAKE_WAREHOUSE,    role=s.SNOWFLAKE_ROLE,    database=s.SNOWFLAKE_DATABASE,    schema=s.SNOWFLAKE_SCHEMA,    metadata_columns=[""source""],)snowflake_documents = snowflake_loader.load()print(snowflake_documents)",en,
https://python.langchain.com/docs/integrations/document_loaders/whatsapp_chat,WhatsApp Chat | ü¶úÔ∏èüîó Langchain,WhatsApp (also called,"WhatsApp ChatWhatsApp (also called\nWhatsApp Messenger) is a freeware, cross-platform, centralized\ninstant messaging (IM) and voice-over-IP (VoIP) service. It allows\nusers to send text and voice messages, make voice and video calls, and\nshare images, documents, user locations, and other content.This notebook covers how to load data from the WhatsApp Chats into a\nformat that can be ingested into LangChain.from langchain_community.document_loaders import WhatsAppChatLoaderloader = WhatsAppChatLoader(""example_data/whatsapp_chat.txt"")loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/mongodb,MongoDB | ü¶úÔ∏èüîó Langchain,"MongoDB is a NoSQL , document-oriented","MongoDBMongoDB is a NoSQL , document-oriented\ndatabase that supports JSON-like documents with a dynamic schema.Overview‚ÄãThe MongoDB Document Loader returns a list of Langchain Documents from a\nMongoDB database.The Loader requires the following parameters:MongoDB connection stringMongoDB database nameMongoDB collection name(Optional) Content Filter dictionary(Optional) List of field names to include in the outputThe output takes the following format:pageContent= Mongo Documentmetadata={‚Äòdatabase‚Äô: ‚Äò[database_name]‚Äô, ‚Äòcollection‚Äô:\n‚Äò[collection_name]‚Äô}Load the Document Loader‚Äã# add this import for running in jupyter notebookimport nest_asyncionest_asyncio.apply()from langchain_community.document_loaders.mongodb import MongodbLoaderloader = MongodbLoader(    connection_string=""mongodb://localhost:27017/"",    db_name=""sample_restaurants"",    collection_name=""restaurants"",    filter_criteria={""borough"": ""Bronx"", ""cuisine"": ""Bakery""},    field_names=[""name"", ""address""],)docs = loader.load()len(docs)71docs[0]Document(page_content=""Morris Park Bake Shop {'building': '1007', 'coord': [-73.856077, 40.848447], 'street': 'Morris Park Ave', 'zipcode': '10462'}"", metadata={'database': 'sample_restaurants', 'collection': 'restaurants'})",en,
https://python.langchain.com/docs/integrations/document_loaders/sitemap,Sitemap | ü¶úÔ∏èüîó Langchain,"Extends from the WebBaseLoader, SitemapLoader loads a sitemap from a","SitemapExtends from the WebBaseLoader, SitemapLoader loads a sitemap from a\ngiven URL, and then scrape and load all pages in the sitemap, returning\neach page as a Document.The scraping is done concurrently. There are reasonable limits to\nconcurrent requests, defaulting to 2 per second. If you aren‚Äôt concerned\nabout being a good citizen, or you control the scrapped server, or don‚Äôt\ncare about load. Note, while this will speed up the scraping process,\nbut it may cause the server to block you. Be careful!%pip install --upgrade --quiet  nest_asyncio# fixes a bug with asyncio and jupyterimport nest_asyncionest_asyncio.apply()from langchain_community.document_loaders.sitemap import SitemapLoadersitemap_loader = SitemapLoader(web_path=""https://api.python.langchain.com/sitemap.xml"")docs = sitemap_loader.load()You can change the requests_per_second parameter to increase the max\nconcurrent requests. and use requests_kwargs to pass kwargs when send\nrequests.sitemap_loader.requests_per_second = 2# Optional: avoid `[SSL: CERTIFICATE_VERIFY_FAILED]` issuesitemap_loader.requests_kwargs = {""verify"": False}docs[0]Document(page_content='\n\n\n\n\n\n\n\n\n\nLangChain Python API Reference Documentation.\n\n\nYou will be automatically redirected to the new location of this page.\n\n', metadata={'source': 'https://api.python.langchain.com/en/stable/', 'loc': 'https://api.python.langchain.com/en/stable/', 'lastmod': '2024-02-09T01:10:49.422114+00:00', 'changefreq': 'weekly', 'priority': '1'})Filtering sitemap URLs‚ÄãSitemaps can be massive files, with thousands of URLs. Often you don‚Äôt\nneed every single one of them. You can filter the URLs by passing a list\nof strings or regex patterns to the filter_urls parameter. Only URLs\nthat match one of the patterns will be loaded.loader = SitemapLoader(    web_path="" https://api.python.langchain.com/sitemap.xml"",    filter_urls=[""https://api.python.langchain.com/en/latest""],)documents = loader.load()documents[0]Document(page_content='\n\n\n\n\n\n\n\n\n\nLangChain Python API Reference Documentation.\n\n\nYou will be automatically redirected to the new location of this page.\n\n', metadata={'source': 'https://api.python.langchain.com/en/latest/', 'loc': 'https://api.python.langchain.com/en/latest/', 'lastmod': '2024-02-12T05:26:10.971077+00:00', 'changefreq': 'daily', 'priority': '0.9'})Add custom scraping rules‚ÄãThe SitemapLoader uses beautifulsoup4 for the scraping process, and\nit scrapes every element on the page by default. The SitemapLoader\nconstructor accepts a custom scraping function. This feature can be\nhelpful to tailor the scraping process to your specific needs; for\nexample, you might want to avoid scraping headers or navigation\nelements.The following example shows how to develop and use a custom function to\navoid navigation and header elements.Import the beautifulsoup4 library and define the custom function.pip install beautifulsoup4from bs4 import BeautifulSoupdef remove_nav_and_header_elements(content: BeautifulSoup) -> str:    # Find all 'nav' and 'header' elements in the BeautifulSoup object    nav_elements = content.find_all(""nav"")    header_elements = content.find_all(""header"")    # Remove each 'nav' and 'header' element from the BeautifulSoup object    for element in nav_elements + header_elements:        element.decompose()    return str(content.get_text())Add your custom function to the SitemapLoader object.loader = SitemapLoader(    ""https://api.python.langchain.com/sitemap.xml"",    filter_urls=[""https://api.python.langchain.com/en/latest/""],    parsing_function=remove_nav_and_header_elements,)Local Sitemap‚ÄãThe sitemap loader can also be used to load local files.sitemap_loader = SitemapLoader(web_path=""example_data/sitemap.xml"", is_local=True)docs = sitemap_loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/copypaste,Copy Paste | ü¶úÔ∏èüîó Langchain,This notebook covers how to load a document object from something you,"Copy PasteThis notebook covers how to load a document object from something you\njust want to copy and paste. In this case, you don‚Äôt even need to use a\nDocumentLoader, but rather can just construct the Document directly.from langchain.docstore.document import Documenttext = ""..... put the text you copy pasted here......""doc = Document(page_content=text)Metadata‚ÄãIf you want to add metadata about the where you got this piece of text,\nyou easily can with the metadata key.metadata = {""source"": ""internet"", ""date"": ""Friday""}doc = Document(page_content=text, metadata=metadata)",en,
https://python.langchain.com/docs/integrations/document_loaders/azure_blob_storage_container,Azure Blob Storage Container | ü¶úÔ∏èüîó Langchain,[Azure Blob,"Azure Blob Storage ContainerAzure Blob\nStorage\nis Microsoft‚Äôs object storage solution for the cloud. Blob Storage is\noptimized for storing massive amounts of unstructured data.\nUnstructured data is data that doesn‚Äôt adhere to a particular data\nmodel or definition, such as text or binary data.Azure Blob Storage is designed for: - Serving images or documents\ndirectly to a browser. - Storing files for distributed access. -\nStreaming video and audio. - Writing to log files. - Storing data for\nbackup and restore, disaster recovery, and archiving. - Storing data for\nanalysis by an on-premises or Azure-hosted service.This notebook covers how to load document objects from a container on\nAzure Blob Storage.%pip install --upgrade --quiet  azure-storage-blobfrom langchain_community.document_loaders import AzureBlobStorageContainerLoaderloader = AzureBlobStorageContainerLoader(conn_str=""<conn_str>"", container=""<container>"")loader.load()[Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmpaa9xl6ch/fake.docx'}, lookup_index=0)]Specifying a prefix‚ÄãYou can also specify a prefix for more finegrained control over what\nfiles to load.loader = AzureBlobStorageContainerLoader(    conn_str=""<conn_str>"", container=""<container>"", prefix=""<prefix>"")loader.load()[Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmpujbkzf_l/fake.docx'}, lookup_index=0)]",en,
https://python.langchain.com/docs/integrations/document_loaders/org_mode,Org-mode | ü¶úÔ∏èüîó Langchain,A Org Mode document is a,"Org-modeA Org Mode document is a\ndocument editing, formatting, and organizing mode, designed for notes,\nplanning, and authoring within the free software text editor Emacs.UnstructuredOrgModeLoader‚ÄãYou can load data from Org-mode files with UnstructuredOrgModeLoader\nusing the following workflow.from langchain_community.document_loaders import UnstructuredOrgModeLoaderloader = UnstructuredOrgModeLoader(file_path=""example_data/README.org"", mode=""elements"")docs = loader.load()print(docs[0])page_content='Example Docs' metadata={'source': 'example_data/README.org', 'filename': 'README.org', 'file_directory': 'example_data', 'filetype': 'text/org', 'page_number': 1, 'category': 'Title'}",en,
https://python.langchain.com/docs/integrations/document_loaders/weather,Weather | ü¶úÔ∏èüîó Langchain,OpenWeatherMap is an open-source,"WeatherOpenWeatherMap is an open-source\nweather service providerThis loader fetches the weather data from the OpenWeatherMap‚Äôs OneCall\nAPI, using the pyowm Python package. You must initialize the loader with\nyour OpenWeatherMap API token and the names of the cities you want the\nweather data for.from langchain_community.document_loaders import WeatherDataLoader%pip install --upgrade --quiet  pyowm# Set API key either by passing it in to constructor directly# or by setting the environment variable ""OPENWEATHERMAP_API_KEY"".from getpass import getpassOPENWEATHERMAP_API_KEY = getpass()loader = WeatherDataLoader.from_params(    [""chennai"", ""vellore""], openweathermap_api_key=OPENWEATHERMAP_API_KEY)documents = loader.load()documents",en,
https://python.langchain.com/docs/integrations/document_loaders/google_cloud_sql_pg,Google Cloud SQL for PostgreSQL | ü¶úÔ∏èüîó Langchain,Cloud SQL for PostgreSQL,"Google Cloud SQL for PostgreSQLCloud SQL for PostgreSQL\nis a fully-managed database service that helps you set up, maintain,\nmanage, and administer your PostgreSQL relational databases on Google\nCloud Platform. Extend your database application to build AI-powered\nexperiences leveraging Cloud SQL for PostgreSQL‚Äôs Langchain\nintegrations.This notebook goes over how to use Cloud SQL for PostgreSQL to load\nDocuments with the PostgresLoader class.Learn more about the package on\nGitHub.Open In ColabBefore you begin‚ÄãTo run this notebook, you will need to do the following:Create a Google Cloud\nProjectEnable the Cloud SQL Admin\nAPI.Create a Cloud SQL for PostgreSQL\ninstance.Create a Cloud SQL for PostgreSQL\ndatabase.Add a User to the\ndatabase.ü¶úüîó Library Installation‚ÄãInstall the integration library, langchain_google_cloud_sql_pg.%pip install --upgrade --quiet  langchain_google_cloud_sql_pgColab only: Uncomment the following cell to restart the kernel or\nuse the button to restart the kernel. For Vertex AI Workbench you can\nrestart the terminal using the button on top.# # Automatically restart kernel after installs so that your environment can access the new packages# import IPython# app = IPython.Application.instance()# app.kernel.do_shutdown(True)üîê Authentication‚ÄãAuthenticate to Google Cloud as the IAM user logged into this notebook\nin order to access your Google Cloud Project.If you are using Colab to run this notebook, use the cell below and\ncontinue.If you are using Vertex AI Workbench, check out the setup\ninstructions\nhere.from google.colab import authauth.authenticate_user()‚òÅ Set Your Google Cloud Project‚ÄãSet your Google Cloud project so that you can leverage Google Cloud\nresources within this notebook.If you don‚Äôt know your project ID, try the following:Run gcloud config list.Run gcloud projects list.See the support page: Locate the project\nID.# @title Project { display-mode: ""form"" }PROJECT_ID = ""gcp_project_id""  # @param {type:""string""}# Set the project id! gcloud config set project {PROJECT_ID}Basic Usage‚ÄãSet Cloud SQL database values‚ÄãFind your database variables, in the Cloud SQL Instances\npage.# @title Set Your Values Here { display-mode: ""form"" }REGION = ""us-central1""  # @param {type: ""string""}INSTANCE = ""my-primary""  # @param {type: ""string""}DATABASE = ""my-database""  # @param {type: ""string""}TABLE_NAME = ""vector_store""  # @param {type: ""string""}Cloud SQL Engine‚ÄãOne of the requirements and arguments to establish PostgreSQL as a\ndocument loader is a PostgresEngine object. The PostgresEngine\nconfigures a connection pool to your Cloud SQL for PostgreSQL database,\nenabling successful connections from your application and following\nindustry best practices.To create a PostgresEngine using PostgresEngine.from_instance() you\nneed to provide only 4 things:project_id : Project ID of the Google Cloud Project where the\nCloud SQL instance is located.region : Region where the Cloud SQL instance is located.instance : The name of the Cloud SQL instance.database : The name of the database to connect to on the Cloud SQL\ninstance.By default, IAM database\nauthentication\nwill be used as the method of database authentication. This library uses\nthe IAM principal belonging to the Application Default Credentials\n(ADC)\nsourced from the environment.Optionally, built-in database\nauthentication using\na username and password to access the Cloud SQL database can also be\nused. Just provide the optional user and password arguments to\nPostgresEngine.from_instance():user : Database user to use for built-in database authentication\nand loginpassword : Database password to use for built-in database\nauthentication and login.Note: This tutorial demonstrates the async interface. All async\nmethods have corresponding sync methods.from langchain_google_cloud_sql_pg import PostgresEngineengine = await PostgresEngine.afrom_instance(    project_id=PROJECT_ID,    region=REGION,    instance=INSTANCE,    database=DATABASE,)Create PostgresLoader‚Äãfrom langchain_google_cloud_sql_pg import PostgresLoader# Creating a basic PostgreSQL objectloader = await PostgresLoader.create(engine, table_name=TABLE_NAME)Load Documents via default table‚ÄãThe loader returns a list of Documents from the table using the first\ncolumn as page_content and all other columns as metadata. The default\ntable will have the first column as page_content and the second column\nas metadata (JSON). Each row becomes a document. Please note that if you\nwant your documents to have ids you will need to add them in.from langchain_google_cloud_sql_pg import PostgresLoader# Creating a basic PostgresLoader objectloader = await PostgresLoader.create(engine, table_name=TABLE_NAME)docs = await loader.aload()print(docs)Load documents via custom table/metadata or custom page content columns‚Äãloader = await PostgresLoader.create(    engine,    table_name=TABLE_NAME,    content_columns=[""product_name""],  # Optional    metadata_columns=[""id""],  # Optional)docs = await loader.aload()print(docs)Set page content format‚ÄãThe loader returns a list of Documents, with one document per row, with\npage content in specified string format, i.e.¬†text (space separated\nconcatenation), JSON, YAML, CSV, etc. JSON and YAML formats include\nheaders, while text and CSV do not include field headers.loader = await PostgresLoader.create(    engine,    table_name=""products"",    content_columns=[""product_name"", ""description""],    format=""YAML"",)docs = await loader.aload()print(docs)",en,
https://python.langchain.com/docs/integrations/document_loaders/airbyte_zendesk_support,Airbyte Zendesk Support (Deprecated) | ü¶úÔ∏èüîó Langchain,Note: This connector-specific loader is deprecated. Please use,"Airbyte Zendesk Support (Deprecated)Note: This connector-specific loader is deprecated. Please use\nAirbyteLoader instead.Airbyte is a data integration\nplatform for ELT pipelines from APIs, databases & files to warehouses\n& lakes. It has the largest catalog of ELT connectors to data\nwarehouses and databases.This loader exposes the Zendesk Support connector as a document loader,\nallowing you to load various objects as documents.Installation‚ÄãFirst, you need to install the airbyte-source-zendesk-support python\npackage.%pip install --upgrade --quiet  airbyte-source-zendesk-supportExample‚ÄãCheck out the Airbyte documentation\npage\nfor details about how to configure the reader. The JSON schema the\nconfig object should adhere to can be found on Github:\nhttps://github.com/airbytehq/airbyte/blob/master/airbyte-integrations/connectors/source-zendesk-support/source_zendesk_support/spec.json.The general shape looks like this:{  ""subdomain"": ""<your zendesk subdomain>"",  ""start_date"": ""<date from which to start retrieving records from in ISO format, e.g. 2020-10-20T00:00:00Z>"",  ""credentials"": {    ""credentials"": ""api_token"",    ""email"": ""<your email>"",    ""api_token"": ""<your api token>""  }}By default all fields are stored as metadata in the documents and the\ntext is set to an empty string. Construct the text of the document by\ntransforming the documents returned by the reader.from langchain_community.document_loaders.airbyte import AirbyteZendeskSupportLoaderconfig = {    # your zendesk-support configuration}loader = AirbyteZendeskSupportLoader(    config=config, stream_name=""tickets"")  # check the documentation linked above for a list of all streamsNow you can load documents the usual waydocs = loader.load()As load returns a list, it will block until all documents are loaded.\nTo have better control over this process, you can also you the\nlazy_load method which returns an iterator instead:docs_iterator = loader.lazy_load()Keep in mind that by default the page content is empty and the metadata\nobject contains all the information from the record. To create documents\nin a different, pass in a record_handler function when creating the\nloader:from langchain.docstore.document import Documentdef handle_record(record, id):    return Document(page_content=record.data[""title""], metadata=record.data)loader = AirbyteZendeskSupportLoader(    config=config, record_handler=handle_record, stream_name=""tickets"")docs = loader.load()Incremental loads‚ÄãSome streams allow incremental loading, this means the source keeps\ntrack of synced records and won‚Äôt load them again. This is useful for\nsources that have a high volume of data and are updated frequently.To take advantage of this, store the last_state property of the loader\nand pass it in when creating the loader again. This will ensure that\nonly new records are loaded.last_state = loader.last_state  # store safelyincremental_loader = AirbyteZendeskSupportLoader(    config=config, stream_name=""tickets"", state=last_state)new_docs = incremental_loader.load()",en,
https://python.langchain.com/docs/integrations/document_loaders/google_datastore,Google Firestore in Datastore Mode | ü¶úÔ∏èüîó Langchain,Firestore in Datastore Mode is a,"Google Firestore in Datastore ModeFirestore in Datastore Mode is a\nNoSQL document database built for automatic scaling, high performance\nand ease of application development. Extend your database application\nto build AI-powered experiences leveraging Datastore‚Äôs Langchain\nintegrations.This notebook goes over how to use Firestore in Datastore\nMode to save, load and delete\nlangchain documents\nwith DatastoreLoader and DatastoreSaver.Learn more about the package on\nGitHub.Open In ColabBefore You Begin‚ÄãTo run this notebook, you will need to do the following:Create a Google Cloud\nProjectEnable the Datastore\nAPICreate a Firestore in Datastore Mode\ndatabaseAfter confirmed access to database in the runtime environment of this\nnotebook, filling the following values and run the cell before running\nexample scripts.ü¶úüîó Library Installation‚ÄãThe integration lives in its own langchain-google-datastore package,\nso we need to install it.%pip install -upgrade --quiet langchain-google-datastoreColab only: Uncomment the following cell to restart the kernel or\nuse the button to restart the kernel. For Vertex AI Workbench you can\nrestart the terminal using the button on top.# # Automatically restart kernel after installs so that your environment can access the new packages# import IPython# app = IPython.Application.instance()# app.kernel.do_shutdown(True)‚òÅ Set Your Google Cloud Project‚ÄãSet your Google Cloud project so that you can leverage Google Cloud\nresources within this notebook.If you don‚Äôt know your project ID, try the following:Run gcloud config list.Run gcloud projects list.See the support page: Locate the project\nID.# @markdown Please fill in the value below with your Google Cloud project ID and then run the cell.PROJECT_ID = ""my-project-id""  # @param {type:""string""}# Set the project id!gcloud config set project {PROJECT_ID}üîê Authentication‚ÄãAuthenticate to Google Cloud as the IAM user logged into this notebook\nin order to access your Google Cloud Project.If you are using Colab to run this notebook, use the cell below and\ncontinue.If you are using Vertex AI Workbench, check out the setup\ninstructions\nhere.from google.colab import authauth.authenticate_user()Basic Usage‚ÄãSave documents‚ÄãSave langchain documents with\nDatastoreSaver.upsert_documents(<documents>). By default it will try\nto extract the entity key from the key in the Document metadata.from langchain_core.documents import Documentfrom langchain_google_datastore import DatastoreSaversaver = DatastoreSaver()data = [Document(page_content=""Hello, World!"")]saver.upsert_documents(data)Save documents without key‚ÄãIf a kind is specified the documents will be stored with an auto\ngenerated id.saver = DatastoreSaver(""MyKind"")saver.upsert_documents(data)Load documents via Kind‚ÄãLoad langchain documents with DatastoreLoader.load() or\nDatastoreLoader.lazy_load(). lazy_load returns a generator that only\nqueries database during the iteration. To initialize DatastoreLoader\nclass you need to provide: 1. source - The source to load the\ndocuments. It can be an instance of Query or the name of the Datastore\nkind to read from.from langchain_google_datastore import DatastoreLoaderloader = DatastoreLoader(""MyKind"")data = loader.load()Load documents via query‚ÄãOther than loading documents from kind, we can also choose to load\ndocuments from query. For example:from google.cloud import datastoreclient = datastore.Client(database=""non-default-db"", namespace=""custom_namespace"")query_load = client.query(kind=""MyKind"")query_load.add_filter(""region"", ""="", ""west_coast"")loader_document = DatastoreLoader(query_load)data = loader_document.load()Delete documents‚ÄãDelete a list of langchain documents from Datastore with\nDatastoreSaver.delete_documents(<documents>).saver = DatastoreSaver()saver.delete_documents(data)keys_to_delete = [    [""Kind1"", ""identifier""],    [""Kind2"", 123],    [""Kind3"", ""identifier"", ""NestedKind"", 456],]# The Documents will be ignored and only the document ids will be used.saver.delete_documents(data, keys_to_delete)Advanced Usage‚ÄãLoad documents with customized document page content & metadata‚ÄãThe arguments of page_content_properties and metadata_properties\nwill specify the Entity properties to be written into LangChain Document\npage_content and metadata.loader = DatastoreLoader(    source=""MyKind"",    page_content_fields=[""data_field""],    metadata_fields=[""metadata_field""],)data = loader.load()Customize Page Content Format‚ÄãWhen the page_content contains only one field the information will be\nthe field value only. Otherwise the page_content will be in JSON\nformat.Customize Connection & Authentication‚Äãfrom google.auth import compute_enginefrom google.cloud.firestore import Clientclient = Client(database=""non-default-db"", creds=compute_engine.Credentials())loader = DatastoreLoader(    source=""foo"",    client=client,)",en,
https://python.langchain.com/docs/integrations/document_loaders/image_captions,Image captions | ü¶úÔ∏èüîó Langchain,"By default, the loader utilizes the pre-trained [Salesforce BLIP image","Image captionsBy default, the loader utilizes the pre-trained Salesforce BLIP image\ncaptioning\nmodel.This notebook shows how to use the ImageCaptionLoader to generate a\nquery-able index of image captions%pip install --upgrade --quiet  transformersfrom langchain_community.document_loaders import ImageCaptionLoaderPrepare a list of image urls from Wikimedia‚Äãlist_image_urls = [    ""https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/Hyla_japonica_sep01.jpg/260px-Hyla_japonica_sep01.jpg"",    ""https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Tibur%C3%B3n_azul_%28Prionace_glauca%29%2C_canal_Fayal-Pico%2C_islas_Azores%2C_Portugal%2C_2020-07-27%2C_DD_14.jpg/270px-Tibur%C3%B3n_azul_%28Prionace_glauca%29%2C_canal_Fayal-Pico%2C_islas_Azores%2C_Portugal%2C_2020-07-27%2C_DD_14.jpg"",    ""https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Thure_de_Thulstrup_-_Battle_of_Shiloh.jpg/251px-Thure_de_Thulstrup_-_Battle_of_Shiloh.jpg"",    ""https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Passion_fruits_-_whole_and_halved.jpg/270px-Passion_fruits_-_whole_and_halved.jpg"",    ""https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/Messier83_-_Heic1403a.jpg/277px-Messier83_-_Heic1403a.jpg"",    ""https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/2022-01-22_Men%27s_World_Cup_at_2021-22_St._Moritz%E2%80%93Celerina_Luge_World_Cup_and_European_Championships_by_Sandro_Halank%E2%80%93257.jpg/288px-2022-01-22_Men%27s_World_Cup_at_2021-22_St._Moritz%E2%80%93Celerina_Luge_World_Cup_and_European_Championships_by_Sandro_Halank%E2%80%93257.jpg"",    ""https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Wiesen_Pippau_%28Crepis_biennis%29-20220624-RM-123950.jpg/224px-Wiesen_Pippau_%28Crepis_biennis%29-20220624-RM-123950.jpg"",]Create the loader‚Äãloader = ImageCaptionLoader(path_images=list_image_urls)list_docs = loader.load()list_docsimport requestsfrom PIL import ImageImage.open(requests.get(list_image_urls[0], stream=True).raw).convert(""RGB"")Create the index‚Äãfrom langchain.indexes import VectorstoreIndexCreatorindex = VectorstoreIndexCreator().from_loaders([loader])Query‚Äãquery = ""What's the painting about?""index.query(query)query = ""What kind of images are there?""index.query(query)",en,
https://python.langchain.com/docs/integrations/document_loaders/youtube_audio,YouTube audio | ü¶úÔ∏èüîó Langchain,Building chat or QA applications on YouTube videos is a topic of high,"YouTube audioBuilding chat or QA applications on YouTube videos is a topic of high\ninterest.Below we show how to easily go from a YouTube url to\naudio of the video to text to chat!We wil use the OpenAIWhisperParser, which will use the OpenAI Whisper\nAPI to transcribe audio to text, and the OpenAIWhisperParserLocal for\nlocal support and running on private clouds or on premise.Note: You will need to have an OPENAI_API_KEY supplied.from langchain_community.document_loaders.blob_loaders.youtube_audio import (    YoutubeAudioLoader,)from langchain_community.document_loaders.generic import GenericLoaderfrom langchain_community.document_loaders.parsers import (    OpenAIWhisperParser,    OpenAIWhisperParserLocal,)We will use yt_dlp to download audio for YouTube urls.We will use pydub to split downloaded audio files (such that we adhere\nto Whisper API‚Äôs 25MB file size limit).%pip install --upgrade --quiet  yt_dlp%pip install --upgrade --quiet  pydub%pip install --upgrade --quiet  librosaYouTube url to text‚ÄãUse YoutubeAudioLoader to fetch / download the audio files.Then, ues OpenAIWhisperParser() to transcribe them to text.Let‚Äôs take the first lecture of Andrej Karpathy‚Äôs YouTube course as an\nexample!# set a flag to switch between local and remote parsing# change this to True if you want to use local parsinglocal = False# Two Karpathy lecture videosurls = [""https://youtu.be/kCc8FmEb1nY"", ""https://youtu.be/VMj-3S1tku0""]# Directory to save audio filessave_dir = ""~/Downloads/YouTube""# Transcribe the videos to textif local:    loader = GenericLoader(        YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParserLocal()    )else:    loader = GenericLoader(YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParser())docs = loader.load()[youtube] Extracting URL: https://youtu.be/kCc8FmEb1nY[youtube] kCc8FmEb1nY: Downloading webpage[youtube] kCc8FmEb1nY: Downloading android player API JSON[info] kCc8FmEb1nY: Downloading 1 format(s): 140[dashsegments] Total fragments: 11[download] Destination: /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let's build GPTÔºö from scratch, in code, spelled out..m4a[download] 100% of  107.73MiB in 00:00:18 at 5.92MiB/s                   [FixupM4a] Correcting container of ""/Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let's build GPTÔºö from scratch, in code, spelled out..m4a""[ExtractAudio] Not converting audio /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/Let's build GPTÔºö from scratch, in code, spelled out..m4a; file is already in target format m4a[youtube] Extracting URL: https://youtu.be/VMj-3S1tku0[youtube] VMj-3S1tku0: Downloading webpage[youtube] VMj-3S1tku0: Downloading android player API JSON[info] VMj-3S1tku0: Downloading 1 format(s): 140[download] /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/The spelled-out intro to neural networks and backpropagationÔºö building micrograd.m4a has already been downloaded[download] 100% of  134.98MiB[ExtractAudio] Not converting audio /Users/31treehaus/Desktop/AI/langchain-fork/docs/modules/indexes/document_loaders/examples/The spelled-out intro to neural networks and backpropagationÔºö building micrograd.m4a; file is already in target format m4a# Returns a list of Documents, which can be easily viewed or parseddocs[0].page_content[0:500]""Hello, my name is Andrej and I've been training deep neural networks for a bit more than a decade. And in this lecture I'd like to show you what neural network training looks like under the hood. So in particular we are going to start with a blank Jupyter notebook and by the end of this lecture we will define and train a neural net and you'll get to see everything that goes on under the hood and exactly sort of how that works on an intuitive level. Now specifically what I would like to do is I w""Building a chat app from YouTube video‚ÄãGiven Documents, we can easily enable chat / question+answering.from langchain.chains import RetrievalQAfrom langchain_community.vectorstores import FAISSfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitter# Combine doccombined_docs = [doc.page_content for doc in docs]text = "" "".join(combined_docs)# Split themtext_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)splits = text_splitter.split_text(text)# Build an indexembeddings = OpenAIEmbeddings()vectordb = FAISS.from_texts(splits, embeddings)# Build a QA chainqa_chain = RetrievalQA.from_chain_type(    llm=ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0),    chain_type=""stuff"",    retriever=vectordb.as_retriever(),)# Ask a question!query = ""Why do we need to zero out the gradient before backprop at each step?""qa_chain.run(query)""We need to zero out the gradient before backprop at each step because the backward pass accumulates gradients in the grad attribute of each parameter. If we don't reset the grad to zero before each backward pass, the gradients will accumulate and add up, leading to incorrect updates and slower convergence. By resetting the grad to zero before each backward pass, we ensure that the gradients are calculated correctly and that the optimization process works as intended.""query = ""What is the difference between an encoder and decoder?""qa_chain.run(query)'In the context of transformers, an encoder is a component that reads in a sequence of input tokens and generates a sequence of hidden representations. On the other hand, a decoder is a component that takes in a sequence of hidden representations and generates a sequence of output tokens. The main difference between the two is that the encoder is used to encode the input sequence into a fixed-length representation, while the decoder is used to decode the fixed-length representation into an output sequence. In machine translation, for example, the encoder reads in the source language sentence and generates a fixed-length representation, which is then used by the decoder to generate the target language sentence.'query = ""For any token, what are x, k, v, and q?""qa_chain.run(query)'For any token, x is the input vector that contains the private information of that token, k and q are the key and query vectors respectively, which are produced by forwarding linear modules on x, and v is the vector that is calculated by propagating the same linear module on x again. The key vector represents what the token contains, and the query vector represents what the token is looking for. The vector v is the information that the token will communicate to other tokens if it finds them interesting, and it gets aggregated for the purposes of the self-attention mechanism.'",en,
https://python.langchain.com/docs/integrations/document_loaders/dropbox,Dropbox | ü¶úÔ∏èüîó Langchain,Dropbox is a file hosting,"DropboxDropbox is a file hosting\nservice that brings everything-traditional files, cloud content, and web\nshortcuts together in one place.This notebook covers how to load documents from Dropbox. In addition\nto common files such as text and PDF files, it also supports Dropbox\nPaper files.Prerequisites‚ÄãCreate a Dropbox app.Give the app these scope permissions: files.metadata.read and\nfiles.content.read.Generate access token:\nhttps://www.dropbox.com/developers/apps/create.pip install dropbox (requires pip install ""unstructured[pdf]""\nfor PDF filetype).Instructions‚Äã`DropboxLoader`` requires you to create a Dropbox App and generate an\naccess token. This can be done from\nhttps://www.dropbox.com/developers/apps/create. You also need to have\nthe Dropbox Python SDK installed (pip install dropbox).DropboxLoader can load data from a list of Dropbox file paths or a\nsingle Dropbox folder path. Both paths should be relative to the root\ndirectory of the Dropbox account linked to the access token.pip install dropboxRequirement already satisfied: dropbox in /Users/rbarragan/.local/share/virtualenvs/langchain-kv0dsrF5/lib/python3.11/site-packages (11.36.2)Requirement already satisfied: requests>=2.16.2 in /Users/rbarragan/.local/share/virtualenvs/langchain-kv0dsrF5/lib/python3.11/site-packages (from dropbox) (2.31.0)Requirement already satisfied: six>=1.12.0 in /Users/rbarragan/.local/share/virtualenvs/langchain-kv0dsrF5/lib/python3.11/site-packages (from dropbox) (1.16.0)Requirement already satisfied: stone>=2 in /Users/rbarragan/.local/share/virtualenvs/langchain-kv0dsrF5/lib/python3.11/site-packages (from dropbox) (3.3.1)Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rbarragan/.local/share/virtualenvs/langchain-kv0dsrF5/lib/python3.11/site-packages (from requests>=2.16.2->dropbox) (3.2.0)Requirement already satisfied: idna<4,>=2.5 in /Users/rbarragan/.local/share/virtualenvs/langchain-kv0dsrF5/lib/python3.11/site-packages (from requests>=2.16.2->dropbox) (3.4)Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rbarragan/.local/share/virtualenvs/langchain-kv0dsrF5/lib/python3.11/site-packages (from requests>=2.16.2->dropbox) (2.0.4)Requirement already satisfied: certifi>=2017.4.17 in /Users/rbarragan/.local/share/virtualenvs/langchain-kv0dsrF5/lib/python3.11/site-packages (from requests>=2.16.2->dropbox) (2023.7.22)Requirement already satisfied: ply>=3.4 in /Users/rbarragan/.local/share/virtualenvs/langchain-kv0dsrF5/lib/python3.11/site-packages (from stone>=2->dropbox) (3.11)Note: you may need to restart the kernel to use updated packages.from langchain_community.document_loaders import DropboxLoader# Generate access token: https://www.dropbox.com/developers/apps/create.dropbox_access_token = ""<DROPBOX_ACCESS_TOKEN>""# Dropbox root folderdropbox_folder_path = """"loader = DropboxLoader(    dropbox_access_token=dropbox_access_token,    dropbox_folder_path=dropbox_folder_path,    recursive=False,)documents = loader.load()File /JHSfLKn0.jpeg could not be decoded as text. Skipping.File /A REPORT ON WILES‚Äô CAMBRIDGE LECTURES.pdf could not be decoded as text. Skipping.for document in documents:    print(document)page_content='# üéâ Getting Started with Dropbox Paper\nDropbox Paper is great for capturing ideas and gathering quick feedback from your team. You can use words, images, code, or media from other apps, or go ahead and connect your calendar and add to-dos for projects.\n\n*Explore and edit this doc to play with some of these features. This doc is all yours. No one will see your edits unless you share this doc.*\n\n\n# The basics\n\n**Selecting text** activates the formatting toolbar, where you can apply basic formatting, create lists, and add comments.\n\n[ ] Create to-do lists\n- Bulleted lists\n1. Numbered lists\n\n**Starting a new line** activates the insert toolbar, where you can add media from other apps, links to Dropbox files, photos, and more.\n\n![](https://paper-attachments.dropbox.com/s_72143DBFDAF4C9DE702BB246920BC47FE7E1FA76AC23CC699374430D94E96DD2_1523574441249_paper-insert.png)\n\n\n\n**Add emojis** to your doc or comment by typing `**:**` ****and choosing a character. \n\n# üëç üëé üëè ‚úÖ ‚ùå ‚ù§Ô∏è ‚≠ê üí° üìå\n\n\n# Images\n\n**Selecting images** activates the image toolbar, where you can align images left, center, right or expand them to full width.\n\n![](https://paper-attachments.dropbox.com/s_72143DBFDAF4C9DE702BB246920BC47FE7E1FA76AC23CC699374430D94E96DD2_1523473869783_Hot_Sauce.jpg)\n\n\nPaste images or gifs right next to each other and they\'ll organize automatically. Click on an image twice to start full-screen gallery view.\n\n\n![](https://paper-attachments.dropbox.com/s_72143DBFDAF4C9DE702BB246920BC47FE7E1FA76AC23CC699374430D94E96DD2_1523564536543_Clock_Melt.png)\n![](https://paper-attachments.dropbox.com/s_72143DBFDAF4C9DE702BB246920BC47FE7E1FA76AC23CC699374430D94E96DD2_1523564528339_Boom_Box_Melt.png)\n![](https://paper-attachments.dropbox.com/s_72143DBFDAF4C9DE702BB246920BC47FE7E1FA76AC23CC699374430D94E96DD2_1523564549819_Soccerball_Melt.png)\n\n![You can add captions too](https://paper-attachments.dropbox.com/s_72143DBFDAF4C9DE702BB246920BC47FE7E1FA76AC23CC699374430D94E96DD2_1523564518899_Cacti_Melt.png)\n![What a strange, melting toaster!](https://paper-attachments.dropbox.com/s_72143DBFDAF4C9DE702BB246920BC47FE7E1FA76AC23CC699374430D94E96DD2_1523564508553_Toaster_Melt.png)\n\n\n \n\n\n# Form meets function\n\nYou and your team can create the way you want, with what you want. Dropbox Paper adapts to the way your team captures ideas.\n\n**Add media from apps** like YouTube and Vimeo, or add audio from Spotify and SoundCloud. Files from Google Drive and Dropbox update automatically. Start a new line and choose add media, or drop in a link to try it out.\n\n\n![](https://paper-attachments.dropbox.com/s_72143DBFDAF4C9DE702BB246920BC47FE7E1FA76AC23CC699374430D94E96DD2_1523575138939_paper-embed.png)\n\n\n\n## YouTube\nhttps://www.youtube.com/watch?v=fmsq1uKOa08&\n\n\n[https://youtu.be/fmsq1uKOa08](https://youtu.be/fmsq1uKOa08)\n\n\n\n## SoundCloud\nhttps://w.soundcloud.com/player/?url=https%3A%2F%2Fsoundcloud.com%2Ftycho%2Fspoon-inside-out-tycho-version&autoplay=false\n\n\n[https://soundcloud.com/tycho/spoon-inside-out-tycho-version](https://soundcloud.com/tycho/spoon-inside-out-tycho-version) \n\n\n\n## Dropbox files\nhttps://www.dropbox.com/s/bgi58tkovntch5e/Wireframe%20render.pdf?dl=0\n\n\n\n\n## Code\n\n**Write code** in Dropbox Paper with automatic language detection and syntax highlighting. Start a new line and type three backticks (```).\n\n\n    public class HelloWorld { \n       public static void main(String[] args) { \n          System.out.println(""Hello, World"");\n       }\n    }\n\n\n\n## Tables\n\n**Create a table** with the menu that shows up on the right when you start a new line.\n\n| To insert a row or column, hover over a dividing line and click the +                                   | ‚≠ê     |\n| ------------------------------------------------------------------------------------------------------- | ----- |\n| To delete, select rows/columns and click the trash can                                                  | ‚≠ê ‚≠ê   |\n| To delete the entire table, click inside a cell, then click the dot in the top left corner of the table | ‚≠ê ‚≠ê ‚≠ê |\n\n\n\n\n\n# Collaborate with people\n\n**Invite people to your doc** so they can view, comment, and edit. Invite anyone you‚Äôd like‚Äîteam members, contractors, stakeholders‚Äîto give them access to your doc.\n\n![](https://paper-attachments.dropbox.com/s_72143DBFDAF4C9DE702BB246920BC47FE7E1FA76AC23CC699374430D94E96DD2_1523574876795_paper-invite.png)\n\n\n**Make your docs discoverable to your team** by adding them to shared folders. Invite-only folders create more privacy.\n\n\n## Comments\n\n**Add comments** on a single character, an entire document, or any asset by highlighting it. **Add stickers** by clicking the üòÑ in the message box.\n\n\n## To-dos\n\n**Bring someone‚Äôs attention to a comment or to-do** by typing **@** and their name or email address. Reference a doc or folder by typing **+** and its name.\n\n[ ] Mentioning someone on a to-do assigns it to them and sends an email [@Patricia J](http://#)\n[ ] Add a due date by clicking the calendar icon [@Jonathan C](http://#) [@Patricia J](http://#)\n[ ] You can also mention docs [+üéâ  Getting Started with Dropbox Paper](http://#)\n\n\n\n# Go mobile\n\nEdit, create, and share Paper docs on Android or iOS phones and tablets. Download the apps in the [App Store](https://itunes.apple.com/us/app/paper-by-dropbox/id1126623662) and [Google Play Store](https://play.google.com/store/apps/details?id=com.dropbox.paper).\n\n\n\n# Help\n\n**Visit the** [**help center**](https://www.dropbox.com/help/topics/paper) for more about Dropbox Paper.\n\n**For more tips,** click the **?** in the bottom right of the screen and choose **Paper guide**.\n\n**Give us feedback** by selecting ‚ÄúFeedback‚Äù from the **?** in the bottom right of the screen. We‚Äôd love to hear what you think. \n\n' metadata={'source': 'dropbox:///_ Getting Started with Dropbox Paper.paper', 'title': '_ Getting Started with Dropbox Paper.paper'}page_content='# ü•Ç Toast to Droplets\n‚ùì **Rationale:** Reflection, especially writing, is the key to deep learning! Let‚Äôs take a few minutes to reflect on your first day at Dropbox individually, and then one lucky person will have the chance to share their toast.\n\n‚úçÔ∏è **How to fill out this template:**\n\n- Option 1: You can sign in and then click ‚ÄúCreate doc‚Äù to make a copy of this template. Fill in the blanks!\n- Option 2: If you don‚Äôt know your personal Dropbox login quickly, you can copy and paste this text into another word processing tool and start typing! \n\n\n\n## To my Droplet class:\n\nI feel so happy and excited to be making a toast to our newest Droplet class at Dropbox Basecamp.\n\nAt the beginning of our first day, I felt a bit underwhelmed with all information, and now, at the end of our first day at Dropbox, I feel I know enough for me to ramp up, but still a lot to learn**.**\n\nI can‚Äôt wait to explore every drl, but especially drl/(App Center)/benefits/allowance. I heard it‚Äôs so informative!\n\nDesigning an enlightened way of working is important, and to me, it means **a lot since I love what I do and I can help people around the globe**.\n\nI am excited to work with my team and flex my **technical and social** skills in my role as a **Software Engineer**.\n\nAs a Droplet, I pledge to:\n\n\n1. Be worthy of trust by **working always with values and integrity**.\n\n\n1. Keep my customers first by  **caring about their happiness and the value that we provide as a company**.\n\n\n1. Own it, keep it simple, and especially make work human by **providing value to people****.**\n\nCongrats, Droplets!\n\n' metadata={'source': 'dropbox:///_ Toast to Droplets.paper', 'title': '_ Toast to Droplets.paper'}page_content='APPEARED IN BULLETIN OF THE AMERICAN MATHEMATICAL SOCIETY Volume 31, Number 1, July 1994, Pages 15-38\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n4 9 9 1\n\nK. RUBIN AND A. SILVERBERG\n\nl u J\n\nAbstract. In lectures at the Newton Institute in June of 1993, Andrew Wiles announced a proof of a large part of the Taniyama-Shimura Conjecture and, as a consequence, Fermat‚Äôs Last Theorem. This report for nonexperts dis- cusses the mathematics involved in Wiles‚Äô lectures, including the necessary background and the mathematical history.\n\n1\n\n] T N . h t a m\n\nIntroduction\n\nOn June 23, 1993, Andrew Wiles wrote on a blackboard, before an audience at the Newton Institute in Cambridge, England, that if p is a prime number, u, v, and w are rational numbers, and up + vp + wp = 0, then uvw = 0. In other words, he announced that he could prove Fermat‚Äôs Last Theorem. His announce- ment came at the end of his series of three talks entitled ‚ÄúModular forms, elliptic curves, and Galois representations‚Äù at the week-long workshop on ‚Äúp-adic Galois representations, Iwasawa theory, and the Tamagawa numbers of motives‚Äù.\n\n[\n\n1 v 0 2 2 7 0 4 9 / h t a m : v i X r a\n\nIn the margin of his copy of the works of Diophantus, next to a problem on\n\nPythagorean triples, Pierre de Fermat (1601‚Äì1665) wrote:\n\nCubum autem in duos cubos, aut quadratoquadratum in duos quadrato- quadratos, et generaliter nullam in inÔ¨Ånitum ultra quadratum potestatem in duos ejusdem nominis fas est dividere : cujus rei demonstrationem mirabilem sane detexi. Hanc marginis exiguitas non caperet.\n\n(It is impossible to separate a cube into two cubes, or a fourth power into two fourth powers, or in general, any power higher than the second into two like powers. I have discovered a truly marvelous proof of this, which this margin is too narrow to contain.)\n\nWe restate Fermat‚Äôs conjecture as follows.\n\nFermat‚Äôs Last Theorem. If n > 2, then an +bn = cn has no solutions in nonzero integers a, b, and c.\n\nA proof by Fermat has never been found, and the problem has remained open, inspiring many generations of mathematicians. Much of modern number theory has been built on attempts to prove Fermat‚Äôs Last Theorem. For details on the\n\nReceived by the editors November 29, 1993. 1991 Mathematics Subject ClassiÔ¨Åcation. Primary 11G05; Secondary 11D41, 11G18. The authors thank the National Science Foundation for Ô¨Ånancial support.\n\nc(cid:13)1994 American Mathematical Society 0273-0979/94 $1.00 + $.25 per page\n\n1\n\n2\n\nK. RUBIN AND A. SILVERBERG\n\nhistory of Fermat‚Äôs Last Theorem (last because it is the last of Fermat‚Äôs questions to be answered) see [5], [6], and [26].\n\nWhat Andrew Wiles announced in Cambridge was that he could prove ‚Äúmany‚Äù elliptic curves are modular, suÔ¨Éciently many to imply Fermat‚Äôs Last Theorem. In this paper we will explain Wiles‚Äô work on elliptic curves and its connection with 1 we introduce elliptic curves and modularity, and Fermat‚Äôs Last Theorem. give the connection between Fermat‚Äôs Last Theorem and the Taniyama-Shimura Conjecture on the modularity of elliptic curves. In 2 we describe how Wiles re- duces the proof of the Taniyama-Shimura Conjecture to what we call the Modular Lifting Conjecture (which can be viewed as a weak form of the Taniyama-Shimura Conjecture), by using a theorem of Langlands and Tunnell. In 4 we show ¬ß how the Semistable Modular Lifting Conjecture is related to a conjecture of Mazur on deformations of Galois representations (Conjecture 4.2), and in 5 we describe Wiles‚Äô method of attack on this conjecture. In order to make this survey as acces- sible as possible to nonspecialists, the more technical details are postponed as long as possible, some of them to the appendices.\n\nIn\n\n¬ß\n\n¬ß\n\n3 and ¬ß\n\n¬ß\n\nMuch of this report is based on Wiles‚Äô lectures in Cambridge. The authors apol- ogize for any errors we may have introduced. We also apologize to those whose mathematical contributions we, due to our incomplete understanding, do not prop- erly acknowledge.\n\nThe ideas Wiles introduced in his Cambridge lectures will have an important inÔ¨Çuence on research in number theory. Because of the great interest in this subject and the lack of a publicly available manuscript, we hope this report will be useful to the mathematics community. In early December 1993, shortly before this paper went to press, Wiles announced that ‚Äúthe Ô¨Ånal calculation of a precise upper bound for the Selmer group in the semistable case‚Äù (see 5.4 below) ‚Äúis not yet ¬ß complete as it stands,‚Äù but that he believes he will be able to Ô¨Ånish it in the near future using the ideas explained in his Cambridge lectures. While Wiles‚Äô proof of Theorem 5.3 below and Fermat‚Äôs Last Theorem depends on the calculation he referred to in his December announcement, Theorem 5.4 and Corollary 5.5 do not. Wiles‚Äô work provides for the Ô¨Årst time inÔ¨Ånitely many modular elliptic curves over the rational numbers which are not isomorphic over the complex numbers (see 5.5 for an explicit inÔ¨Ånite family).\n\n5.3 and\n\n¬ß\n\n¬ß\n\nNotation. The integers, rational numbers, complex numbers, and p-adic integers will be denoted Z, Q, C, and Zp, respectively. If F is a Ô¨Åeld, then ¬ØF denotes an algebraic closure of F .\n\n1. Connection between Fermat‚Äôs Last Theorem and elliptic curves\n\n1.1. Fermat‚Äôs Last Theorem follows from modularity of elliptic curves. Suppose Fermat‚Äôs Last Theorem were false. Then there would exist nonzero integers a, b, c, and n > 2 such that an + bn = cn. It is easy to see that no generality is lost by assuming that n is a prime greater than three (or greater than four million, by [2]; see [14] for n = 3 and 4) and that a and b are relatively prime. Write down the cubic curve:\n\ny2 = x(x + an)(x\n\nbn).\n\n(1)\n\n‚àí\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n3\n\n1.4 we will explain what it means for an elliptic curve to be modular. Kenneth Ribet [27] proved that if n is a prime greater than three, a, b, and c are nonzero integers, and an + bn = cn, then the elliptic curve (1) is not modular. But the results announced by Wiles imply the following.\n\nIn\n\n1.3 we will see that such curves are elliptic curves, and in\n\n¬ß\n\n¬ß\n\nTheorem 1.1 (Wiles). If A and B are distinct, nonzero, relatively prime integers, and AB(A\n\nB) is divisible by 16, then the elliptic curve\n\n‚àí\n\ny2 = x(x + A)(x + B)\n\nis modular.\n\nbn with a, b, c, and n coming from our hypothetical solution to a Fermat equation as above, we see that the conditions of Theorem 1.1 are satisÔ¨Åed since n 5 and one of a, b, and c is even. Thus Theorem 1.1 and Ribet‚Äôs result together imply Fermat‚Äôs Last Theorem!\n\nTaking A = an and B =\n\n‚àí\n\n‚â•\n\n1.2. History. The story of the connection between Fermat‚Äôs Last Theorem and elliptic curves begins in 1955, when Yutaka Taniyama (1927‚Äì1958) posed problems which may be viewed as a weaker version of the following conjecture (see [38]).\n\nTaniyama-Shimura Conjecture. Every elliptic curve over Q is modular.\n\nThe conjecture in the present form was made by Goro Shimura around 1962‚Äì64 and has become better understood due to work of Shimura [33‚Äì37] and of Andr¬¥e Weil [42] (see also [7]). The Taniyama-Shimura Conjecture is one of the major conjectures in number theory.\n\nBeginning in the late 1960s [15‚Äì18], Yves Hellegouarch connected Fermat equa- tions an + bn = cn with elliptic curves of the form (1) and used results about Fer- mat‚Äôs Last Theorem to prove results about elliptic curves. The landscape changed abruptly in 1985 when Gerhard Frey stated in a lecture at Oberwolfach that elliptic curves arising from counterexamples to Fermat‚Äôs Last Theorem could not be mod- ular [11]. Shortly thereafter Ribet [27] proved this, following ideas of Jean-Pierre Serre [32] (see [24] for a survey). In other words, ‚ÄúTaniyama-Shimura Conjecture\n\nFermat‚Äôs Last Theorem‚Äù. Thus, the stage was set. A proof of the Taniyama-Shimura Conjecture (or enough of it to know that elliptic curves coming from Fermat equations are modular) would be a proof of Fermat‚Äôs Last Theorem.\n\n‚áí\n\n1.3. Elliptic curves.\n\nDeÔ¨Ånition. An elliptic curve over Q is a nonsingular curve deÔ¨Åned by an equation of the form\n\ny2 + a1xy + a3y = x3 + a2x2 + a4x + a6\n\n(2)\n\nwhere the coeÔ¨Écients ai are integers. The solution ( on the elliptic curve.\n\n, ‚àû\n\n) will be viewed as a point\n\n‚àû\n\n4\n\nK. RUBIN AND A. SILVERBERG\n\nRemarks. (i) A singular point on a curve f (x, y) = 0 is a point where both partial derivatives vanish. A curve is nonsingular if it has no singular points.\n\n(ii) Two elliptic curves over Q are isomorphic if one can be obtained from the other by changing coordinates x = A2x‚Ä≤ + B, y = A3y‚Ä≤ + Cx‚Ä≤ + D, with A, B, C, D\n\nQ and dividing through by A6.\n\n‚àà (iii) Every elliptic curve over Q is isomorphic to one of the form\n\ny2 = x3 + a2x2 + a4x + a6\n\nwith integers ai. A curve of this form is nonsingular if and only if the cubic on the right side has no repeated roots.\n\nExample. The equation y2 = x(x + 32)(x\n\n42) deÔ¨Ånes an elliptic curve over Q.\n\n‚àí\n\n1.4. Modularity. Let H denote the complex upper half plane C : Im(z) > 0 } where Im(z) is the imaginary part of z. If N is a positive integer, deÔ¨Åne a group of matrices\n\nz\n\n{\n\n‚àà\n\na b c d\n\nSL2(Z) : c is divisible by N\n\n.\n\nŒì0(N ) =\n\n‚àà\n\n(z) = az+b The group Œì0(N ) acts on H by linear fractional transformations cz+d . (cid:9) (cid:1) The quotient space H/Œì0(N ) is a (noncompact) Riemann surface. It can be com- pleted to a compact Riemann surface, denoted X0(N ), by adjoining a Ô¨Ånite set of points called cusps. The cusps are the Ô¨Ånitely many equivalence classes of Q ‚àû} under the action of Œì0(N ) (see Chapter 1 of [35]). The complex points of an elliptic curve can also be viewed as a compact Riemann surface.\n\na b c d\n\n(cid:8)(cid:0)\n\n(cid:1)\n\n(cid:0)\n\ni\n\n‚à™{\n\nDeÔ¨Ånition. An elliptic curve E is modular if, for some integer N , there is a holo- morphic map from X0(N ) onto E.\n\nExample. It can be shown that there is a (holomorphic) isomorphism from X0(15) onto the elliptic curve y2 = x(x + 32)(x\n\n42).\n\n‚àí\n\nRemark . There are many equivalent deÔ¨Ånitions of modularity (see II.4.D of [24] and appendix of [22]). In some cases the equivalence is a deep result. For Wiles‚Äô 1.7 proof of Fermat‚Äôs Last Theorem it suÔ¨Éces to use only the deÔ¨Ånition given in below.\n\n¬ß\n\n¬ß\n\n1.5. Semistability.\n\nDeÔ¨Ånition. An elliptic curve over Q is semistable at the prime q if it is isomorphic to an elliptic curve over Q which modulo q either is nonsingular or has a singu- lar point with two distinct tangent directions. An elliptic curve over Q is called semistable if it is semistable at every prime.\n\nExample. The elliptic curve y2 = x(x + 32)(x isomorphic to y2 + xy + y = x3 + x2 x(x + 42)(x\n\n42) is semistable because it is ‚àí 10, but the elliptic curve y2 =\n\n10x\n\n‚àí\n\n‚àí\n\n32) is not semistable (it is not semistable at 2).\n\n‚àí\n\n2 we explain how Wiles shows that his main result on Galois representations (Theorem 5.3) implies the following part of the Taniyama-Shimura Conjecture.\n\nBeginning in\n\n¬ß\n\nSemistable Taniyama-Shimura Conjecture. Every semistable elliptic curve over Q is modular.\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n5\n\nProposition 1.2. The Semistable Taniyama-Shimura Conjecture implies Theorem 1.1.\n\nProof. If A and B are distinct, nonzero, relatively prime integers, write EA,B for the elliptic curve deÔ¨Åned by y2 = x(x + A)(x + B). Since EA,B and E‚àíA,‚àíB are isomorphic over the complex numbers (i.e., as Riemann surfaces), EA,B is modular if and only if E‚àíA,‚àíB is modular. If further AB(A B) is divisible by 16, then either EA,B or E‚àíA,‚àíB is semistable (this is easy to check directly; see for example I.1 of [24]). The Semistable Taniyama-Shimura Conjecture now implies that both ¬ß EA,B and E‚àíA,‚àíB are modular, and thus implies Theorem 1.1.\n\n‚àí\n\nRemark . In 1.1 we saw that Theorem 1.1 and Ribet‚Äôs Theorem together imply Fermat‚Äôs Last Theorem. Therefore, the Semistable Taniyama-Shimura Conjecture implies Fermat‚Äôs Last Theorem.\n\n¬ß\n\n1.6. Modular forms. In this paper we will work with a deÔ¨Ånition of modularity which uses modular forms.\n\nDeÔ¨Ånition. If N is a positive integer, a modular form f of weight k for Œì0(N ) is C which satisÔ¨Åes a holomorphic function f : H\n\n‚Üí\n\nf (Œ≥(z)) = (cz + d)kf (z)\n\na b c d\n\nH,\n\n(3)\n\nŒì0(N ) and z\n\nfor every Œ≥ =\n\n‚àà\n\n‚àà\n\n(cid:1)\n\n(cid:0)\n\nand is holomorphic at the cusps (see Chapter 2 of [35]).\n\n1 1 0 1\n\nŒì0(N )), so ‚àû n=0 ane2œÄinz, with complex numbers an and it has a Fourier expansion f (z) = (cid:1) . We say f is a cusp form if it with n vanishes at all the cusps; in particular for a cusp form the coeÔ¨Écient a0 (the value at i\n\nA modular form f satisÔ¨Åes f (z) = f (z + 1) (apply (3) to\n\n‚àà\n\n(cid:0)\n\n0 because f is holomorphic at the cusp i\n\n‚â•\n\n‚àû\n\nP\n\n) is zero. Call a cusp form normalized if a1 = 1.\n\n‚àû For Ô¨Åxed N there are commuting linear operators (called Hecke operators) Tm, 1, on the (Ô¨Ånite-dimensional) vector space of cusp forms of weight\n\nfor integers m two for Œì0(N ) (see Chapter 3 of [35]). If f (z) =\n\n‚â•\n\n‚àû n=1 ane2œÄinz, then\n\nP danm/d2\n\n‚àû\n\ne2œÄinz\n\n(4)\n\nTmf (z) =\n\nn=1 X\n\n(d,N )=1 d|(n,m)\n\n(cid:0) X\n\n(cid:1)\n\nwhere (a, b) denotes the greatest common divisor of a and b and a b means that a divides b. The Hecke algebra T (N ) is the ring generated over Z by these operators.\n\n|\n\nDeÔ¨Ånition. In this paper an eigenform will mean a normalized cusp form of weight two for some Œì0(N ) which is an eigenfunction for all the Hecke operators.\n\n‚àû n=1 ane2œÄinz is an eigenform, then Tmf = amf for all m.\n\nBy (4), if f (z) =\n\nP\n\n6\n\nK. RUBIN AND A. SILVERBERG\n\n1.7. Modularity, revisited. Suppose E is an elliptic curve over Q. If p is a prime, write Fp for the Ô¨Ånite Ô¨Åeld with p elements, and let E(Fp) denote the Fp- solutions of the equation for E (including the point at inÔ¨Ånity). We now give a second deÔ¨Ånition of modularity for an elliptic curve.\n\nDeÔ¨Ånition. An elliptic curve E over Q is modular if there exists an eigenform\n\n‚àû n=1 ane2œÄinz such that for all but Ô¨Ånitely many primes q,\n\n#(E(Fq)).\n\n(5) P\n\naq = q + 1\n\n‚àí 2. An overview\n\nThe Ô¨Çow chart shows how Fermat‚Äôs Last Theorem would follow if one knew the Semistable Modular Lifting Conjecture (Conjecture 2.1) for the primes 3 and 5. 1 we discussed the upper arrow, i.e., the implication ‚ÄúSemistable Taniyama- In ¬ß Fermat‚Äôs Last Theorem‚Äù. In this section we will discuss the Shimura Conjecture other implications in the Ô¨Çow chart. The implication given by the lowest arrow is straightforward (Proposition 2.3), while the middle one uses an ingenious idea of Wiles (Proposition 2.4).\n\n‚áí\n\nFermat‚Äôs Last Theorem\n\n‚úª\n\nSemistable Taniyama-Shimura Conjecture\n\n‚úª\n\n(cid:0)\n\n‚ùÖ ‚ùÖ\n\n(cid:0)\n\nSemistable Taniyama-Shimura for ¬ØœÅE,3 irreducible\n\nSemistable Modular Lifting for p = 5\n\n‚úª\n\n(cid:0) (cid:0)\n\n‚ùÖ\n\n‚ùÖ\n\nSemistable Modular Lifting for p = 3\n\nLanglands-Tunnell Theorem\n\nSemistable Modular Lifting Conjecture\n\nFermat‚Äôs Last Theorem .\n\n‚áí\n\nRemark . By the Modular Lifting Conjecture we will mean the Semistable Modular Lifting Conjecture with the hypothesis of semistability removed. The arguments of this section can also be used to show that the Modular Lifting Conjecture for p = 3 and 5, together with the Langlands-Tunnell Theorem, imply the full Taniyama- Shimura Conjecture.\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n7\n\n2.1. Semistable Modular Lifting. Let ¬ØQ denote the algebraic closure of Q in C, and let GQ be the Galois group Gal( ¬ØQ/Q). If p is a prime, write\n\nF√ó p\n\n¬ØŒµp : GQ\n\n‚Üí\n\nfor the character giving the action of GQ on the p-th roots of unity. For the facts about elliptic curves stated below, see [39]. If E is an elliptic curve over Q and F is a subÔ¨Åeld of the complex numbers, there is a natural commutative group law on the set of F -solutions of E, with the point at inÔ¨Ånity as the identity element. Denote this group E(F ). If p is a prime, write E[p] for the subgroup of points in E( ¬ØQ) of order dividing p. Then E[p] ‚àº= F2 p. The action of GQ on E[p] gives a continuous representation\n\nGL2(Fp)\n\n¬ØœÅE,p : GQ\n\n‚Üí\n\n(deÔ¨Åned up to isomorphism) such that\n\n(6)\n\ndet(¬ØœÅE,p) = ¬ØŒµp\n\nand for all but Ô¨Ånitely many primes q,\n\n#(E(Fq))\n\n(7)\n\ntrace(¬ØœÅE,p(Frobq))\n\nq + 1\n\n(mod p).\n\n‚â° (See Appendix A for the deÔ¨Ånition of the Frobenius elements Frobq ‚àà to each prime number q.)\n\n‚àí\n\nGQ attached\n\n‚àû n=1 ane2œÄinz is an eigenform, let\n\nOf denote the ring of integers of the number Ô¨Åeld Q(a2, a3, . . . ). (Recall that our eigenforms are normalized so that a1 = 1.)\n\nIf f (z) =\n\nP\n\nThe following conjecture is in the spirit of a conjecture of Mazur (see Conjectures\n\n3.2 and 4.2).\n\nConjecture 2.1 (Semistable Modular Lifting Conjecture). Suppose p is an odd prime and E is a semistable elliptic curve over Q satisfying\n\n(a) ¬ØœÅE,p is irreducible, (b) there are an eigenform f (z) =\n\n‚àû n=1 ane2œÄinz and a prime ideal Œª of\n\nOf\n\nsuch that p\n\nŒª and for all but Ô¨Ånitely many primes q,\n\n‚àà\n\nP\n\n#(E(Fq))\n\naq ‚â°\n\nq + 1\n\n(mod Œª).\n\n‚àí\n\nThen E is modular.\n\nThe Semistable Modular Lifting Conjecture is a priori weaker than the Semi- stable Taniyama-Shimura Conjecture because of the extra hypotheses (a) and (b). The more serious condition is (b); there is no known way to produce such a form in general. But when p = 3, the existence of such a form follows from the theorem below of Tunnell [41] and Langlands [20]. Wiles then gets around condition (a) by a clever argument (described below) which, when ¬ØœÅE,3 is not irreducible, allows him to use p = 5 instead.\n\n8\n\nK. RUBIN AND A. SILVERBERG\n\n2.2. Langlands-Tunnell Theorem. In order to state the Langlands-Tunnell Theorem, we need weight-one modular forms for a subgroup of Œì0(N ). Let\n\na b c d\n\nSL2(Z) : c\n\n0 (mod N ), a\n\nd\n\n1 (mod N )\n\n.\n\nŒì1(N ) =\n\n‚àà\n\n‚â°\n\n‚â°\n\n‚â°\n\n(cid:1)\n\n(cid:9)\n\n(cid:8)(cid:0)\n\nReplacing Œì0(N ) by Œì1(N ) in 1.6, one can deÔ¨Åne the notion of cusp forms on ¬ß Œì1(N ). See Chapter 3 of [35] for the deÔ¨Ånitions of the Hecke operators on the space of weight-one cusp forms for Œì1(N ).\n\nTheorem 2.2 (Langlands-Tunnell). Suppose œÅ : GQ GL2(C) is a continuous irreducible representation whose image in PGL2(C) is a subgroup of S4 (the sym- metric group on four elements ), œÑ is complex conjugation, and det(œÅ(œÑ )) = 1. ‚àû n=1 bne2œÄinz for some Œì1(N ), which is an Then there is a weight-one cusp form eigenfunction for all the corresponding Hecke operators, such that for all but Ô¨Ånitely many primes q,\n\n‚Üí\n\n‚àí\n\nP\n\n(8)\n\nbq = trace(œÅ(Frobq)).\n\nThe theorem as stated by Langlands [20] and by Tunnell [41] produces an auto- morphic representation rather than a cusp form. Using the fact that det(œÅ(œÑ )) = 1, standard techniques (see for example [12]) show that this automorphic repre-\n\n‚àí sentation corresponds to a weight-one cusp form as in Theorem 2.2.\n\n2.3. Semistable Modular Lifting\n\nSemistable Taniyama-Shimura.\n\n‚áí\n\nProposition 2.3. Suppose the Semistable Modular Lifting Conjecture is true for p = 3, E is a semistable elliptic curve, and ¬ØœÅE,3 is irreducible. Then E is modular.\n\nProof. It suÔ¨Éces to show that hypothesis (b) of the Semistable Modular Lifting Conjecture is satisÔ¨Åed with the given curve E, for p = 3. There is a faithful representation\n\nGL2(Z[‚àö\n\nGL2(C)\n\nœà : GL2(F3) ÷í\n\n2])\n\n‚àí\n\n‚äÇ\n\n‚Üí\n\nGL2(F3),\n\nsuch that for every g\n\n‚àà trace(œà(g))\n\n(mod(1 + ‚àö\n\n(9)\n\ntrace(g)\n\n2))\n\n‚â°\n\n‚àí\n\nand\n\n(10)\n\ndet(œà(g))\n\ndet(g)\n\n(mod 3).\n\n‚â°\n\nExplicitly, œà can be deÔ¨Åned on generators of GL2(F3) by\n\n‚àö\n\n1 1 1 0\n\n1 1 1 0\n\n1 1\n\n1 1\n\n2 1 1 0\n\n.\n\nœà\n\n=\n\nand œà\n\n=\n\n‚àí ‚àí\n\n‚àí ‚àí\n\n‚àí\n\n‚àí\n\n(cid:19)\n\n(cid:18)(cid:18)\n\n(cid:19)(cid:19)\n\n(cid:18)\n\n(cid:18)(cid:18) ¬ØœÅE,3. If œÑ is complex conjugation, then it follows from (6) and (10) that 1. The image of œà in PGL2(C) is a subgroup of PGL2(F3) ‚àº= S4.\n\n(cid:19)\n\n(cid:19)(cid:19)\n\n(cid:18)\n\nLet œÅ = œà ‚ó¶ det(œÅ(œÑ )) = Using that ¬ØœÅE,3 is irreducible, one can show that œÅ is irreducible.\n\n‚àí\n\n‚àû n=1 bne2œÄinz be a weight-one cusp form for some Œì1(N ) obtained by applying the Langlands-Tunnell\n\nLet p be a prime of ¬ØQ containing 1 + ‚àö\n\n2. Let g(z) =\n\n‚àí\n\nP\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n9\n\nTheorem (Theorem 2.2) to œÅ. It follows from (6) and (10) that N is divisible by 3. The function\n\n0 if d 1 if d 1 if d\n\n0 (mod 3), 1 (mod 3), 2 (mod 3)\n\n‚àû\n\n‚â° ‚â° ‚â°\n\nœá(d)e2œÄinz where œá(d) =\n\nE(z) = 1 + 6\n\n\uf8f1 \uf8f2\n\nn=1 X\n\nXd|n\n\n‚àí\n\n‚àû n=1 cne2œÄinz is a weight-one modular form for Œì1(3). The product g(z)E(z) = It is now is a weight-two cusp form for Œì0(N ) with cn ‚â° bn possible to Ô¨Ånd an eigenform f (z) = (mod p) for every n (see 6.10 and 6.11 of [4]). By (7), (8), and (9), f satisÔ¨Åes (b) of the Semistable Modular Lifting Conjecture with p = 3 and with Œª = p\n\n\uf8f3\n\nbn (mod p) for all n. P n=1 ane2œÄinz on Œì0(N ) such that an ‚â° ‚à© Of .\n\n‚àû\n\nP\n\nProposition 2.4 (Wiles). Suppose the Semistable Modular Lifting Conjecture is true for p = 3 and 5, E is a semistable elliptic curve over Q, and ¬ØœÅE,3 is reducible. Then E is modular.\n\nProof. The elliptic curves over Q for which both ¬ØœÅE,3 and ¬ØœÅE,5 are reducible are all known to be modular (see Appendix B.1). Thus we can suppose ¬ØœÅE,5 is irreducible. It suÔ¨Éces to produce an eigenform as in (b) of the Semistable Modular Lifting Conjecture, but this time there is no analogue of the Langlands-Tunnell Theorem to help. Wiles uses the Hilbert Irreducibility Theorem, applied to a parameter space of elliptic curves, to produce another semistable elliptic curve E‚Ä≤ over Q satisfying\n\n(i) ¬ØœÅE‚Ä≤,5 is isomorphic to ¬ØœÅE,5, and (ii) ¬ØœÅE‚Ä≤,3 is irreducible.\n\n(In fact there will be inÔ¨Ånitely many such E‚Ä≤; see Appendix B.2.) Now by Proposi- ‚àû n=1 ane2œÄinz be a corresponding eigenform. tion 2.3, E‚Ä≤ is modular. Let f (z) = Then for all but Ô¨Ånitely many primes q, P\n\n#(E‚Ä≤(Fq)) trace(¬ØœÅE,5(Frobq))\n\naq = q + 1\n\ntrace(¬ØœÅE‚Ä≤,5(Frobq)) #(E(Fq)) q + 1\n\n‚àí\n\n‚â° ‚â°\n\n(mod 5)\n\n‚â°\n\n‚àí\n\nby (7). Thus the form f satisÔ¨Åes hypothesis (b) of the Semistable Modular Lifting Conjecture, and we conclude that E is modular.\n\nTaken together, Propositions 2.3 and 2.4 show that the Semistable Modular Lifting Conjecture for p = 3 and 5 implies the Semistable Taniyama-Shimura Con- jecture.\n\n3. Galois representations\n\nThe next step is to translate the Semistable Modular Lifting Conjecture into a conjecture (Conjecture 3.2) about the modularity of liftings of Galois repre- sentations. Throughout this paper, if A is a topological ring, a representation GL2(A) will mean a continuous homomorphism and [œÅ] will denote the œÅ : GQ isomorphism class of œÅ. If p is a prime, let\n\n‚Üí\n\nZ√ó p\n\nŒµp : GQ\n\n‚Üí\n\nbe the character giving the action of GQ on p-power roots of unity.\n\n10\n\nK. RUBIN AND A. SILVERBERG\n\n3.1. The p-adic representation attached to an elliptic curve. Suppose E is an elliptic curve over Q and p is a prime number. For every positive integer n, write E[pn] for the subgroup in E( ¬ØQ) of points of order dividing pn and Tp(E) for the inverse limit of the E[pn] with respect to multiplication by p. For every n, E[pn] ‚àº= (Z/pnZ)2, and so Tp(E) ‚àº= Z2 p. The action of GQ induces a representation\n\nGL2(Zp)\n\nœÅE,p : GQ\n\n‚Üí\n\nsuch that det(œÅE,p) = Œµp and for all but Ô¨Ånitely many primes q,\n\n#(E(Fq)).\n\n(11)\n\ntrace(œÅE,p(Frobq)) = q + 1\n\n‚àí\n\nComposing œÅE,p with the reduction map from Zp to Fp gives ¬ØœÅE,p of\n\n2.1. ¬ß\n\n3.2. Modular representations. If f is an eigenform and Œª is a prime ideal of Of at Œª. Of , let\n\nOf,Œª denote the completion of\n\nDeÔ¨Ånition. If A is a ring, a representation œÅ : GQ if there are an eigenform f (z) = homomorphism Œπ :\n\nGL2(A) is called modular ‚àû n=1 ane2œÄinz, a ring A‚Ä≤ containing A, and a\n\n‚Üí\n\nA‚Ä≤ such that for all but Ô¨Ånitely many primes q,\n\nOf ‚Üí\n\nP\n\ntrace(œÅ(Frobq)) = Œπ(aq).\n\n‚àû n=1 ane2œÄinz and a prime ideal Œª of\n\nExamples. (i) Given an eigenform f (z) = Of , Eichler and Shimura (see\n\n7.6 of [35]) constructed a representation\n\n¬ß\n\nP\n\nœÅf,Œª : GQ\n\nGL2(\n\nOf,Œª)\n\n‚Üí\n\nZ = pZ) and for all but Ô¨Ånitely many primes q,\n\nsuch that det(œÅf,Œª) = Œµp (where Œª\n\n‚à©\n\n(12)\n\ntrace(œÅf,Œª(Frobq)) = aq.\n\nThus œÅf,Œª is modular with Œπ taken to be the inclusion of\n\nOf in\n\nOf,Œª.\n\n(ii) Suppose p is a prime and E is an elliptic curve over Q. If E is modular, then œÅE,p and ¬ØœÅE,p are modular by (11), (7), and (5). Conversely, if œÅE,p is modular, then it follows from (11) that E is modular. This proves the following.\n\nTheorem 3.1. Suppose E is an elliptic curve over Q. Then\n\nE is modular\n\nœÅE,p is modular for every p\n\nœÅE,p is modular for one p.\n\n‚áî\n\n‚áî\n\nRemark . In this language, the Semistable Modular Lifting Conjecture says that if p is an odd prime, E is a semistable elliptic curve over Q, and ¬ØœÅE,p is modular and irreducible, then œÅE,p is modular.\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n11\n\n3.3. Liftings of Galois representations. Fix a prime p and a Ô¨Ånite Ô¨Åeld k of characteristic p. Recall that ¬Øk denotes an algebraic closure of k.\n\nGiven a map œÜ : A\n\nB, the induced map from GL2(A) to GL2(B) will also be\n\n‚Üí\n\ndenoted œÜ. If œÅ : GQ A‚Ä≤ for the composition of œÅ with the inclusion of GL2(A) in GL2(A‚Ä≤).\n\nGL2(A) is a representation and A‚Ä≤ is a ring containing A, we write\n\n‚Üí\n\nœÅ\n\n‚äó\n\nDeÔ¨Ånition. If ¬ØœÅ : GQ œÅ : GQ Zp-algebra and there exists a homomorphism Œπ : A\n\nGL2(k) is a representation, we say that a representation GL2(A) is a lifting of ¬ØœÅ (to A) if A is a complete noetherian local\n\n‚Üí\n\n‚Üí\n\n¬Øk such that the diagram\n\n‚Üí GL2(A)\n\n‚úü‚úü‚úØ\n\n[œÅ]\n\n‚úü‚úü\n\nŒπ ‚ùÑ GL2(¬Øk)\n\n‚ú≤\n\nGQ\n\n[ ¬ØœÅ ‚äó ¬Øk]\n\n¬Øk].\n\ncommutes, in the sense that [Œπ\n\nœÅ] = [¬ØœÅ\n\n\n\n‚äó\n\nExamples. (i) If E is an elliptic curve then œÅE,p is a lifting of ¬ØœÅE,p.\n\n(ii) If E is an elliptic curve, p is a prime, and hypotheses (a) and (b) of Conjecture\n\n2.1 hold with an eigenform f and prime ideal Œª, then œÅf,Œª is a lifting of ¬ØœÅE,p.\n\n3.4. Deformation data. We will be interested not in all liftings of a given ¬ØœÅ, but rather in those satisfying various restrictions. See Appendix A for the deÔ¨Ånition of GQ associated to primes q. We say that a representation œÅ the inertia groups Iq ‚äÇ of GQ is unramiÔ¨Åed at a prime q if œÅ(Iq) = 1. If Œ£ is a set of primes, we say œÅ is unramiÔ¨Åed outside of Œ£ if œÅ is unramiÔ¨Åed at every q / ‚àà\n\nŒ£.\n\nDeÔ¨Ånition. By deformation data we mean a pair\n\n= (Œ£, t)\n\nD where Œ£ is a Ô¨Ånite set of primes and t is one of the words ordinary or Ô¨Çat.\n\nZ√ó\n\nA√ó be the composition of the\n\nIf A is a Zp-algebra, let ŒµA : GQ\n\np ‚Üí\n\n‚Üí\n\ncyclotomic character Œµp with the structure map.\n\nDeÔ¨Ånition. Given deformation data type- outside of Œ£, and œÅ is t at p (where t\n\nGL2(A) is if A is a complete noetherian local Zp-algebra, det(œÅ) = ŒµA, œÅ is unramiÔ¨Åed\n\n, a representation œÅ : GQ\n\nD\n\n‚Üí\n\nD\n\nordinary, Ô¨Çat }\n\n; see Appendix C).\n\n‚àà {\n\nDeÔ¨Ånition. A representation ¬ØœÅ : GQ eigenform f and a prime ideal Œª of\n\nmodular if there are an\n\nGL2(k) is Of such that œÅf,Œª is a type-\n\n‚Üí\n\nD\n\nlifting of ¬ØœÅ.\n\nD\n\nRemarks. (i) A representation with a type- fore if a representation is\n\nlifting must itself be type-\n\n. There-\n\nD\n\nD and modular.\n\nmodular, then it is both type-\n\nD\n\nD\n\n(ii) Conversely, if ¬ØœÅ is type-\n\n, modular, and satisÔ¨Åes (ii) of Theorem 5.3 below, -modular, by work of Ribet and others (see [28]). This plays an important\n\nD\n\nthen ¬ØœÅ is D role in Wiles‚Äô work.\n\n12\n\nK. RUBIN AND A. SILVERBERG\n\n3.5. Mazur Conjecture.\n\nDeÔ¨Ånition. A representation ¬ØœÅ : GQ ¬ØœÅ\n\nGL2(k) is called absolutely irreducible if\n\n‚Üí\n\n¬Øk is irreducible.\n\n‚äó\n\nThe following variant of a conjecture of Mazur (see Conjecture 18 of [23]; see\n\nalso Conjecture 4.2 below) implies the Semistable Modular Lifting Conjecture.\n\nConjecture 3.2 (Mazur). Suppose p is an odd prime, k is a Ô¨Ånite Ô¨Åeld of charac- GL2(k) is an absolutely irreducible teristic p, lifting of ¬ØœÅ to the ring of integers of\n\nis deformation data, and ¬ØœÅ : GQ -modular representation. Then every type-\n\nD\n\n‚Üí D\n\nD a Ô¨Ånite extension of Qp is modular.\n\nRemark . Loosely speaking, Conjecture 3.2 says that if ¬ØœÅ is modular, then every lifting which ‚Äúlooks modular‚Äù is modular.\n\nDeÔ¨Ånition. An elliptic curve E over Q has good (respectively, bad ) reduction at a prime q if E is nonsingular (respectively, singular) modulo q. An elliptic curve E over Q has ordinary (respectively, supersingular) reduction at q if E has good reduction at q and E[q] has (respectively, does not have) a subgroup of order q stable under the inertia group Iq.\n\nProposition 3.3. Conjecture 3.2 implies Conjecture 2.1.\n\nProof. Suppose p is an odd prime and E is a semistable elliptic curve over Q which satisÔ¨Åes (a) and (b) of Conjecture 2.1. We will apply Conjecture 3.2 with ¬ØœÅ = ¬ØœÅE,p. Write œÑ for complex conjugation. Then œÑ 2 = 1, and by (6), det(¬ØœÅE,p(œÑ )) = 1. Since ¬ØœÅE,p is irreducible and p is odd, a simple linear algebra argument now shows that ¬ØœÅE,p is absolutely irreducible.\n\n‚àí\n\nSince E satisÔ¨Åes (b) of Conjecture 2.1, ¬ØœÅE,p is modular. Let\n\nŒ£ = t = ordinary if E has ordinary or bad reduction at p, t = Ô¨Çat if E has supersingular reduction at p,\n\np\n\nprimes q : E has bad reduction at q\n\n,\n\n‚Ä¢\n\n{\n\n} ‚à™ {\n\n}\n\n= (Œ£, t).\n\nD\n\nUsing the semistability of E, one can show that œÅE,p is a type- (by combining results of several people; see [28]) that ¬ØœÅE,p is 3.2 then says œÅE,p is modular. By Theorem 3.1, E is modular.\n\nlifting of ¬ØœÅE,p and -modular. Conjecture\n\nD\n\nD\n\n4. Mazur‚Äôs deformation theory\n\nNext we reformulate Conjecture 3.2 as a conjecture (Conjecture 4.2) that the algebras which parametrize liftings and modular liftings of a given representation are isomorphic. It is this form of Mazur‚Äôs conjecture that Wiles attacks directly.\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n13\n\n4.1. The universal deformation algebra R. Fix an odd prime p, a Ô¨Ånite Ô¨Åeld k of characteristic p, deformation data representation ¬ØœÅ : GQ extension of Qp with residue Ô¨Åeld k.\n\n, and an absolutely irreducible type-\n\nD\n\nD is the ring of integers of a Ô¨Ånite\n\nGL2(k). Suppose\n\n‚Üí\n\nO\n\nDeÔ¨Ånition. We say œÅ : GQ complete noetherian local commutes\n\n)-lifting of ¬ØœÅ if œÅ is type-\n\n, A is a ‚Üí -algebra with residue Ô¨Åeld k, and the following diagram\n\nGL2(A) is a (\n\n,\n\nD\n\nO\n\nD\n\nO\n\nGL2(A)\n\n‚úü‚úü‚úØ\n\n[œÅ]\n\n‚úü‚úü\n\n‚ùÑ GL2(k)\n\n‚ú≤\n\nGQ\n\n[ ¬ØœÅ]\n\nwhere the vertical map is reduction modulo the maximal ideal of A.\n\nTheorem 4.1 (Mazur-Ramakrishna). With p, k, an that for every ( œÜœÅ : R\n\nas above, there are D GL2(R) of ¬ØœÅ, with the property -algebra homomorphism\n\n, ¬ØœÅ, and\n\nO\n\nalgebra R and a (\n\n)-lifting œÅR : GQ )-lifting œÅ of ¬ØœÅ to A there is a unique\n\n,\n\nO\n\nD\n\nO\n\n‚Üí\n\n,\n\nD\n\nO\n\nO\n\nA such that the diagram\n\n‚Üí\n\n[œÅR]\n\n‚ú≤\n\nGQ\n\nGL2(R)\n\n‚ùç\n\n‚ùç‚ùç\n\nœÜœÅ ‚ùÑ GL2(A)\n\n[œÅ]\n\n‚ùç‚ùç‚ù•\n\ncommutes.\n\nThis theorem was proved by Mazur [21] in the case when\n\nis ordinary and is Ô¨Çat. Theorem 4.1 determines R and œÅR up to\n\nD\n\nby Ramakrishna [25] when isomorphism.\n\nD\n\n4.2. The universal modular deformation algebra T. Fix an odd prime p, a , and an absolutely irreducible Ô¨Ånite Ô¨Åeld k of characteristic p, deformation data -modular, and Ô¨Åx an type- representation ¬ØœÅ : GQ eigenform f and a prime ideal Œª of lifting of ¬ØœÅ. is the ring of integers of a Ô¨Ånite extension of Qp with Suppose in addition that residue Ô¨Åeld k, Of,Œª ‚äÜ O\n\nD\n\nGL2(k). Assume ¬ØœÅ is\n\nD\n\n‚Üí\n\nD\n\nOf such that œÅf,Œª is a type-\n\nD\n\nO , and the diagram\n\nGL2(\n\nOf,Œª) ‚ùÑ GL2(k)\n\n‚úü‚úü‚úü‚úØ ‚ú≤\n\n[œÅf,Œª] ‚úü\n\nGQ\n\n[ ¬ØœÅ]\n\ncommutes, where the vertical map is the reduction map.\n\n)-lifting of ¬ØœÅ, and Wiles constructs a generalized Hecke algebra T which has the following properties (recall that Hecke algebras T (N ) were deÔ¨Åned in\n\nUnder these assumptions œÅf,Œª ‚äó O 1.6).\n\nis a (\n\n,\n\nD\n\nO\n\n¬ß\n\n(T1) T is a complete noetherian local\n\nalgebra with residue Ô¨Åeld k.\n\nO\n\n14\n\nK. RUBIN AND A. SILVERBERG\n\n(T2) There are an integer N divisible only by primes in Œ£ and a homomorphism by the Œ£. By abuse of notation\n\nfrom the Hecke algebra T (N ) to T such that T is generated over images of the Hecke operators Tq for primes q / ‚àà we write Tq also for its image in T.\n\nO\n\n(T3) There is a (\n\n,\n\n)-lifting\n\nD\n\nO\n\nGL2(T)\n\nœÅT : GQ\n\n‚Üí\n\nof ¬ØœÅ with the property that trace(œÅT(Frobq)) = Tq for every prime q / ‚àà\n\nŒ£. )-lifting of ¬ØœÅ to A, then there is a unique\n\n(T4) If œÅ is modular and is a (\n\n,\n\nD\n\nO\n\nalgebra homomorphism œàœÅ : T\n\nA such that the diagram\n\nO\n\n‚Üí [œÅ T]\n\n‚ú≤\n\nGL2(T)\n\nGQ\n\n‚ùç\n\n‚ùç‚ùç\n\nœàœÅ ‚ùÑ GL2(A)\n\n[œÅ]\n\n‚ùç‚ùç‚ù•\n\ncommutes.\n\nSince œÅT is a (\n\n,\n\n)-lifting of ¬ØœÅ, by Theorem 4.1 there is a homomorphism\n\nD\n\nO\n\nT\n\nœï : R\n\n‚Üí\n\nœÅR. By (T3), œï(trace(œÅR(Frobq))) = Tq for every\n\nsuch that œÅT is isomorphic to œï prime q / ‚àà\n\nŒ£, so it follows from (T2) that œï is surjective.\n\n4.3. Mazur Conjecture, revisited. Conjecture 3.2 can be reformulated in the following way.\n\nConjecture 4.2 (Mazur). Suppose p, k, T is an isomorphism. above map œï : R\n\n, ¬ØœÅ, and\n\nare as in\n\n4.2. Then the\n\nD\n\nO\n\n¬ß\n\n‚Üí\n\nConjecture 4.2 was stated in [23] (Conjecture 18) for\n\nordinary, and Wiles\n\nD\n\nmodiÔ¨Åed the conjecture to include the Ô¨Çat case.\n\nProposition 4.3. Conjecture 4.2 implies Conjecture 3.2.\n\nProof. Suppose ¬ØœÅ : GQ -modular, A is D the ring of integers of a Ô¨Ånite extension of Qp, and œÅ is a type- lifting of ¬ØœÅ to A. to be the ring of integers of a suÔ¨Éciently large Ô¨Ånite extension of Qp, and Taking and its residue Ô¨Åeld, respectively, we may assume that œÅ is extending œÅ and ¬ØœÅ to A, with œÜœÅ a ( as in Theorem 4.1. By (T3) and Theorem 4.1, œà(Tq) = trace(œÅ(Frobq)) for all but 3.5 of [35], given such a homomorphism œà (and viewing A as Ô¨Ånitely many q. By ‚àû n=1 ane2œÄinz where aq = œà(Tq) for all but a subring of C), there is an eigenform Ô¨Ånitely many primes q. Thus œÅ is modular.\n\nGL2(k) is absolutely irreducible and\n\n‚Üí\n\nD\n\nO )-lifting of ¬ØœÅ. Assuming Conjecture 4.2, let œà = œÜœÅ ‚ó¶\n\nO\n\nœï‚àí1 : T\n\n,\n\nD\n\nO\n\n‚Üí\n\n¬ß\n\nP\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n15\n\n5. Wiles‚Äô approach to the Mazur Conjecture\n\nIn this section we sketch the major ideas of Wiles‚Äô attack on Conjecture 4.2. The Ô¨Årst step (Theorem 5.2), and the key to Wiles‚Äô proof, is to reduce Conjecture 4.2 to a bound on the order of the cotangent space at a prime of R. In 5.2 we ¬ß see that the corresponding tangent space is a Selmer group, and in 5.3 we outline a general procedure due to Kolyvagin for bounding sizes of Selmer groups. The input for Kolyvagin‚Äôs method is known as an Euler system. The most diÔ¨Écult 5.4), and the part described as ‚Äúnot yet complete‚Äù in his part of Wiles‚Äô work ( ¬ß December announcement, is his construction of a suitable Euler system. In 5.5 we state the results announced by Wiles (Theorems 5.3 and 5.4 and Corollary 5.5) and explain why Theorem 5.3 suÔ¨Éces for proving the Semistable Taniyama-Shimura Conjecture. As an application of Corollary 5.5 we write down an inÔ¨Ånite family of modular elliptic curves. , ¬ØœÅ, 5 Ô¨Åx p, k,\n\n¬ß\n\n¬ß\n\n‚àû n=1 ane2œÄinz, and Œª as in\n\n4.2.\n\nFor O By property (T4) there is a homomorphism\n\n, f (z) =\n\n¬ß\n\n¬ß\n\nD\n\nP\n\nœÄ : T\n\n‚Üí O . By property (T2) and (12), œÄ satisÔ¨Åes\n\nsuch that œÄ œÄ(Tq) = aq for all but Ô¨Ånitely many q.\n\nœÅT is isomorphic to œÅf,Œª ‚äó O\n\n\n\n5.1. Key reduction. Wiles uses the following generalization of a theorem of Mazur, which says that T is Gorenstein.\n\nTheorem 5.1. There is a (noncanonical ) T-module isomorphism\n\n) ‚àº ‚Üí\n\nHomO(T,\n\nT.\n\nO\n\nLet Œ∑ denote the ideal of\n\ngenerated by the image under the composition\n\nO HomO(T,\n\n) ‚àº ‚Üí\n\nT œÄ\n\nO\n\n‚Üí O\n\nHomO(T,\n\nof the element œÄ ‚àà choice of isomorphism in Theorem 5.1.\n\n). The ideal Œ∑ is well deÔ¨Åned independent of the\n\nO\n\nThe map œÄ determines distinguished prime ideals of T and R,\n\nœï) = œï‚àí1(pT).\n\npT = ker(œÄ),\n\npR = ker(œÄ\n\n\n\nTheorem 5.2 (Wiles). If\n\n#(pR/p2\n\nR)\n\n#(\n\n/Œ∑) <\n\n, ‚àû\n\n‚â§\n\nO\n\nT is an isomorphism.\n\nthen œï : R\n\n‚Üí\n\nThe proof is entirely commutative algebra. The surjectivity of œï shows that /Œ∑). Thus if\n\n#(pR/p2 #(pR/p2\n\n#(pT/p2 #(\n\nT), and Wiles proves that #(pT/p2\n\nR) R)\n\nT)\n\n#(\n\n‚â• ‚â§\n\n‚â•\n\nO\n\n/Œ∑), then\n\nO\n\n#(pR/p2\n\nR) = #(pT/p2\n\n(13)\n\nT) = #(\n\n/Œ∑).\n\nO\n\nThe Ô¨Årst equality in (13) shows that œï induces an isomorphism of tangent spaces. Wiles uses the second equality in (13) and Theorem 5.1 to deduce that T is a local\n\n16\n\nK. RUBIN AND A. SILVERBERG\n\ncomplete intersection over that\n\n(that is, there are f1, . . . , fr ‚àà O\n\n[[x1, . . . , xr]] such\n\nO\n\nT ‚àº=\n\n[[x1, . . . , xr]]/(f1, . . . , fr)\n\nO\n\nas morphism.\n\nalgebras). Wiles then combines these two results to prove that œï is an iso-\n\nO\n\n5.2. Selmer groups. In general, if M is a torsion GQ-module, a Selmer group attached to M is a subgroup of the Galois cohomology group H 1(GQ, M ) deter- mined by certain ‚Äúlocal conditions‚Äù in the following way. If q is a prime with decomposition group Dq ‚äÇ\n\nGQ, then there is a restriction map\n\nresq : H 1(GQ, M )\n\nH 1(Dq, M ).\n\n‚Üí Jq ‚äÜ\n\nH 1(Dq, M ) : q prime\n\n= For a Ô¨Åxed collection of subgroups { the particular problem under consideration, the corresponding Selmer group is\n\ndepending on\n\nJ\n\n}\n\nres‚àí1\n\nH 1(GQ, M ).\n\nS(M ) =\n\nq (Jq)\n\n‚äÜ\n\nq \\ Write H i(Q, M ) for H i(GQ, M ), and H i(Qq, M ) for H i(Dq, M ).\n\nExample. The original examples of Selmer groups come from elliptic curves. Fix an elliptic curve E and a positive integer m, and take M = E[m], the subgroup of points in E( ¬ØQ) of order dividing m. There is a natural inclusion\n\nH 1(Q, E[m])\n\nE(Q)/mE(Q) ÷í\n\n(14)\n\n‚Üí\n\nE( ¬ØQ) is any\n\nE(Q) to the cocycle œÉ\n\nobtained by sending x point satisfying my = x. Similarly, for every prime q there is a natural inclusion\n\nœÉ(y)\n\ny, where y\n\n‚àà\n\n7‚Üí\n\n‚àí\n\n‚àà\n\nH 1(Qq, E[m]).\n\nE(Qq)/mE(Qq) ÷í\n\n‚Üí DeÔ¨Åne the Selmer group S(E[m]) in this case by taking the group Jq to be the image of E(Qq)/mE(Qq) in H 1(Qq, E[m]), for every q. This Selmer group is an important tool in studying the arithmetic of E because it contains (via (14)) E(Q)/mE(Q).\n\n5, let m denote the maximal ideal /mn) can be\n\nRetaining the notation from the beginning of\n\n¬ß\n\nand Ô¨Åx a positive integer n. The tangent space HomO(pR/p2 R,\n\nof identiÔ¨Åed with a Selmer group as follows. Let Vn be the matrix algebra M2(\n\nO\n\nO\n\n/mn), with GQ acting via the adjoint repre-\n\nO\n\nsentation œÉ(B) = œÅf,Œª(œÉ)BœÅf,Œª(œÉ)‚àí1. There is a natural injection\n\ns : HomO(pR/p2 R,\n\n/mn) ÷í\n\nH 1(Q, Vn)\n\nO\n\n‚Üí\n\nwhich is described in Appendix D (see also\n\n1.6 of [21]). Wiles deÔ¨Ånes a collection . Let SD(Vn) denote the associated Selmer\n\n¬ß\n\nH 1(Qq, Vn) }\n\n=\n\nJq ‚äÜ\n\ndepending on\n\nJ group. Wiles proves that s induces an isomorphism\n\n{\n\nD\n\n/mn) ‚àº ‚Üí\n\nHomO(pR/p2 R,\n\nSD(Vn).\n\nO\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n17\n\n5.3. Euler systems. We have now reduced the proof of Mazur‚Äôs conjecture to bounding the size of the Selmer groups SD(Vn). About Ô¨Åve years ago Kolyvagin [19], building on ideas of his own and of Thaine [40], introduced a revolutionary new method for bounding the size of a Selmer group. This new machinery, which is crucial for Wiles‚Äô proof, is what we now describe.\n\nH 1(Qq,M ) is } 5.2. Let ÀÜM = a system of subgroups with associated Selmer group S(M ) as in Hom(M, ¬µm), where ¬µm is the group of m-th roots of unity. For every prime q, the cup product gives a nondegenerate Tate pairing\n\nSuppose M is a GQ-module of odd exponent m and\n\n=\n\nJq ‚äÜ ¬ß\n\nJ\n\n{\n\nH 2(Qq, ¬µm) ‚àº ‚Üí H 1(Q, ÀÜM ), then\n\nH 1(Qq, ÀÜM )\n\niq : H 1(Qq, M )\n\nZ/mZ\n\n,\n\nh\n\n√ó\n\n‚Üí\n\nH 1(Q, M ) and d\n\n(see Chapters VI and VII of [3]). If c\n\n‚àà\n\n‚àà\n\n(15)\n\nresq(c), resq(d) h\n\niq = 0.\n\nq X\n\nH 1(Q, ÀÜM ) be the Selmer\n\nis a Ô¨Ånite set of primes. Let S‚àó\n\nSuppose that\n\nL ‚äÜ H 1(Qq, ÀÜM ) }\n\nL group given by the local conditions\n\n‚àó =\n\nJ ‚àó q ‚äÜ\n\n, where\n\nJ\n\n{\n\nthe orthogonal complement of Jq under H 1(Qq, ÀÜM )\n\n,\n\nif q / if q\n\n, ‚àà L . ‚àà L\n\niq\n\nJ ‚àó q =\n\nh\n\n(\n\nH 1(Q, ÀÜM ), deÔ¨Åne\n\nIf d\n\n‚àà\n\nZ/mZ\n\nŒ∏d :\n\nJq ‚Üí\n\nYq‚ààL\n\nby\n\nŒ∏d((cq)) =\n\ncq, resq(d) h\n\niq.\n\nXq‚ààL\n\nWrite resL : H 1(Q, M ) maps. By (15) and the deÔ¨Ånition of J ‚àó in addition resL is injective on S(M ), then\n\nq‚ààL H 1(Qq, M ) for the product of the restriction ker(Œ∏d). If\n\n‚Üí\n\nS‚àó\n\nq , if d\n\nL, then resL(S(M ))\n\n‚àà\n\n‚äÜ\n\nQ\n\n#(S(M ))\n\n#\n\nker(Œ∏d)\n\n.\n\n‚â§\n\n(cid:0) \\d‚ààS‚àó\n\nL\n\n(cid:1)\n\nThe diÔ¨Éculty is to produce enough cohomology classes in S‚àó\n\nL to show that the right side of the above inequality is small. Following Kolyvagin, an Euler system is S‚àó L for a large (inÔ¨Ånite) collection of sets of a compatible collection of classes Œ∫( )) primes is related to res‚Ñì(Œ∫( )). Once an Euler system is given, Kolyvagin has an inductive procedure for choosing a set\n\n) L\n\n‚àà\n\n. Loosely speaking, compatible means that if ‚Ñì /\n\n, then res‚Ñì(Œ∫(\n\n‚Ñì\n\nL\n\n‚àà L\n\nL ‚à™ {\n\n}\n\nL\n\nsuch that\n\nL\n\nresL is injective on S(M ),\n\n‚Ä¢\n\nP‚äÜL ker(Œ∏Œ∫(P)) can be computed in terms of Œ∫( ‚àÖ\n\n).\n\nT\n\n18\n\nK. RUBIN AND A. SILVERBERG\n\nS‚àó\n\nS‚àó\n\n, then S‚àó\n\nL.)\n\nL, so Œ∫(\n\n)\n\n(Note that if\n\nP ‚äÜ\n\nP For several important Selmer groups it is possible to construct Euler systems for\n\n‚àà\n\nP ‚äÜ L\n\nwhich Kolyvagin‚Äôs procedure produces a set\n\nactually giving an equality\n\nL ker(Œ∏Œ∫(P))\n\n#(S(M )) = #\n\n.\n\n(cid:0) \\P‚äÜL This is what Wiles needs to do for the Selmer group SD(Vn). There are several examples in the literature where this kind of argument is worked out in some detail. For the simplest case, where the Selmer group in question is the ideal class group ) are constructed from cyclotomic units, of a real abelian number Ô¨Åeld and the Œ∫( L see [29]. For other cases involving ideal class groups and Selmer groups of elliptic curves, see [19], [31], [30], [13].\n\n(cid:1)\n\n5.4. Wiles‚Äô geometric Euler system. The task now is to construct an Euler system of cohomology classes with which to bound #(SD(Vn)) using Kolyvagin‚Äôs method. This is the most technically diÔ¨Écult part of Wiles‚Äô proof and is the part of Wiles‚Äô work he referred to as not yet complete in his December announcement. We give only general remarks about Wiles‚Äô construction.\n\nThe Ô¨Årst step in the construction is due to Flach [10]. He constructed classes consisting of just one prime. This allows one to bound the ) L\n\nS‚àó\n\nŒ∫( exponent of SD(Vn), but not its order.\n\nL for sets\n\n‚àà\n\nL\n\nEvery Euler system starts with some explicit, concrete objects. Earlier examples of Euler systems come from cyclotomic or elliptic units, Gauss sums, or Heegner points on elliptic curves. Wiles (following Flach) constructs his cohomology classes from modular units, i.e., meromorphic functions on modular curves which are holo- morphic and nonzero away from the cusps. More precisely, Œ∫( ) comes from an explicit function on the modular curve X1(L, N ), the curve obtained by taking the quotient space of the upper half plane by the action of the group\n\nL\n\na b c d\n\nSL2(Z) : c\n\n1 (mod L) } ‚â° ‚Ñì‚ààL ‚Ñì and where N is the N of (T2) of\n\n0\n\n(mod LN ),\n\na\n\nd\n\n,\n\nŒì1(L, N ) =\n\n‚àà\n\n‚â°\n\n‚â°\n\n{ (cid:1) (cid:0) and adjoining the cusps, where L = The construction and study of the classes Œ∫( [8], [9] and others.\n\n4.2. ) rely heavily on results of Faltings\n\n¬ß\n\nL\n\nQ\n\n5.5. Wiles‚Äô results. Wiles announced two main results (Theorems 5.3 and 5.4 below) in the direction of Mazur‚Äôs conjecture, under two diÔ¨Äerent sets of hypotheses on the representation ¬ØœÅ. Theorem 5.3 implies the Semistable Taniyama-Shimura Conjecture and Fermat‚Äôs Last Theorem. Wiles‚Äô proof of Theorem 5.3 depends on the not-yet-complete construction of an appropriate Euler system (as in 5.4), while his proof of Theorem 5.4 (though not yet fully checked) does not. For Theorem 5.4, Wiles bounds the Selmer group of 5.2 without constructing a new Euler system, by using results from the Iwasawa theory of imaginary quadratic Ô¨Åelds. (These results in turn rely on Kolyvagin‚Äôs method and the Euler system of elliptic units; see [31].)\n\n¬ß\n\n¬ß\n\nSince for ease of exposition we deÔ¨Åned modularity of representations in terms of Œì0(N ) instead of Œì1(N ), the theorems stated below are weaker than those an- nounced by Wiles, but have the same applications to elliptic curves. (Note that by our deÔ¨Ånition of type-\n\n, if ¬ØœÅ is type-\n\n, then det(¬ØœÅ) = ¬ØŒµp.)\n\nD\n\nD\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n19\n\nIf ¬ØœÅ is a representation of GQ on a vector space V , Sym2(¬ØœÅ) denotes the repre-\n\nsentation on the symmetric square of V induced by ¬ØœÅ.\n\nTheorem 5.3 (Wiles). Suppose p, k, the following additional conditions :\n\n, ¬ØœÅ, and\n\nare as in\n\n4.2 and ¬ØœÅ satisÔ¨Åes\n\nD\n\nO\n\n¬ß\n\n(i) Sym2(¬ØœÅ) is absolutely irreducible, (ii) if ¬ØœÅ is ramiÔ¨Åed at q and q (iii) if p is 3 or 5, then for some prime q, p divides #(¬ØœÅ(Iq)).\n\n= p, then the restriction of ¬ØœÅ to Dq is reducible,\n\n6\n\nT is an isomorphism.\n\nThen œï : R\n\n‚Üí\n\nSince Theorem 5.3 does not yield the full Mazur Conjecture (Conjecture 4.2) for 2 to see which elliptic curves ¬ß\n\np = 3 and 5, we need to reexamine the arguments of E can be proved modular using Theorem 5.3 applied to ¬ØœÅE,3 and ¬ØœÅE,5.\n\nHypothesis (i) of Theorem 5.3 will be satisÔ¨Åed if the image of ¬ØœÅE,p is suÔ¨Éciently large in GL2(Fp) (for example, if ¬ØœÅE,p is surjective). For p = 3 and p = 5, if ¬ØœÅE,p satisÔ¨Åes hypothesis (iii) and is irreducible, then it satisÔ¨Åes hypothesis (i).\n\nIf E is semistable, p is an odd prime, and ¬ØœÅE,p is irreducible and modular, then (see the proof of Proposition 3.3) and ¬ØœÅE,p satisÔ¨Åes (ii) ¬ØœÅE,p is D 14 of Appendix C of [39]). Therefore by Propositions and (iii) (use Tate curves; see 4.3 and 3.3, Theorem 5.3 implies that the Semistable Modular Lifting Conjecture (Conjecture 2.1) holds for p = 3 and for p = 5. As shown in 2, the Semistable Taniyama-Shimura Conjecture and Fermat‚Äôs Last Theorem follow.\n\nmodular for some\n\nD\n\n¬ß\n\n¬ß\n\nTheorem 5.4 (Wiles). Suppose p, k, contains no nontrivial p-th roots of unity. Suppose also that there are an imaginary quadratic Ô¨Åeld F of discriminant prime to p and a character œá : Gal( ¬ØQ/F ) √ó such that T is the induced representation Indœá of GQ is a ( an isomorphism.\n\n, ¬ØœÅ, and\n\nare as in\n\n4.2 and\n\nD\n\nO\n\n¬ß\n\nO\n\n‚Üí O\n\n)-lifting of ¬ØœÅ. Then œï : R\n\n,\n\nD\n\nO\n\n‚Üí\n\nCorollary 5.5 (Wiles). Suppose E is an elliptic curve over Q with complex mul- tiplication by an imaginary quadratic Ô¨Åeld F and p is an odd prime at which E has good reduction. If E‚Ä≤ is an elliptic curve over Q satisfying\n\nE‚Ä≤ has good reduction at p and ¬ØœÅE‚Ä≤,p is isomorphic to ¬ØœÅE,p,\n\n‚Ä¢\n\nthen E‚Ä≤ is modular.\n\nProof of corollary. Let p be a prime of F containing p, and deÔ¨Åne = the ring of integers of the completion of F at p,\n\nO ‚Ä¢ ‚Ä¢ ‚Ä¢\n\n/p primes at which E or E‚Ä≤ has bad reduction\n\nk = Œ£ = t = ordinary if E has ordinary reduction at p, t = Ô¨Çat if E has supersingular reduction at p,\n\n,\n\nO {\n\nO\n\np\n\n,\n\n} ‚à™ {\n\n}\n\n= (Œ£, t).\n\nD\n\nLet\n\nœá : Gal( ¬ØQ/F )\n\nAutO(E[p‚àû]) ‚àº=\n\n√ó\n\n‚Üí\n\nO\n\nbe the character giving the action of Gal( ¬ØQ/F ) on E[p‚àû] (where E[p‚àû] is the group of points of E killed by the endomorphisms of E which lie in some power of p). It is not hard to see that œÅE,p ‚äó O\n\nis isomorphic to Indœá.\n\n20\n\nK. RUBIN AND A. SILVERBERG\n\nSince E has complex multiplication, it is well known that E and ¬ØœÅE,p are mod- ular. Since E has good reduction at p, it can be shown that the discriminant of contains no nontrivial p-th roots of unity. One can show F is prime to p and that all of the hypotheses of Theorem 5.4 are satisÔ¨Åed with ¬ØœÅ = ¬ØœÅE,p ‚äó k. By our assumptions on E‚Ä≤, œÅE‚Ä≤,p ‚äó O )-lifting of ¬ØœÅ, and we conclude (using the D same reasoning as in the proofs of Propositions 3.3 and 4.3) that œÅE‚Ä≤,p is modular and hence E‚Ä≤ is modular.\n\nO\n\nis a (\n\n,\n\nO\n\nRemarks. (i) The elliptic curves E‚Ä≤ of Corollary 5.5 are not semistable.\n\n(ii) Suppose E and p are as in Corollary 5.5 and p = 3 or 5. As in Appendix B.2 one can show that the elliptic curves E‚Ä≤ over Q with good reduction at p and with ¬ØœÅE‚Ä≤,p isomorphic to ¬ØœÅE,p give inÔ¨Ånitely many C-isomorphism classes.\n\nExample. Take E to be the elliptic curve deÔ¨Åned by\n\ny2 = x3\n\nx2\n\n3x\n\n1.\n\n‚àí\n\n‚àí\n\n‚àí\n\nThen E has complex multiplication by Q(‚àö DeÔ¨Åne polynomials\n\n2), and E has good reduction at 3.\n\n‚àí\n\n1512t3 3, a4(t) = a6(t) = 40824t6 + 31104t5 + 8370t4 + 504t3\n\n2430t4\n\n396t2\n\n56t\n\n‚àí\n\n‚àí\n\n‚àí\n\n‚àí\n\n‚àí\n\n148t2\n\n24t\n\n1,\n\n‚àí\n\n‚àí\n\n‚àí\n\nQ let Et be the elliptic curve\n\nand for each t\n\n‚àà\n\ny2 = x3\n\nx2 + a4(t)x + a6(t)\n\n‚àí\n\nQ, ¬ØœÅEt,3 is isomorphic to (note that E0 = E). It can be shown that for every t 0 or 1 (mod 3) (or more generally if t = 3a/b or t = 3a/b + 1 ¬ØœÅE,3. If t with a and b integers and b not divisible by 3), then Et has good reduction at 3, for instance because the discriminant of Et is\n\n‚àà\n\nZ and t\n\n‚àà\n\n‚â°\n\n29(27t2 + 10t + 1)3(27t2 + 18t + 1)3.\n\nThus for these values of t, Corollary 5.5 shows that Et is modular and so is any elliptic curve over Q isomorphic over C to Et, i.e., any elliptic curve over Q with j-invariant equal to\n\n3\n\n4(27t2 + 6t + 1)(135t2 + 54t + 5) (27t2 + 10t + 1)(27t2 + 18t + 1)\n\n.\n\n(cid:18)\n\n(cid:19)\n\nThis explicitly gives inÔ¨Ånitely many modular elliptic curves over Q which are\n\nnonisomorphic over C.\n\n(For deÔ¨Ånitions of complex multiplication, discriminant, and j-invariant, see any\n\nstandard reference on elliptic curves, such as [39].)\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n21\n\nAppendix A. Galois groups and Frobenius elements\n\nWrite GQ = Gal( ¬ØQ/Q). If q is a prime number and\n\nis a prime ideal dividing\n\nQ\n\nq in the ring of integers of ¬ØQ, there is a Ô¨Åltration\n\nGQ\n\nDQ ‚äÉ\n\nIQ\n\n‚äÉ where the decomposition group DQ and the inertia group IQ are deÔ¨Åned by\n\nDQ = IQ =\n\nœÉ\n\nGQ : œÉ ‚àà Q ‚àà DQ : œÉx\n\n=\n\n,\n\n{\n\nQ} x (mod\n\nœÉ\n\n) for all algebraic integers x }\n\n.\n\n‚â° { There are natural identiÔ¨Åcations\n\nQ\n\nDQ/IQ ‚àº= Gal( ¬ØFq/Fq),\n\nDQ ‚àº= Gal( ¬ØQq/Qq),\n\nxq of GQ\n\nand FrobQ ‚àà Gal( ¬ØFq/Fq). If and\n\nDQ/IQ denotes the inverse image of the canonical generator x\n\n7‚Üí for some œÉ\n\n‚Ä≤ is another prime ideal above q, then\n\n‚Ä≤ = œÉ\n\nQ DQ‚Ä≤ = œÉDQœÉ‚àí1,\n\nQ\n\nQ\n\n‚àà\n\nFrobQ‚Ä≤ = œÉFrobQœÉ‚àí1.\n\nIQ‚Ä≤ = œÉIQœÉ‚àí1,\n\nSince we will care about these objects only up to conjugation, we will write Dq and GQ for any representative of a FrobQ. If œÅ is a represen- Iq. We will write Frobq ‚àà tation of GQ which is unramiÔ¨Åed at q, then trace(œÅ(Frobq)) and det(œÅ(Frobq)) are well deÔ¨Åned independent of any choices.\n\nAppendix B. Some details on the proof of Proposition 2.4\n\nB.1. The modular curve X0(15) can be viewed as a curve deÔ¨Åned over Q in such a way that the noncusp rational points correspond to isomorphism classes (over C) E( ¬ØQ) is a subgroup of pairs (E‚Ä≤, 42), of order 15 stable under GQ. An equation for X0(15) is y2 = x(x + 32)(x the elliptic curve discussed in 1. There are eight rational points on X0(15), four of ¬ß which are cusps. There are four modular elliptic curves, corresponding to a modular form for Œì0(50) (see p. 86 of [1]), which lie in the four distinct C-isomorphism classes that correspond to the noncusp rational points on X0(15).\n\n) where E‚Ä≤ is an elliptic curve over Q and\n\nC\n\nC ‚äÇ\n\n‚àí\n\nTherefore every elliptic curve over Q with a GQ-stable subgroup of order 15 is modular. Equivalently, if E is an elliptic curve over Q and both ¬ØœÅE,3 and ¬ØœÅE,5 are reducible, then E is modular.\n\nB.2. Fix a semistable elliptic curve E over Q. We will show that there are inÔ¨Ånitely many semistable elliptic curves E‚Ä≤ over Q such that\n\n(i) ¬ØœÅE‚Ä≤,5 is isomorphic to ¬ØœÅE,5, and (ii) ¬ØœÅE‚Ä≤,3 is irreducible. Let\n\n1 0 0 1\n\na b c d\n\na b c d\n\nSL2(Z) :\n\n(mod 5) }\n\n.\n\nŒì(5) =\n\n‚â°\n\n‚àà\n\n{\n\nLet X be the twist of the classical modular curve X(5) (see [35]) by the cocycle (cid:0) induced by ¬ØœÅE,5, and let S be the set of cusps of X. Then X is a curve deÔ¨Åned over Q which has the following properties. The rational points on X ‚àí (E‚Ä≤, œÜ) where E‚Ä≤ is an elliptic curve over Q and œÜ : E[5] module isomorphism.\n\n(cid:1)\n\n(cid:0)\n\n(cid:1)\n\n(cid:1)\n\n(cid:0)\n\nS correspond to isomorphism classes of pairs E‚Ä≤[5] is a GQ-\n\n\n\n‚Üí\n\n22\n\nK. RUBIN AND A. SILVERBERG\n\nS is four copies of H/Œì(5), so each component of\n\nAs a complex manifold X X has genus zero.\n\n\n\n‚àí\n\nLet X 0 be the component of X containing the rational point corresponding to (E, identity). Then X 0 is a curve of genus zero deÔ¨Åned over Q with a rational point, so it has inÔ¨Ånitely many rational points. We want to show that inÔ¨Ånitely many of these points correspond to semistable elliptic curves E‚Ä≤ with ¬ØœÅE‚Ä≤,3 irreducible.\n\nThere is another modular curve ÀÜX deÔ¨Åned over Q, with a Ô¨Ånite set ÀÜS of cusps,\n\nwhich has the following properties. The rational points on ÀÜX (E‚Ä≤, œÜ, module isomorphism, and As a complex manifold ÀÜX The map that forgets the subgroup X deÔ¨Åned over Q and of degree [Œì(5) : Œì(5)\n\nÀÜS correspond to isomorphism classes of triples E‚Ä≤[5] is a GQ-\n\n\n\n‚àí\n\n) where E‚Ä≤ is an elliptic curve over Q, œÜ : E[5]\n\nC\n\n‚Üí\n\nE‚Ä≤[3] is a GQ-stable subgroup of order 3.\n\nC ‚äÇ ‚àí\n\nÀÜS is four copies of H/(Œì(5)\n\nŒì0(3)).\n\n‚Ä¢\n\n‚à© induces a surjective morphism Œ∏ : ÀÜX\n\nC\n\n‚Üí\n\nŒì0(3)] = 4.\n\n‚à©\n\nLet ÀÜX 0 be the component of ÀÜX which maps to X 0. The function Ô¨Åeld of X 0 is Q(t), and the function Ô¨Åeld of ÀÜX 0 is Q(t)[x]/f (t, x) where f (t, x) Q(t)[x] is irreducible and has degree 4 in x. If t‚Ä≤ Q is suÔ¨Éciently close 5-adically to the value of t which corresponds to E, then the corresponding elliptic curve is semistable at Q so that f (t1, x) is 5. By the Hilbert Irreducibility Theorem we can Ô¨Ånd a t1 ‚àà irreducible in Q[x]. It is possible to Ô¨Åx a prime ‚Ñì = 5 such that f (t1, x) has no roots modulo ‚Ñì. If t‚Ä≤ Q is suÔ¨Éciently close ‚Ñì-adically to t1, then f (t‚Ä≤, x) has no rational roots, and thus t‚Ä≤ corresponds to a rational point of X 0 which is not the image of a rational point of ÀÜX 0. Therefore there are inÔ¨Ånitely many elliptic curves E‚Ä≤ over Q which are semistable at 5 and satisfy\n\n‚àà\n\n‚àà\n\n6\n\n‚àà\n\n(i) E‚Ä≤[5] ‚àº= E[5] as GQ-modules, and (ii) E‚Ä≤[3] has no subgroup of order 3 stable under GQ.\n\nIt follows from (i) and the semistability of E that E‚Ä≤ is semistable at all primes = 5, and thus E‚Ä≤ is semistable. We therefore have inÔ¨Ånitely many semistable q elliptic curves E‚Ä≤ which satisfy the desired conditions.\n\n6\n\nAppendix C. Representation types\n\nSuppose A is a complete noetherian local Zp-algebra and œÅ : GQ\n\nGL2(A) is a |Dp for the restriction of œÅ to the decomposition group Dp.\n\n‚Üí\n\nrepresentation. Write œÅ We say œÅ is\n\nordinary at p if œÅ\n\n|Dp is (after a change of basis, if necessary) of the form Ô¨Çat at p if œÅ is not ordinary, and for every ideal a of Ô¨Ånite index in A, the (cid:0) |Dp modulo a is the representation associated to the ¬ØQp-points reduction of œÅ of a Ô¨Ånite Ô¨Çat group scheme over Zp.\n\n\n\n‚àó ‚àó 0 œá\n\nwhere œá is unramiÔ¨Åed and the * are functions from Dp to A;\n\n(cid:1)\n\n\n\nAppendix D. Selmer groups\n\nWith notation as in\n\n5 (see especially ¬ß\n\n5.2), deÔ¨Åne\n\n¬ß\n\n[«´]/(«´2, mn)\n\nOn =\n\nO\n\nA REPORT ON WILES‚Äô CAMBRIDGE LECTURES\n\n23\n\nwhere «´ is an indeterminate. Then v\n\n1 + «´v deÔ¨Ånes an isomorphism\n\n7‚Üí On) : Œ¥ GL2(\n\n‚àº ‚àà ‚Üí { HomO(pR/p2 R,\n\n(16)\n\n1 (mod «´) } /mn) there is a unique -algebra homomorphism ‚Üí On whose restriction to pR is «´Œ±. Composing with the representation œÅR On. (In particular œÅ0 )-lifting obtained when Œ± = 0.) DeÔ¨Åne a one-cocycle cŒ± on GQ\n\nŒ¥\n\n.\n\nVn\n\n‚â°\n\nFor every Œ±\n\nO\n\nO\n\n‚àà\n\nœàŒ± : R of Theorem 4.1 gives a ( denotes the ( by\n\n,\n\n)-lifting œÅŒ± = œàŒ± ‚ó¶\n\nœÅR of ¬ØœÅ to\n\nD\n\nO\n\n,\n\nD\n\nO\n\ncŒ±(g) = œÅŒ±(g)œÅ0(g)‚àí1.\n\nH 1(Q, Vn). This deÔ¨Ånes a\n\nSince œÅŒ± ‚â° homomorphism\n\nœÅ0 (mod «´), using (16) we can view cŒ± ‚àà\n\ns : HomO(pR/p2 R,\n\n/mn)\n\nH 1(Q, Vn),\n\nO and it is not diÔ¨Écult to see that s is injective. The fact that œÅ0 and œÅŒ± are type- D gives information about the restrictions resq(cŒ±) for various primes q, and using this H 1(Q, Vn) and veriÔ¨Åes that s information Wiles deÔ¨Ånes a Selmer group SD(Vn) is an isomorphism onto SD(Vn).\n\n‚Üí\n\n‚äÇ\n\nReferences\n\n[1] B. Birch and W. Kuyk, eds., Modular functions of one variable. IV, Lecture Notes in Math.,\n\nvol. 476, Springer-Verlag, New York, 1975, pp. 74‚Äì144.\n\n[2] J. Buhler, R. Crandall, R. Ernvall, and T. Mets¬®ankyl¬®a, Irregular primes and cyclotomic\n\ninvariants to four million, Math. Comp. 61 (1993), 151‚Äì153.\n\n[3] J. W. S. Cassels and A. Frohlich, Algebraic number theory, Academic Press, London, 1967. [4] P. Deligne and J.-P. Serre, Formes modulaires de poids 1, Ann. Sci. ¬¥Ecole Norm. Sup. (4) 7\n\n(1974), 507‚Äì530.\n\n[5] L. E. Dickson, History of the theory of numbers (Vol. II), Chelsea Publ. Co., New York, 1971. [6] H. M. Edwards, Fermat‚Äôs Last Theorem. A genetic introduction to algebraic number theory,\n\nSpringer-Verlag, New York, 1977.\n\n[7] M. Eichler, Quatern¬®are quadratische Formen und die Riemannsche Vermutung f¬®ur die Kon-\n\ngruenzzetafunktion, Arch. Math. (Basel) 5 (1954), 355‚Äì366.\n\n[8] G. Faltings, p-adic Hodge theory, J. Amer. Math. Soc. 1 (1988), 255‚Äì299. [9]\n\n, Crystalline cohomology and p-adic Galois representations, Algebraic Analysis, Ge- ometry and Number Theory, Proceedings of the JAMI Inaugural Conference (J. I. Igusa, ed.), Johns Hopkins Univ. Press, Baltimore, MD, 1989, pp. 25‚Äì80.\n\n[10] M. Flach, A Ô¨Åniteness theorem for the symmetric square of an elliptic curve, Invent. Math.\n\n109 (1992), 307‚Äì327.\n\n[11] G. Frey, Links between solutions of A ‚àí B = C and elliptic curves, Number Theory, Ulm 1987, Proceedings, Lecture Notes in Math., vol. 1380, Springer-Verlag, New York, 1989, pp. 31‚Äì62.\n\n[12] S. Gelbart, Automorphic forms on adele groups, Ann. of Math. Stud., vol. 83, Princeton\n\nUniv. Press, Princeton, NJ, 1975.\n\n[13] B. Gross, Kolyvagin‚Äôs work on modular elliptic curves, L-functions and Arithmetic, London Math. Soc. Lecture Note Ser., vol. 153, Cambridge Univ. Press, Cambridge, 1991, pp. 235‚Äì256. [14] G. H. Hardy and E. M. Wright, An introduction to the theory of numbers, Fourth ed., Oxford\n\nUniv. Press, London, 1971.\n\n[15] Y. Hellegouarch, ¬¥Etude des points d‚Äôordre Ô¨Åni des vari¬¥et¬¥es de dimension un d¬¥eÔ¨Ånies sur un\n\nanneau principal, J. Reine Angew. Math. 244 (1970), 20‚Äì36.\n\n, Points d‚Äôordre Ô¨Åni des vari¬¥et¬¥es ab¬¥eliennes de dimension un, Colloque de Th¬¥eorie des Nombres (Univ. Bordeaux, Bordeaux, 1969), Bull. Soc. Math. France, M¬¥em. 25, Soc. Math. France, Paris, 1971, pp. 107‚Äì112.\n\n[16]\n\n, Points d‚Äôordre Ô¨Åni sur les courbes elliptiques, C. R. Acad. Sci. Paris S¬¥er. A-B 273\n\n[17]\n\n(1971), A540‚ÄìA543.\n\n24\n\nK. RUBIN AND A. SILVERBERG\n\n, Points d‚Äôordre 2ph sur les courbes elliptiques, Acta. Arith. 26 (1974/75), 253‚Äì263. [18] [19] V. A. Kolyvagin, Euler systems, The Grothendieck Festschrift (Vol. II) (P. Cartier et al.,\n\neds.), Birkh¬®auser, Boston, 1990, pp. 435‚Äì483.\n\n[20] R. Langlands, Base change for GL(2), Ann. of Math. Stud., vol. 96, Princeton Univ. Press,\n\nPrinceton, NJ, 1980.\n\n[21] B. Mazur, Deforming Galois representations, Galois groups over Q (Y. Ihara, K. Ribet, and J.-P. Serre, eds.), Math. Sci. Res. Inst. Publ., vol. 16, Springer-Verlag, New York, 1989, pp. 385‚Äì437.\n\n, Number theory as gadÔ¨Çy, Amer. Math. Monthly 98 (1991), 593‚Äì610.\n\n[22] [23] B. Mazur and J. Tilouine, Repr¬¥esentations galoisiennes, diÔ¨Ä¬¥erentielles de K¬®ahler et ‚Äúconjec-\n\ntures principales‚Äù, Inst. Hautes ¬¥Etudes Sci. Publ. Math. 71 (1990), 65‚Äì103.\n\n[24] J. Oesterl¬¥e, Nouvelles approches du ‚Äúth¬¥eor`eme‚Äù de Fermat, S¬¥eminaire Bourbaki no. 694\n\n(1987‚Äì1988), Ast¬¥erisque 161/162 (1988) 165‚Äì186.\n\n, On a variation of Mazur ‚Äôs deformation functor, Compositio Math. 87 (1993), 269‚Äì\n\n[25]\n\n286.\n\n[26] P. Ribenboim, 13 lectures on Fermat ‚Äôs Last Theorem, Springer-Verlag, New York, 1979. [27] K. Ribet, On modular representations of Gal( ¬ØQ/Q) arising from modular forms, Invent.\n\nMath. 100 (1990), 431‚Äì476.\n\n, Report on mod ‚Ñì representations of Gal( ¬ØQ/Q), Motives (U. Jannsen, S. Kleiman, and J-P. Serre, eds.), Proc. Sympos. Pure Math., vol. 55 (Part 2), Amer. Math. Soc., Providence, RI, 1994 (to appear).\n\n[28]\n\n[29] K. Rubin, The main conjecture. (Appendix to Cyclotomic Ô¨Åelds I and II, S. Lang), Graduate\n\nTexts in Math., vol. 121, Springer-Verlag, New York, 1990, pp. 397‚Äì419.\n\n, Kolyvagin‚Äôs system of Gauss sums, Arithmetic Algebraic Geometry (G. van der Geer, F. Oort, and J. Steenbrink, eds.), Progr. Math., vol. 89, Birkh¬®auser, Boston, 1991, pp. 309‚Äì324.\n\n[30]\n\n, The ‚Äúmain conjectures‚Äù of Iwasawa theory for imaginary quadratic Ô¨Åelds, Invent.\n\n[31]\n\nMath. 103 (1991), 25‚Äì68.\n\n[32] J.-P. Serre, Sur les repr¬¥esentations modulaires de degr¬¥e 2 de Gal( ¬ØQ/Q), Duke Math. J. 54\n\n(1987), 179‚Äì230.\n\n[33] G. Shimura, Correspondances modulaires et les fonctions Œ∂ de courbes alg¬¥ebriques, J. Math.\n\nSoc. Japan 10 (1958), 1‚Äì28.\n\n, Construction of class Ô¨Åelds and zeta functions of algebraic curves, Ann. of Math.\n\n[34]\n\n85 (1967), 58‚Äì159.\n\n, Introduction to the arithmetic theory of automorphic functions, Princeton Univ.\n\n[35]\n\nPress, Princeton, NJ, 1971.\n\n, On elliptic curves with complex multiplication as factors of the Jacobians of modular\n\n[36]\n\nfunction Ô¨Åelds, Nagoya Math. J. 43 (1971), 199‚Äì208.\n\n, On the factors of the jacobian variety of a modular function Ô¨Åeld, J. Math. Soc.\n\n[37]\n\nJapan 25 (1973), 523‚Äì544.\n\n, Yutaka Taniyama and his time. Very personal recollections, Bull. London Math.\n\n[38]\n\nSoc. 21 (1989), 186‚Äì196.\n\n[39] J. Silverman, The arithmetic of elliptic curves, Graduate Texts in Math., vol. 106, Springer-\n\nVerlag, New York, 1986.\n\n[40] F. Thaine, On the ideal class groups of real abelian number Ô¨Åelds, Ann. of Math. (2) 128\n\n(1988), 1‚Äì18.\n\n[41] J. Tunnell, Artin‚Äôs conjecture for representations of octahedral type, Bull. Amer. Math. Soc.\n\n(N.S.) 5 (1981), 173‚Äì175.\n\n[42] A. Weil, ¬®Uber die Bestimmung Dirichletscher Reihen durch Funktionalgleichungen, Math.\n\nAnn. 168 (1967), 149‚Äì156.\n\nDepartment of Mathematics, Ohio State University, Columbus, Ohio 43210 E-mail address: rubin@math.ohio-state.edu\n\nDepartment of Mathematics, Ohio State University, Columbus, Ohio 43210 E-mail address: silver@math.ohio-state.edu' metadata={'source': '/var/folders/l1/lphj87z16c3282pjwy91wtm80000gn/T/tmpdh5kk5yb/tmp.pdf'}page_content='This is text file' metadata={'source': 'dropbox:///test.txt', 'title': 'test.txt'}",en,
https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory,Google Cloud Storage Directory | ü¶úÔ∏èüîó Langchain,[Google Cloud,"Google Cloud Storage DirectoryGoogle Cloud\nStorage is a\nmanaged service for storing unstructured data.This covers how to load document objects from an\nGoogle Cloud Storage (GCS) directory (bucket).%pip install --upgrade --quiet  google-cloud-storagefrom langchain_community.document_loaders import GCSDirectoryLoaderloader = GCSDirectoryLoader(project_name=""aist"", bucket=""testing-hwc"")loader.load()/Users/harrisonchase/workplace/langchain/.venv/lib/python3.10/site-packages/google/auth/_default.py:83: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a ""quota exceeded"" or ""API not enabled"" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)/Users/harrisonchase/workplace/langchain/.venv/lib/python3.10/site-packages/google/auth/_default.py:83: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a ""quota exceeded"" or ""API not enabled"" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)[Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmpz37njh7u/fake.docx'}, lookup_index=0)]Specifying a prefix‚ÄãYou can also specify a prefix for more finegrained control over what\nfiles to load -including loading all files from a specific folder-.loader = GCSDirectoryLoader(project_name=""aist"", bucket=""testing-hwc"", prefix=""fake"")loader.load()/Users/harrisonchase/workplace/langchain/.venv/lib/python3.10/site-packages/google/auth/_default.py:83: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a ""quota exceeded"" or ""API not enabled"" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)/Users/harrisonchase/workplace/langchain/.venv/lib/python3.10/site-packages/google/auth/_default.py:83: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a ""quota exceeded"" or ""API not enabled"" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)[Document(page_content='Lorem ipsum dolor sit amet.', lookup_str='', metadata={'source': '/var/folders/y6/8_bzdg295ld6s1_97_12m4lr0000gn/T/tmpylg6291i/fake.docx'}, lookup_index=0)]",en,
https://python.langchain.com/docs/integrations/document_loaders/duckdb,DuckDB | ü¶úÔ∏èüîó Langchain,DuckDB is an in-process SQL OLAP database,"DuckDBDuckDB is an in-process SQL OLAP database\nmanagement system.Load a DuckDB query with one document per row.%pip install --upgrade --quiet  duckdbfrom langchain_community.document_loaders import DuckDBLoader%%file example.csvTeam,PayrollNationals,81.34Reds,82.20Writing example.csvloader = DuckDBLoader(""SELECT * FROM read_csv_auto('example.csv')"")data = loader.load()print(data)[Document(page_content='Team: Nationals\nPayroll: 81.34', metadata={}), Document(page_content='Team: Reds\nPayroll: 82.2', metadata={})]Specifying Which Columns are Content vs Metadata‚Äãloader = DuckDBLoader(    ""SELECT * FROM read_csv_auto('example.csv')"",    page_content_columns=[""Team""],    metadata_columns=[""Payroll""],)data = loader.load()print(data)[Document(page_content='Team: Nationals', metadata={'Payroll': 81.34}), Document(page_content='Team: Reds', metadata={'Payroll': 82.2})]Adding Source to Metadata‚Äãloader = DuckDBLoader(    ""SELECT Team, Payroll, Team As source FROM read_csv_auto('example.csv')"",    metadata_columns=[""source""],)data = loader.load()print(data)[Document(page_content='Team: Nationals\nPayroll: 81.34\nsource: Nationals', metadata={'source': 'Nationals'}), Document(page_content='Team: Reds\nPayroll: 82.2\nsource: Reds', metadata={'source': 'Reds'})]",en,
https://python.langchain.com/docs/integrations/document_loaders/figma,Figma | ü¶úÔ∏èüîó Langchain,Figma is a collaborative web application for,"FigmaFigma is a collaborative web application for\ninterface design.This notebook covers how to load data from the Figma REST API into a\nformat that can be ingested into LangChain, along with example usage for\ncode generation.import osfrom langchain.indexes import VectorstoreIndexCreatorfrom langchain.prompts.chat import (    ChatPromptTemplate,    HumanMessagePromptTemplate,    SystemMessagePromptTemplate,)from langchain_community.document_loaders.figma import FigmaFileLoaderfrom langchain_openai import ChatOpenAIThe Figma API Requires an access token, node_ids, and a file key.The file key can be pulled from the URL.\nhttps://www.figma.com/file/{filekey}/sampleFilenameNode IDs are also available in the URL. Click on anything and look for\nthe ‚Äò?node-id={node_id}‚Äô param.Access token instructions are in the Figma help center article:\nhttps://help.figma.com/hc/en-us/articles/8085703771159-Manage-personal-access-tokensfigma_loader = FigmaFileLoader(    os.environ.get(""ACCESS_TOKEN""),    os.environ.get(""NODE_IDS""),    os.environ.get(""FILE_KEY""),)# see https://python.langchain.com/en/latest/modules/data_connection/getting_started.html for more detailsindex = VectorstoreIndexCreator().from_loaders([figma_loader])figma_doc_retriever = index.vectorstore.as_retriever()def generate_code(human_input):    # I have no idea if the Jon Carmack thing makes for better code. YMMV.    # See https://python.langchain.com/en/latest/modules/models/chat/getting_started.html for chat info    system_prompt_template = """"""You are expert coder Jon Carmack. Use the provided design context to create idiomatic HTML/CSS code as possible based on the user request.    Everything must be inline in one file and your response must be directly renderable by the browser.    Figma file nodes and metadata: {context}""""""    human_prompt_template = ""Code the {text}. Ensure it's mobile responsive""    system_message_prompt = SystemMessagePromptTemplate.from_template(        system_prompt_template    )    human_message_prompt = HumanMessagePromptTemplate.from_template(        human_prompt_template    )    # delete the gpt-4 model_name to use the default gpt-3.5 turbo for faster results    gpt_4 = ChatOpenAI(temperature=0.02, model_name=""gpt-4"")    # Use the retriever's 'get_relevant_documents' method if needed to filter down longer docs    relevant_nodes = figma_doc_retriever.get_relevant_documents(human_input)    conversation = [system_message_prompt, human_message_prompt]    chat_prompt = ChatPromptTemplate.from_messages(conversation)    response = gpt_4(        chat_prompt.format_prompt(            context=relevant_nodes, text=human_input        ).to_messages()    )    return responseresponse = generate_code(""page top header"")Returns the following in response.content:<!DOCTYPE html>\n<html lang=""en"">\n<head>\n    <meta charset=""UTF-8"">\n    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">\n    <style>\n        @import url(\'https://fonts.googleapis.com/css2?family=DM+Sans:wght@500;700&family=Inter:wght@600&display=swap\');\n\n        body {\n            margin: 0;\n            font-family: \'DM Sans\', sans-serif;\n        }\n\n        .header {\n            display: flex;\n            justify-content: space-between;\n            align-items: center;\n            padding: 20px;\n            background-color: #fff;\n            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n        }\n\n        .header h1 {\n            font-size: 16px;\n            font-weight: 700;\n            margin: 0;\n        }\n\n        .header nav {\n            display: flex;\n            align-items: center;\n        }\n\n        .header nav a {\n            font-size: 14px;\n            font-weight: 500;\n            text-decoration: none;\n            color: #000;\n            margin-left: 20px;\n        }\n\n        @media (max-width: 768px) {\n            .header nav {\n                display: none;\n            }\n        }\n    </style>\n</head>\n<body>\n    <header class=""header"">\n        <h1>Company Contact</h1>\n        <nav>\n            <a href=""#"">Lorem Ipsum</a>\n            <a href=""#"">Lorem Ipsum</a>\n            <a href=""#"">Lorem Ipsum</a>\n        </nav>\n    </header>\n</body>\n</html>",en,
https://python.langchain.com/docs/integrations/document_loaders/docugami,Docugami | ü¶úÔ∏èüîó Langchain,This notebook covers how to load documents from Docugami. It provides,"DocugamiThis notebook covers how to load documents from Docugami. It provides\nthe advantages of using this system over alternative data loaders.Prerequisites‚ÄãInstall necessary python packages.Grab an access token for your workspace, and make sure it is set as\nthe DOCUGAMI_API_KEY environment variable.Grab some docset and document IDs for your processed documents, as\ndescribed here: https://help.docugami.com/home/docugami-api# You need the dgml-utils package to use the DocugamiLoader (run pip install directly without ""poetry run"" if you are not using poetry)!poetry run pip install docugami-langchain dgml-utils==0.3.0 --upgrade --quietQuick start‚ÄãCreate a Docugami workspace (free trials\navailable)Add your documents (PDF, DOCX or DOC) and allow Docugami to ingest\nand cluster them into sets of similar documents, e.g.¬†NDAs, Lease\nAgreements, and Service Agreements. There is no fixed set of\ndocument types supported by the system, the clusters created depend\non your particular documents, and you can change the docset\nassignments\nlater.Create an access token via the Developer Playground for your\nworkspace. Detailed\ninstructionsExplore the Docugami API to get a\nlist of your processed docset IDs, or just the document IDs for a\nparticular docset.Use the DocugamiLoader as detailed below, to get rich semantic\nchunks for your documents.Optionally, build and publish one or more reports or\nabstracts. This helps\nDocugami improve the semantic XML with better tags based on your\npreferences, which are then added to the DocugamiLoader output as\nmetadata. Use techniques like self-querying\nretriever to\ndo high accuracy Document QA.Advantages vs Other Chunking Techniques‚ÄãAppropriate chunking of your documents is critical for retrieval from\ndocuments. Many chunking techniques exist, including simple ones that\nrely on whitespace and recursive chunk splitting based on character\nlength. Docugami offers a different approach:Intelligent Chunking: Docugami breaks down every document into a\nhierarchical semantic XML tree of chunks of varying sizes, from\nsingle words or numerical values to entire sections. These chunks\nfollow the semantic contours of the document, providing a more\nmeaningful representation than arbitrary length or simple\nwhitespace-based chunking.Semantic Annotations: Chunks are annotated with semantic tags\nthat are coherent across the document set, facilitating consistent\nhierarchical queries across multiple documents, even if they are\nwritten and formatted differently. For example, in set of lease\nagreements, you can easily identify key provisions like the\nLandlord, Tenant, or Renewal Date, as well as more complex\ninformation such as the wording of any sub-lease provision or\nwhether a specific jurisdiction has an exception section within a\nTermination Clause.Structured Representation: In addition, the XML tree indicates\nthe structural contours of every document, using attributes denoting\nheadings, paragraphs, lists, tables, and other common elements, and\ndoes that consistently across all supported document formats, such\nas scanned PDFs or DOCX files. It appropriately handles long-form\ndocument characteristics like page headers/footers or multi-column\nflows for clean text extraction.Additional Metadata: Chunks are also annotated with additional\nmetadata, if a user has been using Docugami. This additional\nmetadata can be used for high-accuracy Document QA without context\nwindow restrictions. See detailed code walk-through below.import osfrom docugami_langchain.document_loaders import DocugamiLoaderLoad Documents‚ÄãIf the DOCUGAMI_API_KEY environment variable is set, there is no need to\npass it in to the loader explicitly otherwise you can pass it in as the\naccess_token parameter.DOCUGAMI_API_KEY = os.environ.get(""DOCUGAMI_API_KEY"")docset_id = ""26xpy3aes7xp""document_ids = [""d7jqdzcj50sj"", ""cgd1eacfkchw""]# To load all docs in the given docset ID, just don't provide document_idsloader = DocugamiLoader(docset_id=docset_id, document_ids=document_ids)chunks = loader.load()len(chunks)120The metadata for each Document (really, a chunk of an actual PDF,\nDOC or DOCX) contains some useful additional information:id and source: ID and Name of the file (PDF, DOC or DOCX) the\nchunk is sourced from within Docugami.xpath: XPath inside the XML representation of the document, for\nthe chunk. Useful for source citations directly to the actual chunk\ninside the document XML.structure: Structural attributes of the chunk, e.g.¬†h1, h2, div,\ntable, td, etc. Useful to filter out certain kinds of chunks if\nneeded by the caller.tag: Semantic tag for the chunk, using various generative and\nextractive techniques. More details here:\nhttps://github.com/docugami/DFM-benchmarksYou can control chunking behavior by setting the following properties on\nthe DocugamiLoader instance:You can set min and max chunk size, which the system tries to adhere\nto with minimal truncation. You can set loader.min_text_length and\nloader.max_text_length to control these.By default, only the text for chunks is returned. However,\nDocugami‚Äôs XML knowledge graph has additional rich information\nincluding semantic tags for entities inside the chunk. Set\nloader.include_xml_tags = True if you want the additional xml\nmetadata on the returned chunks.In addition, you can set loader.parent_hierarchy_levels if you\nwant Docugami to return parent chunks in the chunks it returns. The\nchild chunks point to the parent chunks via the\nloader.parent_id_key value. This is useful e.g.¬†with the\nMultiVector\nRetriever\nfor small-to-big\nretrieval. See detailed example later in this notebook.loader.min_text_length = 64loader.include_xml_tags = Truechunks = loader.load()for chunk in chunks[:5]:    print(chunk)page_content='MASTER SERVICES AGREEMENT\n <ThisServicesAgreement> This Services Agreement (the ‚ÄúAgreement‚Äù) sets forth terms under which <Company>MagicSoft, Inc. </Company>a <Org><USState>Washington </USState>Corporation </Org>(‚ÄúCompany‚Äù) located at <CompanyAddress><CompanyStreetAddress><Company>600 </Company><Company>4th Ave</Company></CompanyStreetAddress>, <Company>Seattle</Company>, <Client>WA </Client><ProvideServices>98104 </ProvideServices></CompanyAddress>shall provide services to <Client>Daltech, Inc.</Client>, a <Company><USState>Washington </USState>Corporation </Company>(the ‚ÄúClient‚Äù) located at <ClientAddress><ClientStreetAddress><Client>701 </Client><Client>1st St</Client></ClientStreetAddress>, <Client>Kirkland</Client>, <State>WA </State><Client>98033</Client></ClientAddress>. This Agreement is effective as of <EffectiveDate>February 15, 2021 </EffectiveDate>(‚ÄúEffective Date‚Äù). </ThisServicesAgreement>' metadata={'xpath': '/dg:chunk/docset:MASTERSERVICESAGREEMENT-section/dg:chunk', 'id': 'c28554d0af5114e2b102e6fc4dcbbde5', 'name': 'Master Services Agreement - Daltech.docx', 'source': 'Master Services Agreement - Daltech.docx', 'structure': 'h1 p', 'tag': 'chunk ThisServicesAgreement', 'Liability': '', 'Workers Compensation Insurance': '$1,000,000', 'Limit': '$1,000,000', 'Commercial General Liability Insurance': '$2,000,000', 'Technology Professional Liability Errors Omissions Policy': '$5,000,000', 'Excess Liability Umbrella Coverage': '$9,000,000', 'Client': 'Daltech, Inc.', 'Services Agreement Date': 'INITIAL STATEMENT  OF WORK (SOW)  The purpose of this SOW is to describe the Software and Services that Company will initially provide to  Daltech, Inc.  the ‚ÄúClient‚Äù) under the terms and conditions of the  Services Agreement  entered into between the parties on  June 15, 2021', 'Completion of the Services by Company Date': 'February 15, 2022', 'Charge': 'one hundred percent (100%)', 'Company': 'MagicSoft, Inc.', 'Effective Date': 'February 15, 2021', 'Start Date': '03/15/2021', 'Scheduled Onsite Visits Are Cancelled': 'ten (10) working days', 'Limit on Liability': '', 'Liability Cap': '', 'Business Automobile Liability': 'Business Automobile Liability  covering all vehicles that Company owns, hires or leases with a limit of no less than  $1,000,000  (combined single limit for bodily injury and property damage) for each accident.', 'Contractual Liability Coverage': 'Commercial General Liability insurance including  Contractual Liability Coverage , with coverage for products liability, completed operations, property damage and bodily injury, including  death , with an aggregate limit of no less than  $2,000,000 . This policy shall name Client as an additional insured with respect to the provision of services provided under this Agreement. This policy shall include a waiver of subrogation against Client.', 'Technology Professional Liability Errors Omissions': 'Technology Professional Liability Errors & Omissions policy (which includes Cyber Risk coverage and Computer Security and Privacy Liability coverage) with a limit of no less than  $5,000,000  per occurrence and in the aggregate.'}page_content='A. STANDARD SOFTWARE AND SERVICES AGREEMENT\n 1. Deliverables.\n Company shall provide Client with software, technical support, product management, development, and <_testRef>testing </_testRef>services (‚ÄúServices‚Äù) to the Client as described on one or more Statements of Work signed by Company and Client that reference this Agreement (‚ÄúSOW‚Äù or ‚ÄúStatement of Work‚Äù). Company shall perform Services in a prompt manner and have the final product or service (‚ÄúDeliverable‚Äù) ready for Client no later than the due date specified in the applicable SOW (‚ÄúCompletion Date‚Äù). This due date is subject to change in accordance with the Change Order process defined in the applicable SOW. Client shall assist Company by promptly providing all information requests known or available and relevant to the Services in a timely manner.' metadata={'xpath': '/dg:chunk/docset:MASTERSERVICESAGREEMENT-section/docset:MASTERSERVICESAGREEMENT/dg:chunk[1]/docset:Standard/dg:chunk[1]/dg:chunk[1]', 'id': 'de60160d328df10fa2637637c803d2d4', 'name': 'Master Services Agreement - Daltech.docx', 'source': 'Master Services Agreement - Daltech.docx', 'structure': 'lim h1 lim h1 div', 'tag': 'chunk', 'Liability': '', 'Workers Compensation Insurance': '$1,000,000', 'Limit': '$1,000,000', 'Commercial General Liability Insurance': '$2,000,000', 'Technology Professional Liability Errors Omissions Policy': '$5,000,000', 'Excess Liability Umbrella Coverage': '$9,000,000', 'Client': 'Daltech, Inc.', 'Services Agreement Date': 'INITIAL STATEMENT  OF WORK (SOW)  The purpose of this SOW is to describe the Software and Services that Company will initially provide to  Daltech, Inc.  the ‚ÄúClient‚Äù) under the terms and conditions of the  Services Agreement  entered into between the parties on  June 15, 2021', 'Completion of the Services by Company Date': 'February 15, 2022', 'Charge': 'one hundred percent (100%)', 'Company': 'MagicSoft, Inc.', 'Effective Date': 'February 15, 2021', 'Start Date': '03/15/2021', 'Scheduled Onsite Visits Are Cancelled': 'ten (10) working days', 'Limit on Liability': '', 'Liability Cap': '', 'Business Automobile Liability': 'Business Automobile Liability  covering all vehicles that Company owns, hires or leases with a limit of no less than  $1,000,000  (combined single limit for bodily injury and property damage) for each accident.', 'Contractual Liability Coverage': 'Commercial General Liability insurance including  Contractual Liability Coverage , with coverage for products liability, completed operations, property damage and bodily injury, including  death , with an aggregate limit of no less than  $2,000,000 . This policy shall name Client as an additional insured with respect to the provision of services provided under this Agreement. This policy shall include a waiver of subrogation against Client.', 'Technology Professional Liability Errors Omissions': 'Technology Professional Liability Errors & Omissions policy (which includes Cyber Risk coverage and Computer Security and Privacy Liability coverage) with a limit of no less than  $5,000,000  per occurrence and in the aggregate.'}page_content='2. Onsite Services.\n 2.1 Onsite visits will be charged on a <Frequency>daily </Frequency>basis (minimum <OnsiteVisits>8 hours</OnsiteVisits>).' metadata={'xpath': '/dg:chunk/docset:MASTERSERVICESAGREEMENT-section/docset:MASTERSERVICESAGREEMENT/dg:chunk[1]/docset:Standard/dg:chunk[3]/dg:chunk[1]', 'id': 'db18315b437ac2de6b555d2d8ef8f893', 'name': 'Master Services Agreement - Daltech.docx', 'source': 'Master Services Agreement - Daltech.docx', 'structure': 'lim h1 lim p', 'tag': 'chunk', 'Liability': '', 'Workers Compensation Insurance': '$1,000,000', 'Limit': '$1,000,000', 'Commercial General Liability Insurance': '$2,000,000', 'Technology Professional Liability Errors Omissions Policy': '$5,000,000', 'Excess Liability Umbrella Coverage': '$9,000,000', 'Client': 'Daltech, Inc.', 'Services Agreement Date': 'INITIAL STATEMENT  OF WORK (SOW)  The purpose of this SOW is to describe the Software and Services that Company will initially provide to  Daltech, Inc.  the ‚ÄúClient‚Äù) under the terms and conditions of the  Services Agreement  entered into between the parties on  June 15, 2021', 'Completion of the Services by Company Date': 'February 15, 2022', 'Charge': 'one hundred percent (100%)', 'Company': 'MagicSoft, Inc.', 'Effective Date': 'February 15, 2021', 'Start Date': '03/15/2021', 'Scheduled Onsite Visits Are Cancelled': 'ten (10) working days', 'Limit on Liability': '', 'Liability Cap': '', 'Business Automobile Liability': 'Business Automobile Liability  covering all vehicles that Company owns, hires or leases with a limit of no less than  $1,000,000  (combined single limit for bodily injury and property damage) for each accident.', 'Contractual Liability Coverage': 'Commercial General Liability insurance including  Contractual Liability Coverage , with coverage for products liability, completed operations, property damage and bodily injury, including  death , with an aggregate limit of no less than  $2,000,000 . This policy shall name Client as an additional insured with respect to the provision of services provided under this Agreement. This policy shall include a waiver of subrogation against Client.', 'Technology Professional Liability Errors Omissions': 'Technology Professional Liability Errors & Omissions policy (which includes Cyber Risk coverage and Computer Security and Privacy Liability coverage) with a limit of no less than  $5,000,000  per occurrence and in the aggregate.'}page_content='2.2 <Expenses>Time and expenses will be charged based on actuals unless otherwise described in an Order Form or accompanying SOW. </Expenses>' metadata={'xpath': '/dg:chunk/docset:MASTERSERVICESAGREEMENT-section/docset:MASTERSERVICESAGREEMENT/dg:chunk[1]/docset:Standard/dg:chunk[3]/dg:chunk[2]/docset:ADailyBasis/dg:chunk[2]/dg:chunk', 'id': '506220fa472d5c48c8ee3db78c1122c1', 'name': 'Master Services Agreement - Daltech.docx', 'source': 'Master Services Agreement - Daltech.docx', 'structure': 'lim p', 'tag': 'chunk Expenses', 'Liability': '', 'Workers Compensation Insurance': '$1,000,000', 'Limit': '$1,000,000', 'Commercial General Liability Insurance': '$2,000,000', 'Technology Professional Liability Errors Omissions Policy': '$5,000,000', 'Excess Liability Umbrella Coverage': '$9,000,000', 'Client': 'Daltech, Inc.', 'Services Agreement Date': 'INITIAL STATEMENT  OF WORK (SOW)  The purpose of this SOW is to describe the Software and Services that Company will initially provide to  Daltech, Inc.  the ‚ÄúClient‚Äù) under the terms and conditions of the  Services Agreement  entered into between the parties on  June 15, 2021', 'Completion of the Services by Company Date': 'February 15, 2022', 'Charge': 'one hundred percent (100%)', 'Company': 'MagicSoft, Inc.', 'Effective Date': 'February 15, 2021', 'Start Date': '03/15/2021', 'Scheduled Onsite Visits Are Cancelled': 'ten (10) working days', 'Limit on Liability': '', 'Liability Cap': '', 'Business Automobile Liability': 'Business Automobile Liability  covering all vehicles that Company owns, hires or leases with a limit of no less than  $1,000,000  (combined single limit for bodily injury and property damage) for each accident.', 'Contractual Liability Coverage': 'Commercial General Liability insurance including  Contractual Liability Coverage , with coverage for products liability, completed operations, property damage and bodily injury, including  death , with an aggregate limit of no less than  $2,000,000 . This policy shall name Client as an additional insured with respect to the provision of services provided under this Agreement. This policy shall include a waiver of subrogation against Client.', 'Technology Professional Liability Errors Omissions': 'Technology Professional Liability Errors & Omissions policy (which includes Cyber Risk coverage and Computer Security and Privacy Liability coverage) with a limit of no less than  $5,000,000  per occurrence and in the aggregate.'}page_content='2.3 <RegularWorkingHours>All work will be executed during regular working hours <RegularWorkingHours>Monday</RegularWorkingHours>-<Weekday>Friday </Weekday><RegularWorkingHours><RegularWorkingHours>0800</RegularWorkingHours>-<Number>1900</Number></RegularWorkingHours>. For work outside of these hours on weekdays, Company will charge <Charge>one hundred percent (100%) </Charge>of the regular hourly rate and <Charge>two hundred percent (200%) </Charge>for Saturdays, Sundays and public holidays applicable to Company. </RegularWorkingHours>' metadata={'xpath': '/dg:chunk/docset:MASTERSERVICESAGREEMENT-section/docset:MASTERSERVICESAGREEMENT/dg:chunk[1]/docset:Standard/dg:chunk[3]/dg:chunk[2]/docset:ADailyBasis/dg:chunk[3]/dg:chunk', 'id': 'dac7a3ded61b5c4f3e59771243ea46c1', 'name': 'Master Services Agreement - Daltech.docx', 'source': 'Master Services Agreement - Daltech.docx', 'structure': 'lim p', 'tag': 'chunk RegularWorkingHours', 'Liability': '', 'Workers Compensation Insurance': '$1,000,000', 'Limit': '$1,000,000', 'Commercial General Liability Insurance': '$2,000,000', 'Technology Professional Liability Errors Omissions Policy': '$5,000,000', 'Excess Liability Umbrella Coverage': '$9,000,000', 'Client': 'Daltech, Inc.', 'Services Agreement Date': 'INITIAL STATEMENT  OF WORK (SOW)  The purpose of this SOW is to describe the Software and Services that Company will initially provide to  Daltech, Inc.  the ‚ÄúClient‚Äù) under the terms and conditions of the  Services Agreement  entered into between the parties on  June 15, 2021', 'Completion of the Services by Company Date': 'February 15, 2022', 'Charge': 'one hundred percent (100%)', 'Company': 'MagicSoft, Inc.', 'Effective Date': 'February 15, 2021', 'Start Date': '03/15/2021', 'Scheduled Onsite Visits Are Cancelled': 'ten (10) working days', 'Limit on Liability': '', 'Liability Cap': '', 'Business Automobile Liability': 'Business Automobile Liability  covering all vehicles that Company owns, hires or leases with a limit of no less than  $1,000,000  (combined single limit for bodily injury and property damage) for each accident.', 'Contractual Liability Coverage': 'Commercial General Liability insurance including  Contractual Liability Coverage , with coverage for products liability, completed operations, property damage and bodily injury, including  death , with an aggregate limit of no less than  $2,000,000 . This policy shall name Client as an additional insured with respect to the provision of services provided under this Agreement. This policy shall include a waiver of subrogation against Client.', 'Technology Professional Liability Errors Omissions': 'Technology Professional Liability Errors & Omissions policy (which includes Cyber Risk coverage and Computer Security and Privacy Liability coverage) with a limit of no less than  $5,000,000  per occurrence and in the aggregate.'}Basic Use: Docugami Loader for Document QA‚ÄãYou can use the Docugami Loader like a standard loader for Document QA\nover multiple docs, albeit with much better chunks that follow the\nnatural contours of the document. There are many great tutorials on how\nto do this, e.g.¬†this\none. We can just use the\nsame code, but use the DocugamiLoader for better chunking, instead of\nloading text or PDF files directly with basic splitting techniques.!poetry run pip install --upgrade langchain-openai tiktoken chromadb hnswlib# For this example, we already have a processed docset for a set of lease documentsloader = DocugamiLoader(docset_id=""zo954yqy53wp"")chunks = loader.load()# strip semantic metadata intentionally, to test how things work without semantic metadatafor chunk in chunks:    stripped_metadata = chunk.metadata.copy()    for key in chunk.metadata:        if key not in [""name"", ""xpath"", ""id"", ""structure""]:            # remove semantic metadata            del stripped_metadata[key]    chunk.metadata = stripped_metadataprint(len(chunks))4674The documents returned by the loader are already split, so we don‚Äôt need\nto use a text splitter. Optionally, we can use the metadata on each\ndocument, for example the structure or tag attributes, to do any\npost-processing we want.We will just use the output of the DocugamiLoader as-is to set up a\nretrieval QA chain the usual way.from langchain.chains import RetrievalQAfrom langchain_community.vectorstores.chroma import Chromafrom langchain_openai import OpenAI, OpenAIEmbeddingsembedding = OpenAIEmbeddings()vectordb = Chroma.from_documents(documents=chunks, embedding=embedding)retriever = vectordb.as_retriever()qa_chain = RetrievalQA.from_chain_type(    llm=OpenAI(), chain_type=""stuff"", retriever=retriever, return_source_documents=True)# Try out the retriever with an example queryqa_chain(""What can tenants do with signage on their properties?""){'query': 'What can tenants do with signage on their properties?', 'result': ' Tenants can place or attach signage (digital or otherwise) to their property after receiving written permission from the landlord, which permission shall not be unreasonably withheld. The signage must conform to all applicable laws, ordinances, etc. governing the same, and tenants must remove all such signs by the termination of the lease.', 'source_documents': [Document(page_content='6.01 Signage. Tenant may place or attach to the Premises signs (digital or otherwise) or other such identification as needed after receiving written permission from the Landlord, which permission shall not be unreasonably withheld. Any damage caused to the Premises by the Tenant‚Äôs erecting or removing such signs shall be repaired promptly by the Tenant at the Tenant‚Äôs expense. Any signs or other form of identification allowed must conform to all applicable laws, ordinances, etc. governing the same. Tenant also agrees to have any window or glass identification completely removed and cleaned at its expense promptly upon vacating the Premises. ARTICLE VII UTILITIES', metadata={'id': '1c290eea05915ba0f24c4a1ffc05d6f3', 'name': 'Sample Commercial Leases/TruTone Lane 6.pdf', 'structure': 'lim h1', 'xpath': '/dg:chunk/dg:chunk/dg:chunk[2]/dg:chunk[1]/docset:TheApprovedUse/dg:chunk[12]/dg:chunk[1]'}),  Document(page_content='6.01 Signage. Tenant may place or attach to the Premises signs (digital or otherwise) or other such identification as needed after receiving written permission from the Landlord, which permission shall not be unreasonably withheld. Any damage caused to the Premises by the Tenant‚Äôs erecting or removing such signs shall be repaired promptly by the Tenant at the Tenant‚Äôs expense. Any signs or other form of identification allowed must conform to all applicable laws, ordinances, etc. governing the same. Tenant also agrees to have any window or glass identification completely removed and cleaned at its expense promptly upon vacating the Premises. ARTICLE VII UTILITIES', metadata={'id': '1c290eea05915ba0f24c4a1ffc05d6f3', 'name': 'Sample Commercial Leases/TruTone Lane 2.pdf', 'structure': 'lim h1', 'xpath': '/dg:chunk/dg:chunk/dg:chunk[2]/dg:chunk[1]/docset:TheApprovedUse/dg:chunk[12]/dg:chunk[1]'}),  Document(page_content='Tenant may place or attach to the Premises signs (digital or otherwise) or other such identification as needed after receiving written permission from the Landlord, which permission shall not be unreasonably withheld. Any damage caused to the Premises by the Tenant‚Äôs erecting or removing such signs shall be repaired promptly by the Tenant at the Tenant‚Äôs expense. Any signs or other form of identification allowed must conform to all applicable laws, ordinances, etc. governing the same. Tenant also agrees to have any window or glass identification completely removed and cleaned at its expense promptly upon vacating the Premises.', metadata={'id': '58d268162ecc36d8633b7bc364afcb8c', 'name': 'Sample Commercial Leases/TruTone Lane 2.docx', 'structure': 'div', 'xpath': '/docset:OFFICELEASEAGREEMENT-section/docset:OFFICELEASEAGREEMENT/dg:chunk/docset:ARTICLEVISIGNAGE-section/docset:ARTICLEVISIGNAGE/docset:_601Signage'}),  Document(page_content='8. SIGNS:\n Tenant shall not install signs upon the Premises without Landlord‚Äôs prior written approval, which approval shall not be unreasonably withheld or delayed, and any such signage shall be subject to any applicable governmental laws, ordinances, regulations, and other requirements. Tenant shall remove all such signs by the terminations of this Lease. Such installations and removals shall be made in such a manner as to avoid injury or defacement of the Building and other improvements, and Tenant shall repair any injury or defacement, including without limitation discoloration caused by such installations and/or removal.', metadata={'id': '6b7d88f0c979c65d5db088fc177fa81f', 'name': 'Lease Agreements/Bioplex, Inc.pdf', 'structure': 'lim h1 div', 'xpath': '/dg:chunk/docset:WITNESSETH-section/docset:WITNESSETH/dg:chunk/docset:TheObligation/dg:chunk[8]/dg:chunk'})]}Using Docugami Knowledge Graph for High Accuracy Document QA‚ÄãOne issue with large documents is that the correct answer to your\nquestion may depend on chunks that are far apart in the document.\nTypical chunking techniques, even with overlap, will struggle with\nproviding the LLM sufficent context to answer such questions. With\nupcoming very large context LLMs, it may be possible to stuff a lot of\ntokens, perhaps even entire documents, inside the context but this will\nstill hit limits at some point with very long documents, or a lot of\ndocuments.For example, if we ask a more complex question that requires the LLM to\ndraw on chunks from different parts of the document, even OpenAI‚Äôs\npowerful LLM is unable to answer correctly.chain_response = qa_chain(""What is rentable area for the property owned by DHA Group?"")chain_response[""result""]  # correct answer should be 13,500 sq ft"" I don't know.""chain_response[""source_documents""][Document(page_content='1.6 Rentable Area of the Premises.', metadata={'id': '5b39a1ae84d51682328dca1467be211f', 'name': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'structure': 'lim h1', 'xpath': '/docset:OFFICELEASE-section/docset:OFFICELEASE-section/docset:OFFICELEASE/docset:WITNESSETH-section/docset:WITNESSETH/dg:chunk/dg:chunk/docset:BasicLeaseInformation/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS-section/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS/docset:CatalystGroup/dg:chunk[6]/dg:chunk'}), Document(page_content='1.6 Rentable Area of the Premises.', metadata={'id': '5b39a1ae84d51682328dca1467be211f', 'name': 'Sample Commercial Leases/Shorebucks LLC_AZ.pdf', 'structure': 'lim h1', 'xpath': '/docset:OFFICELEASE-section/docset:OFFICELEASE-section/docset:WITNESSETH-section/docset:WITNESSETH/dg:chunk/dg:chunk/docset:BasicLeaseInformation/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS-section/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS/docset:MenloGroup/dg:chunk[6]/dg:chunk'}), Document(page_content='1.6 Rentable Area of the Premises.', metadata={'id': '5b39a1ae84d51682328dca1467be211f', 'name': 'Sample Commercial Leases/Shorebucks LLC_FL.pdf', 'structure': 'lim h1', 'xpath': '/docset:OFFICELEASE-section/docset:OFFICELEASE/docset:WITNESSETH-section/docset:WITNESSETH/docset:Florida-section/docset:Florida/docset:Shorebucks/dg:chunk[5]/dg:chunk'}), Document(page_content='1.6 Rentable Area of the Premises.', metadata={'id': '5b39a1ae84d51682328dca1467be211f', 'name': 'Sample Commercial Leases/Shorebucks LLC_TX.pdf', 'structure': 'lim h1', 'xpath': '/docset:OFFICELEASE-section/docset:OFFICELEASE/docset:WITNESSETH-section/docset:WITNESSETH/dg:chunk/dg:chunk/docset:BasicLeaseInformation/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS-section/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS/docset:LandmarkLlc/dg:chunk[6]/dg:chunk'})]At first glance the answer may seem reasonable, but it is incorrect. If\nyou review the source chunks carefully for this answer, you will see\nthat the chunking of the document did not end up putting the Landlord\nname and the rentable area in the same context, and produced irrelevant\nchunks therefore the answer is incorrect (should be 13,500 sq ft)Docugami can help here. Chunks are annotated with additional metadata\ncreated using different techniques if a user has been using\nDocugami. More technical\napproaches will be added later.Specifically, let‚Äôs ask Docugami to return XML tags on its output, as\nwell as additional metadata:loader = DocugamiLoader(docset_id=""zo954yqy53wp"")loader.include_xml_tags = (    True  # for additional semantics from the Docugami knowledge graph)chunks = loader.load()print(chunks[0].metadata){'xpath': '/docset:OFFICELEASE-section/dg:chunk', 'id': '47297e277e556f3ce8b570047304560b', 'name': 'Sample Commercial Leases/Shorebucks LLC_AZ.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_AZ.pdf', 'structure': 'h1 h1 p', 'tag': 'chunk Lease', 'Lease Date': 'March  29th , 2019', 'Landlord': 'Menlo Group', 'Tenant': 'Shorebucks LLC', 'Premises Address': '1564  E Broadway Rd ,  Tempe ,  Arizona  85282', 'Term of Lease': '96  full calendar months', 'Square Feet': '16,159'}We can use a self-querying\nretriever to\nimprove our query accuracy, using this additional metadata:!poetry run pip install --upgrade lark --quietfrom langchain.chains.query_constructor.schema import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_community.vectorstores.chroma import ChromaEXCLUDE_KEYS = [""id"", ""xpath"", ""structure""]metadata_field_info = [    AttributeInfo(        name=key,        description=f""The {key} for this chunk"",        type=""string"",    )    for key in chunks[0].metadata    if key.lower() not in EXCLUDE_KEYS]document_content_description = ""Contents of this chunk""llm = OpenAI(temperature=0)vectordb = Chroma.from_documents(documents=chunks, embedding=embedding)retriever = SelfQueryRetriever.from_llm(    llm, vectordb, document_content_description, metadata_field_info, verbose=True)qa_chain = RetrievalQA.from_chain_type(    llm=OpenAI(),    chain_type=""stuff"",    retriever=retriever,    return_source_documents=True,    verbose=True,)Let‚Äôs run the same question again. It returns the correct result since\nall the chunks have metadata key/value pairs on them carrying key\ninformation about the document even if this information is physically\nvery far away from the source chunk used to generate the answer.qa_chain(    ""What is rentable area for the property owned by DHA Group?"")  # correct answer should be 13,500 sq ft> Entering new RetrievalQA chain...> Finished chain.{'query': 'What is rentable area for the property owned by DHA Group?', 'result': ' The rentable area of the property owned by DHA Group is 13,500 square feet.', 'source_documents': [Document(page_content='1.6 Rentable Area of the Premises.', metadata={'Landlord': 'DHA Group', 'Lease Date': 'March  29th , 2019', 'Premises Address': '111  Bauer Dr ,  Oakland ,  New Jersey ,  07436', 'Square Feet': '13,500', 'Tenant': 'Shorebucks LLC', 'Term of Lease': '84  full calendar  months', 'id': '5b39a1ae84d51682328dca1467be211f', 'name': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'structure': 'lim h1', 'tag': 'chunk', 'xpath': '/docset:OFFICELEASE-section/docset:OFFICELEASE-section/docset:OFFICELEASE/docset:WITNESSETH-section/docset:WITNESSETH/dg:chunk/dg:chunk/docset:BasicLeaseInformation/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS-section/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS/docset:DhaGroup/dg:chunk[6]/dg:chunk'}),  Document(page_content='<RentableAreaofthePremises><SquareFeet>13,500 </SquareFeet>square feet. This square footage figure includes an add-on factor for Common Areas in the Building and has been agreed upon by the parties as final and correct and is not subject to challenge or dispute by either party. </RentableAreaofthePremises>', metadata={'Landlord': 'DHA Group', 'Lease Date': 'March  29th , 2019', 'Premises Address': '111  Bauer Dr ,  Oakland ,  New Jersey ,  07436', 'Square Feet': '13,500', 'Tenant': 'Shorebucks LLC', 'Term of Lease': '84  full calendar  months', 'id': '4c06903d087f5a83e486ee42cd702d31', 'name': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'structure': 'div', 'tag': 'RentableAreaofthePremises', 'xpath': '/docset:OFFICELEASE-section/docset:OFFICELEASE-section/docset:OFFICELEASE/docset:WITNESSETH-section/docset:WITNESSETH/dg:chunk/dg:chunk/docset:BasicLeaseInformation/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS-section/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS/docset:DhaGroup/dg:chunk[6]/docset:RentableAreaofthePremises-section/docset:RentableAreaofthePremises'}),  Document(page_content='<TheTermAnnualMarketRent>shall mean (i) for the initial Lease Year (‚ÄúYear 1‚Äù) <Money>$2,239,748.00 </Money>per year (i.e., the product of the Rentable Area of the Premises multiplied by <Money>$82.00</Money>) (the ‚ÄúYear 1 Market Rent Hurdle‚Äù); (ii) for the Lease Year thereafter, <Percent>one hundred three percent (103%) </Percent>of the Year 1 Market Rent Hurdle, and (iii) for each Lease Year thereafter until the termination or expiration of this Lease, the Annual Market Rent Threshold shall be <AnnualMarketRentThreshold>one hundred three percent (103%) </AnnualMarketRentThreshold>of the Annual Market Rent Threshold for the immediately prior Lease Year. </TheTermAnnualMarketRent>', metadata={'Landlord': 'DHA Group', 'Lease Date': 'March  29th , 2019', 'Premises Address': '111  Bauer Dr ,  Oakland ,  New Jersey ,  07436', 'Square Feet': '13,500', 'Tenant': 'Shorebucks LLC', 'Term of Lease': '84  full calendar  months', 'id': '6b90beeadace5d4d12b25706fb48e631', 'name': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'structure': 'div', 'tag': 'TheTermAnnualMarketRent', 'xpath': '/docset:OFFICELEASE-section/docset:OFFICELEASE-section/docset:OFFICELEASE/docset:WITNESSETH-section/docset:WITNESSETH/docset:GrossRentCredit-section/docset:GrossRentCredit/dg:chunk/dg:chunk/dg:chunk/dg:chunk[2]/docset:PercentageRent/dg:chunk[2]/dg:chunk[2]/docset:TenantSRevenue/dg:chunk[2]/docset:TenantSRevenue/dg:chunk[3]/docset:TheTermAnnualMarketRent-section/docset:TheTermAnnualMarketRent'}),  Document(page_content='1.11 Percentage Rent.\n (a) <GrossRevenue><Percent>55% </Percent>of Gross Revenue to Landlord until Landlord receives Percentage Rent in an amount equal to the Annual Market Rent Hurdle (as escalated); and </GrossRevenue>', metadata={'Landlord': 'DHA Group', 'Lease Date': 'March  29th , 2019', 'Premises Address': '111  Bauer Dr ,  Oakland ,  New Jersey ,  07436', 'Square Feet': '13,500', 'Tenant': 'Shorebucks LLC', 'Term of Lease': '84  full calendar  months', 'id': 'c8bb9cbedf65a578d9db3f25f519dd3d', 'name': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'structure': 'lim h1 lim p', 'tag': 'chunk GrossRevenue', 'xpath': '/docset:OFFICELEASE-section/docset:OFFICELEASE-section/docset:OFFICELEASE/docset:WITNESSETH-section/docset:WITNESSETH/docset:GrossRentCredit-section/docset:GrossRentCredit/dg:chunk/dg:chunk/dg:chunk/docset:PercentageRent/dg:chunk[1]/dg:chunk[1]'})]}This time the answer is correct, since the self-querying retriever\ncreated a filter on the landlord attribute of the metadata, correctly\nfiltering to document that specifically is about the DHA Group landlord.\nThe resulting source chunks are all relevant to this landlord, and this\nimproves answer accuracy even though the landlord is not directly\nmentioned in the specific chunk that contains the correct answer.Advanced Topic: Small-to-Big Retrieval with Document Knowledge Graph HierarchyDocuments are inherently semi-structured and the DocugamiLoader is able\nto navigate the semantic and structural contours of the document to\nprovide parent chunk references on the chunks it returns. This is useful\ne.g.¬†with the MultiVector\nRetriever for\nsmall-to-big retrieval.To get parent chunk references, you can set\nloader.parent_hierarchy_levels to a non-zero value.from typing import Dict, Listfrom docugami_langchain.document_loaders import DocugamiLoaderfrom langchain_core.documents import Documentloader = DocugamiLoader(docset_id=""zo954yqy53wp"")loader.include_xml_tags = (    True  # for additional semantics from the Docugami knowledge graph)loader.parent_hierarchy_levels = 3  # for expanded contextloader.max_text_length = (    1024 * 8)  # 8K chars are roughly 2K tokens (ref: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)loader.include_project_metadata_in_doc_metadata = (    False  # Not filtering on vector metadata, so remove to lighten the vectors)chunks: List[Document] = loader.load()# build separate maps of parent and child chunksparents_by_id: Dict[str, Document] = {}children_by_id: Dict[str, Document] = {}for chunk in chunks:    chunk_id = chunk.metadata.get(""id"")    parent_chunk_id = chunk.metadata.get(loader.parent_id_key)    if not parent_chunk_id:        # parent chunk        parents_by_id[chunk_id] = chunk    else:        # child chunk        children_by_id[chunk_id] = chunk# Explore some of the parent chunk relationshipsfor id, chunk in list(children_by_id.items())[:5]:    parent_chunk_id = chunk.metadata.get(loader.parent_id_key)    if parent_chunk_id:        # child chunks have the parent chunk id set        print(f""PARENT CHUNK {parent_chunk_id}: {parents_by_id[parent_chunk_id]}"")        print(f""CHUNK {id}: {chunk}"")PARENT CHUNK 7df09fbfc65bb8377054808aac2d16fd: page_content='OFFICE LEASE\n THIS OFFICE LEASE\n <Lease>(the ""Lease"") is made and entered into as of <LeaseDate>March 29th, 2019</LeaseDate>, by and between Landlord and Tenant. ""Date of this Lease"" shall mean the date on which the last one of the Landlord and Tenant has signed this Lease. </Lease>\nW I T N E S S E T H\n <TheTerms> Subject to and on the terms and conditions of this Lease, Landlord leases to Tenant and Tenant hires from Landlord the Premises. </TheTerms>\n1. BASIC LEASE INFORMATION AND DEFINED TERMS.\nThe key business terms of this Lease and the defined terms used in this Lease are as follows:' metadata={'xpath': '/docset:OFFICELEASE-section/dg:chunk', 'id': '7df09fbfc65bb8377054808aac2d16fd', 'name': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'structure': 'h1 h1 p h1 p lim h1 p', 'tag': 'chunk Lease chunk TheTerms'}CHUNK 47297e277e556f3ce8b570047304560b: page_content='OFFICE LEASE\n THIS OFFICE LEASE\n <Lease>(the ""Lease"") is made and entered into as of <LeaseDate>March 29th, 2019</LeaseDate>, by and between Landlord and Tenant. ""Date of this Lease"" shall mean the date on which the last one of the Landlord and Tenant has signed this Lease. </Lease>' metadata={'xpath': '/docset:OFFICELEASE-section/dg:chunk', 'id': '47297e277e556f3ce8b570047304560b', 'name': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_NJ.pdf', 'structure': 'h1 h1 p', 'tag': 'chunk Lease', 'doc_id': '7df09fbfc65bb8377054808aac2d16fd'}PARENT CHUNK bb84925da3bed22c30ea1bdc173ff54f: page_content='OFFICE LEASE\n THIS OFFICE LEASE\n <Lease>(the ""Lease"") is made and entered into as of <LeaseDate>January 8th, 2018</LeaseDate>, by and between Landlord and Tenant. ""Date of this Lease"" shall mean the date on which the last one of the Landlord and Tenant has signed this Lease. </Lease>\nW I T N E S S E T H\n <TheTerms> Subject to and on the terms and conditions of this Lease, Landlord leases to Tenant and Tenant hires from Landlord the Premises. </TheTerms>\n1. BASIC LEASE INFORMATION AND DEFINED TERMS.\nThe key business terms of this Lease and the defined terms used in this Lease are as follows:\n1.1 Landlord.\n <Landlord>Catalyst Group LLC </Landlord>' metadata={'xpath': '/docset:OFFICELEASE-section/dg:chunk', 'id': 'bb84925da3bed22c30ea1bdc173ff54f', 'name': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'structure': 'h1 h1 p h1 p lim h1 p lim h1 div', 'tag': 'chunk Lease chunk TheTerms chunk Landlord'}CHUNK 2f1746cbd546d1d61a9250c50de7a7fa: page_content='W I T N E S S E T H\n <TheTerms> Subject to and on the terms and conditions of this Lease, Landlord leases to Tenant and Tenant hires from Landlord the Premises. </TheTerms>' metadata={'xpath': '/docset:OFFICELEASE-section/docset:OFFICELEASE-section/docset:OFFICELEASE/docset:WITNESSETH-section/dg:chunk', 'id': '2f1746cbd546d1d61a9250c50de7a7fa', 'name': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'structure': 'h1 p', 'tag': 'chunk TheTerms', 'doc_id': 'bb84925da3bed22c30ea1bdc173ff54f'}PARENT CHUNK 0b0d765b6e504a6ba54fa76b203e62ec: page_content='OFFICE LEASE\n THIS OFFICE LEASE\n <Lease>(the ""Lease"") is made and entered into as of <LeaseDate>January 8th, 2018</LeaseDate>, by and between Landlord and Tenant. ""Date of this Lease"" shall mean the date on which the last one of the Landlord and Tenant has signed this Lease. </Lease>\nW I T N E S S E T H\n <TheTerms> Subject to and on the terms and conditions of this Lease, Landlord leases to Tenant and Tenant hires from Landlord the Premises. </TheTerms>\n1. BASIC LEASE INFORMATION AND DEFINED TERMS.\nThe key business terms of this Lease and the defined terms used in this Lease are as follows:\n1.1 Landlord.\n <Landlord>Catalyst Group LLC </Landlord>\n1.2 Tenant.\n <Tenant>Shorebucks LLC </Tenant>' metadata={'xpath': '/docset:OFFICELEASE-section/dg:chunk', 'id': '0b0d765b6e504a6ba54fa76b203e62ec', 'name': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'structure': 'h1 h1 p h1 p lim h1 p lim h1 div lim h1 div', 'tag': 'chunk Lease chunk TheTerms chunk Landlord chunk Tenant'}CHUNK b362dfe776ec5a7a66451a8c7c220b59: page_content='1. BASIC LEASE INFORMATION AND DEFINED TERMS.' metadata={'xpath': '/docset:OFFICELEASE-section/docset:OFFICELEASE-section/docset:OFFICELEASE/docset:WITNESSETH-section/docset:WITNESSETH/dg:chunk/dg:chunk/docset:BasicLeaseInformation/dg:chunk', 'id': 'b362dfe776ec5a7a66451a8c7c220b59', 'name': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'structure': 'lim h1', 'tag': 'chunk', 'doc_id': '0b0d765b6e504a6ba54fa76b203e62ec'}PARENT CHUNK c942010baaf76aa4d4657769492f6edb: page_content='OFFICE LEASE\n THIS OFFICE LEASE\n <Lease>(the ""Lease"") is made and entered into as of <LeaseDate>January 8th, 2018</LeaseDate>, by and between Landlord and Tenant. ""Date of this Lease"" shall mean the date on which the last one of the Landlord and Tenant has signed this Lease. </Lease>\nW I T N E S S E T H\n <TheTerms> Subject to and on the terms and conditions of this Lease, Landlord leases to Tenant and Tenant hires from Landlord the Premises. </TheTerms>\n1. BASIC LEASE INFORMATION AND DEFINED TERMS.\nThe key business terms of this Lease and the defined terms used in this Lease are as follows:\n1.1 Landlord.\n <Landlord>Catalyst Group LLC </Landlord>\n1.2 Tenant.\n <Tenant>Shorebucks LLC </Tenant>\n1.3 Building.\n <Building>The building containing the Premises located at <PremisesAddress><PremisesStreetAddress><MainStreet>600 </MainStreet><StreetName>Main Street</StreetName></PremisesStreetAddress>, <City>Bellevue</City>, <State>WA</State>, <Premises>98004</Premises></PremisesAddress>. The Building is located within the Project. </Building>' metadata={'xpath': '/docset:OFFICELEASE-section/dg:chunk', 'id': 'c942010baaf76aa4d4657769492f6edb', 'name': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'structure': 'h1 h1 p h1 p lim h1 p lim h1 div lim h1 div lim h1 div', 'tag': 'chunk Lease chunk TheTerms chunk Landlord chunk Tenant chunk Building'}CHUNK a95971d693b7aa0f6640df1fbd18c2ba: page_content='The key business terms of this Lease and the defined terms used in this Lease are as follows:' metadata={'xpath': '/docset:OFFICELEASE-section/docset:OFFICELEASE-section/docset:OFFICELEASE/docset:WITNESSETH-section/docset:WITNESSETH/dg:chunk/dg:chunk/docset:BasicLeaseInformation/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS-section/docset:BASICLEASEINFORMATIONANDDEFINEDTERMS/dg:chunk', 'id': 'a95971d693b7aa0f6640df1fbd18c2ba', 'name': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'source': 'Sample Commercial Leases/Shorebucks LLC_WA.pdf', 'structure': 'p', 'tag': 'chunk', 'doc_id': 'c942010baaf76aa4d4657769492f6edb'}PARENT CHUNK f34b649cde7fc4ae156849a56d690495: page_content='W I T N E S S E T H\n <TheTerms> Subject to and on the terms and conditions of this Lease, Landlord leases to Tenant and Tenant hires from Landlord the Premises. </TheTerms>\n1. BASIC LEASE INFORMATION AND DEFINED TERMS.\n<BASICLEASEINFORMATIONANDDEFINEDTERMS>The key business terms of this Lease and the defined terms used in this Lease are as follows: </BASICLEASEINFORMATIONANDDEFINEDTERMS>\n1.1 Landlord.\n <Landlord><Landlord>Menlo Group</Landlord>, a <USState>Delaware </USState>limited liability company authorized to transact business in <USState>Arizona</USState>. </Landlord>\n1.2 Tenant.\n <Tenant>Shorebucks LLC </Tenant>\n1.3 Building.\n <Building>The building containing the Premises located at <PremisesAddress><PremisesStreetAddress><Premises>1564 </Premises><Premises>E Broadway Rd</Premises></PremisesStreetAddress>, <City>Tempe</City>, <USState>Arizona </USState><Premises>85282</Premises></PremisesAddress>. The Building is located within the Project. </Building>\n1.4 Project.\n <Project>The parcel of land and the buildings and improvements located on such land known as Shorebucks Office <ShorebucksOfficeAddress><ShorebucksOfficeStreetAddress><ShorebucksOffice>6 </ShorebucksOffice><ShorebucksOffice6>located at <Number>1564 </Number>E Broadway Rd</ShorebucksOffice6></ShorebucksOfficeStreetAddress>, <City>Tempe</City>, <USState>Arizona </USState><Number>85282</Number></ShorebucksOfficeAddress>. The Project is legally described in EXHIBIT ""A"" to this Lease. </Project>' metadata={'xpath': '/dg:chunk/docset:WITNESSETH-section/dg:chunk', 'id': 'f34b649cde7fc4ae156849a56d690495', 'name': 'Sample Commercial Leases/Shorebucks LLC_AZ.docx', 'source': 'Sample Commercial Leases/Shorebucks LLC_AZ.docx', 'structure': 'h1 p lim h1 div lim h1 div lim h1 div lim h1 div lim h1 div', 'tag': 'chunk TheTerms BASICLEASEINFORMATIONANDDEFINEDTERMS chunk Landlord chunk Tenant chunk Building chunk Project'}CHUNK 21b4d9517f7ccdc0e3a028ce5043a2a0: page_content='1.1 Landlord.\n <Landlord><Landlord>Menlo Group</Landlord>, a <USState>Delaware </USState>limited liability company authorized to transact business in <USState>Arizona</USState>. </Landlord>' metadata={'xpath': '/dg:chunk/docset:WITNESSETH-section/docset:WITNESSETH/dg:chunk[1]/dg:chunk[1]/dg:chunk/dg:chunk[2]/dg:chunk', 'id': '21b4d9517f7ccdc0e3a028ce5043a2a0', 'name': 'Sample Commercial Leases/Shorebucks LLC_AZ.docx', 'source': 'Sample Commercial Leases/Shorebucks LLC_AZ.docx', 'structure': 'lim h1 div', 'tag': 'chunk Landlord', 'doc_id': 'f34b649cde7fc4ae156849a56d690495'}from langchain.retrievers.multi_vector import MultiVectorRetriever, SearchTypefrom langchain.storage import InMemoryStorefrom langchain_community.vectorstores.chroma import Chromafrom langchain_openai import OpenAIEmbeddings# The vectorstore to use to index the child chunksvectorstore = Chroma(collection_name=""big2small"", embedding_function=OpenAIEmbeddings())# The storage layer for the parent documentsstore = InMemoryStore()# The retriever (empty to start)retriever = MultiVectorRetriever(    vectorstore=vectorstore,    docstore=store,    search_type=SearchType.mmr,  # use max marginal relevance search    search_kwargs={""k"": 2},)# Add child chunks to vector storeretriever.vectorstore.add_documents(list(children_by_id.values()))# Add parent chunks to docstoreretriever.docstore.mset(parents_by_id.items())# Query vector store directly, should return chunksfound_chunks = vectorstore.similarity_search(    ""what signs does Birch Street allow on their property?"", k=2)for chunk in found_chunks:    print(chunk.page_content)    print(chunk.metadata[loader.parent_id_key])24. SIGNS. <SIGNS>No signage shall be placed by Tenant on any portion of the Project. However, Tenant shall be permitted to place a sign bearing its name in a location approved by Landlord near the entrance to the Premises (at Tenant's cost) and will be furnished a single listing of its name in the Building's directory (at Landlord's cost), all in accordance with the criteria adopted <Frequency>from time to time </Frequency>by Landlord for the Project. Any changes or additional listings in the directory shall be furnished (subject to availability of space) for the then Building Standard charge. </SIGNS>43090337ed2409e0da24ee07e2adbe94<TheExterior> Tenant agrees that all signs, awnings, protective gates, security devices and other installations visible from the exterior of the Premises shall be subject to Landlord's prior written approval, shall be subject to the prior approval of the <Org>Landmarks </Org><Landmarks>Preservation Commission </Landmarks>of the City of <USState>New <Org>York</Org></USState>, if required, and shall not interfere with or block either of the adjacent stores, provided, however, that Landlord shall not unreasonably withhold consent for signs that Tenant desires to install. Tenant agrees that any permitted signs, awnings, protective gates, security devices, and other installations shall be installed at Tenant‚Äôs sole cost and expense professionally prepared and dignified and subject to Landlord's prior written approval, which shall not be unreasonably withheld, delayed or conditioned, and subject to such reasonable rules and restrictions as Landlord <Frequency>from time to time </Frequency>may impose. Tenant shall submit to Landlord drawings of the proposed signs and other installations, showing the size, color, illumination and general appearance thereof, together with a statement of the manner in which the same are to be affixed to the Premises. Tenant shall not commence the installation of the proposed signs and other installations unless and until Landlord shall have approved the same in writing. . Tenant shall not install any neon sign. The aforesaid signs shall be used solely for the purpose of identifying Tenant's business. No changes shall be made in the signs and other installations without first obtaining Landlord's prior written consent thereto, which consent shall not be unreasonably withheld, delayed or conditioned. Tenant shall, at its own cost and expense, obtain and exhibit to Landlord such permits or certificates of approval as Tenant may be required to obtain from any and all City, State and other authorities having jurisdiction covering the erection, installation, maintenance or use of said signs or other installations, and Tenant shall maintain the said signs and other installations together with any appurtenances thereto in good order and condition and to the satisfaction of the Landlord and in accordance with any and all orders, regulations, requirements and rules of any public authorities having jurisdiction thereover. Landlord consents to Tenant‚Äôs Initial Signage described in annexed Exhibit D. </TheExterior>54ddfc3e47f41af7e747b2bc439ea96b# Query retriever, should return parents (using MMR since that was set as search_type above)retrieved_parent_docs = retriever.get_relevant_documents(    ""what signs does Birch Street allow on their property?"")for chunk in retrieved_parent_docs:    print(chunk.page_content)    print(chunk.metadata[""id""])21. SERVICES AND UTILITIES. <SERVICESANDUTILITIES>Landlord shall have no obligation to provide any utilities or services to the Premises other than passenger elevator service to the Premises. Tenant shall be solely responsible for and shall promptly pay all charges for water, electricity, or any other utility used or consumed in the Premises, including all costs associated with separately metering for the Premises. Tenant shall be responsible for repairs and maintenance to exit lighting, emergency lighting, and fire extinguishers for the Premises. Tenant is responsible for interior janitorial, pest control, and waste removal services. Landlord may at any time change the electrical utility provider for the Building. Tenant‚Äôs use of electrical, HVAC, or other services furnished by Landlord shall not exceed, either in voltage, rated capacity, use, or overall load, that which Landlord deems to be standard for the Building. In no event shall Landlord be liable for damages resulting from the failure to furnish any service, and any interruption or failure shall in no manner entitle Tenant to any remedies including abatement of Rent. If at any time during the Lease Term the Project has any type of card access system for the Parking Areas or the Building, Tenant shall purchase access cards for all occupants of the Premises from Landlord at a Building Standard charge and shall comply with Building Standard terms relating to access to the Parking Areas and the Building. </SERVICESANDUTILITIES>22. SECURITY DEPOSIT. <SECURITYDEPOSIT>The Security Deposit shall be held by Landlord as security for Tenant's full and faithful performance of this Lease including the payment of Rent. Tenant grants Landlord a security interest in the Security Deposit. The Security Deposit may be commingled with other funds of Landlord and Landlord shall have no liability for payment of any interest on the Security Deposit. Landlord may apply the Security Deposit to the extent required to cure any default by Tenant. If Landlord so applies the Security Deposit, Tenant shall deliver to Landlord the amount necessary to replenish the Security Deposit to its original sum within <Deliver>five days </Deliver>after notice from Landlord. The Security Deposit shall not be deemed an advance payment of Rent or a measure of damages for any default by Tenant, nor shall it be a defense to any action that Landlord may bring against Tenant. </SECURITYDEPOSIT>23. GOVERNMENTAL REGULATIONS. <GOVERNMENTALREGULATIONS>Tenant, at Tenant's sole cost and expense, shall promptly comply (and shall cause all subtenants and licensees to comply) with all laws, codes, and ordinances of governmental authorities, including the Americans with Disabilities Act of <AmericanswithDisabilitiesActDate>1990 </AmericanswithDisabilitiesActDate>as amended (the ""ADA""), and all recorded covenants and restrictions affecting the Project, pertaining to Tenant, its conduct of business, and its use and occupancy of the Premises, including the performance of any work to the Common Areas required because of Tenant's specific use (as opposed to general office use) of the Premises or Alterations to the Premises made by Tenant. </GOVERNMENTALREGULATIONS>24. SIGNS. <SIGNS>No signage shall be placed by Tenant on any portion of the Project. However, Tenant shall be permitted to place a sign bearing its name in a location approved by Landlord near the entrance to the Premises (at Tenant's cost) and will be furnished a single listing of its name in the Building's directory (at Landlord's cost), all in accordance with the criteria adopted <Frequency>from time to time </Frequency>by Landlord for the Project. Any changes or additional listings in the directory shall be furnished (subject to availability of space) for the then Building Standard charge. </SIGNS>25. BROKER. <BROKER>Landlord and Tenant each represent and warrant that they have neither consulted nor negotiated with any broker or finder regarding the Premises, except the Landlord's Broker and Tenant's Broker. Tenant shall indemnify, defend, and hold Landlord harmless from and against any claims for commissions from any real estate broker other than Landlord's Broker and Tenant's Broker with whom Tenant has dealt in connection with this Lease. Landlord shall indemnify, defend, and hold Tenant harmless from and against payment of any leasing commission due Landlord's Broker and Tenant's Broker in connection with this Lease and any claims for commissions from any real estate broker other than Landlord's Broker and Tenant's Broker with whom Landlord has dealt in connection with this Lease. The terms of this article shall survive the expiration or earlier termination of this Lease. </BROKER>26. END OF TERM. <ENDOFTERM>Tenant shall surrender the Premises to Landlord at the expiration or sooner termination of this Lease or Tenant's right of possession in good order and condition, broom-clean, except for reasonable wear and tear. All Alterations made by Landlord or Tenant to the Premises shall become Landlord's property on the expiration or sooner termination of the Lease Term. On the expiration or sooner termination of the Lease Term, Tenant, at its expense, shall remove from the Premises all of Tenant's personal property, all computer and telecommunications wiring, and all Alterations that Landlord designates by notice to Tenant. Tenant shall also repair any damage to the Premises caused by the removal. Any items of Tenant's property that shall remain in the Premises after the expiration or sooner termination of the Lease Term, may, at the option of Landlord and without notice, be deemed to have been abandoned, and in that case, those items may be retained by Landlord as its property to be disposed of by Landlord, without accountability or notice to Tenant or any other party, in the manner Landlord shall determine, at Tenant's expense. </ENDOFTERM>27. ATTORNEYS' FEES. <ATTORNEYSFEES>Except as otherwise provided in this Lease, the prevailing party in any litigation or other dispute resolution proceeding, including arbitration, arising out of or in any manner based on or relating to this Lease, including tort actions and actions for injunctive, declaratory, and provisional relief, shall be entitled to recover from the losing party actual attorneys' fees and costs, including fees for litigating the entitlement to or amount of fees or costs owed under this provision, and fees in connection with bankruptcy, appellate, or collection proceedings. No person or entity other than Landlord or Tenant has any right to recover fees under this paragraph. In addition, if Landlord becomes a party to any suit or proceeding affecting the Premises or involving this Lease or Tenant's interest under this Lease, other than a suit between Landlord and Tenant, or if Landlord engages counsel to collect any of the amounts owed under this Lease, or to enforce performance of any of the agreements, conditions, covenants, provisions, or stipulations of this Lease, without commencing litigation, then the costs, expenses, and reasonable attorneys' fees and disbursements incurred by Landlord shall be paid to Landlord by Tenant. </ATTORNEYSFEES>43090337ed2409e0da24ee07e2adbe94<TenantsSoleCost> Tenant, at Tenant's sole cost and expense, shall be responsible for the removal and disposal of all of garbage, waste, and refuse from the Premises on a <Frequency>daily </Frequency>basis. Tenant shall cause all garbage, waste and refuse to be stored within the Premises until <Stored>thirty (30) minutes </Stored>before closing, except that Tenant shall be permitted, to the extent permitted by law, to place garbage outside the Premises after the time specified in the immediately preceding sentence for pick up prior to <PickUp>6:00 A.M. </PickUp>next following. Garbage shall be placed at the edge of the sidewalk in front of the Premises at the location furthest from he main entrance to the Building or such other location in front of the Building as may be specified by Landlord. </TenantsSoleCost><ItsSoleCost> Tenant, at its sole cost and expense, agrees to use all reasonable diligence in accordance with the best prevailing methods for the prevention and extermination of vermin, rats, and mice, mold, fungus, allergens, <Bacterium>bacteria </Bacterium>and all other similar conditions in the Premises. Tenant, at Tenant's expense, shall cause the Premises to be exterminated <Exterminated>from time to time </Exterminated>to the reasonable satisfaction of Landlord and shall employ licensed exterminating companies. Landlord shall not be responsible for any cleaning, waste removal, janitorial, or similar services for the Premises, and Tenant sha ll not be entitled to seek any abatement, setoff or credit from the Landlord in the event any conditions described in this Article are found to exist in the Premises. </ItsSoleCost>42B. Sidewalk Use and Maintenance<TheSidewalk> Tenant shall, at its sole cost and expense, keep the sidewalk in front of the Premises 18 inches into the street from the curb clean free of garbage, waste, refuse, excess water, snow, and ice and Tenant shall pay, as additional rent, any fine, cost, or expense caused by Tenant's failure to do so. In the event Tenant operates a sidewalk caf√©, Tenant shall, at its sole cost and expense, maintain, repair, and replace as necessary, the sidewalk in front of the Premises and the metal trapdoor leading to the basement of the Premises, if any. Tenant shall post warning signs and cones on all sides of any side door when in use and attach a safety bar across any such door at all times when open. </TheSidewalk><Display> In no event shall Tenant use, or permit to be used, the space adjacent to or any other space outside of the Premises, for display, sale or any other similar undertaking; except [1] in the event of a legal and licensed ‚Äústreet fair‚Äù type program or [<Number>2</Number>] if the local zoning, Community Board [if applicable] and other municipal laws, rules and regulations, allow for sidewalk caf√© use and, if such I s the case, said operation shall be in strict accordance with all of the aforesaid requirements and conditions. . In no event shall Tenant use, or permit to be used, any advertising medium and/or loud speaker and/or sound amplifier and/or radio or television broadcast which may be heard outside of the Premises or which does not comply with the reasonable rules and regulations of Landlord which then will be in effect. </Display>42C. Store Front Maintenance <TheBulkheadAndSecurityGate> Tenant agrees to wash the storefront, including the bulkhead and security gate, from the top to the ground, monthly or more often as Landlord reasonably requests and make all repairs and replacements as and when deemed necessary by Landlord, to all windows and plate and ot her glass in or about the Premises and the security gate, if any. In case of any default by Tenant in maintaining the storefront as herein provided, Landlord may do so at its own expense and bill the cost thereof to Tenant as additional rent. </TheBulkheadAndSecurityGate>42D. Music, Noise, and Vibration4474c92ae7ccec9184ed2fef9f072734",en,
https://python.langchain.com/docs/integrations/document_loaders/college_confidential,College Confidential | ü¶úÔ∏èüîó Langchain,College Confidential gives,"College ConfidentialCollege Confidential gives\ninformation on 3,800+ colleges and universities.This covers how to load College Confidential webpages into a document\nformat that we can use downstream.from langchain_community.document_loaders import CollegeConfidentialLoaderloader = CollegeConfidentialLoader(    ""https://www.collegeconfidential.com/colleges/brown-university/"")data = loader.load()data[Document(page_content='\n\n\n\n\n\n\n\nA68FEB02-9D19-447C-B8BC-818149FD6EAF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                    Media (2)\n                \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE45B8B13-33D4-450E-B7DB-F66EFE8F2097\n\n\n\n\n\n\n\n\n\nE45B8B13-33D4-450E-B7DB-F66EFE8F2097\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Brown\n\n\n\n\n\n\nBrown University Overview\nBrown University is a private, nonprofit school in the urban setting of Providence, Rhode Island. Brown was founded in 1764 and the school currently enrolls around 10,696 students a year, including 7,349 undergraduates. Brown provides on-campus housing for students. Most students live in off campus housing.\nüìÜ Mark your calendar! January 5, 2023 is the final deadline to submit an application for the Fall 2023 semester. \nThere are many ways for students to get involved at Brown! \nLove music or performing? Join a campus band, sing in a chorus, or perform with one of the school\'s theater groups.\nInterested in journalism or communications? Brown students can write for the campus newspaper, host a radio show or be a producer for the student-run television channel.\nInterested in joining a fraternity or sorority? Brown has fraternities and sororities.\nPlanning to play sports? Brown has many options for athletes. See them all and learn more about life at Brown on the Student Life page.\n\n\n\n2022 Brown Facts At-A-Glance\n\n\n\n\n\nAcademic Calendar\nOther\n\n\nOverall Acceptance Rate\n6%\n\n\nEarly Decision Acceptance Rate\n16%\n\n\nEarly Action Acceptance Rate\nEA not offered\n\n\nApplicants Submitting SAT scores\n51%\n\n\nTuition\n$62,680\n\n\nPercent of Need Met\n100%\n\n\nAverage First-Year Financial Aid Package\n$59,749\n\n\n\n\nIs Brown a Good School?\n\nDifferent people have different ideas about what makes a ""good"" school. Some factors that can help you determine what a good school for you might be include admissions criteria, acceptance rate, tuition costs, and more.\nLet\'s take a look at these factors to get a clearer sense of what Brown offers and if it could be the right college for you.\nBrown Acceptance Rate 2022\nIt is extremely difficult to get into Brown. Around 6% of applicants get into Brown each year. In 2022, just 2,568 out of the 46,568 students who applied were accepted.\nRetention and Graduation Rates at Brown\nRetention refers to the number of students that stay enrolled at a school over time. This is a way to get a sense of how satisfied students are with their school experience, and if they have the support necessary to succeed in college. \nApproximately 98% of first-year, full-time undergrads who start at Browncome back their sophomore year. 95% of Brown undergrads graduate within six years. The average six-year graduation rate for U.S. colleges and universities is 61% for public schools, and 67% for private, non-profit schools.\nJob Outcomes for Brown Grads\nJob placement stats are a good resource for understanding the value of a degree from Brown by providing a look on how job placement has gone for other grads. \nCheck with Brown directly, for information on any information on starting salaries for recent grads.\nBrown\'s Endowment\nAn endowment is the total value of a school\'s investments, donations, and assets. Endowment is not necessarily an indicator of the quality of a school, but it can give you a sense of how much money a college can afford to invest in expanding programs, improving facilities, and support students. \nAs of 2022, the total market value of Brown University\'s endowment was $4.7 billion. The average college endowment was $905 million in 2021. The school spends $34,086 for each full-time student enrolled. \nTuition and Financial Aid at Brown\nTuition is another important factor when choose a college. Some colleges may have high tuition, but do a better job at meeting students\' financial need.\nBrown meets 100% of the demonstrated financial need for undergraduates.  The average financial aid package for a full-time, first-year student is around $59,749 a year. \nThe average student debt for graduates in the class of 2022 was around $24,102 per student, not including those with no debt. For context, compare this number with the average national debt, which is around $36,000 per borrower. \nThe 2023-2024 FAFSA Opened on October 1st, 2022\nSome financial aid is awarded on a first-come, first-served basis, so fill out the FAFSA as soon as you can. Visit the FAFSA website to apply for student aid. Remember, the first F in FAFSA stands for FREE! You should never have to pay to submit the Free Application for Federal Student Aid (FAFSA), so be very wary of anyone asking you for money.\nLearn more about Tuition and Financial Aid at Brown.\nBased on this information, does Brown seem like a good fit? Remember, a school that is perfect for one person may be a terrible fit for someone else! So ask yourself: Is Brown a good school for you?\nIf Brown University seems like a school you want to apply to, click the heart button to save it to your college list.\n\nStill Exploring Schools?\nChoose one of the options below to learn more about Brown:\nAdmissions\nStudent Life\nAcademics\nTuition & Aid\nBrown Community Forums\nThen use the college admissions predictor to take a data science look at your chances  of getting into some of the best colleges and universities in the U.S.\nWhere is Brown?\nBrown is located in the urban setting of Providence, Rhode Island, less than an hour from Boston. \nIf you would like to see Brown for yourself, plan a visit. The best way to reach campus is to take Interstate 95 to Providence, or book a flight to the nearest airport, T.F. Green.\nYou can also take a virtual campus tour to get a sense of what Brown and Providence are like without leaving home.\nConsidering Going to School in Rhode Island?\nSee a full list of colleges in Rhode Island and save your favorites to your college list.\n\n\n\nCollege Info\n\n\n\n\n\n\n\n\n\n                    Providence, RI 02912\n                \n\n\n\n                    Campus Setting: Urban\n                \n\n\n\n\n\n\n\n                        (401) 863-2378\n                    \n\n                            Website\n                        \n\n                        Virtual Tour\n                        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrown Application Deadline\n\n\n\nFirst-Year Applications are Due\n\nJan 5\n\nTransfer Applications are Due\n\nMar 1\n\n\n\n            \n                The deadline for Fall first-year applications to Brown is \n                Jan 5. \n                \n            \n          \n\n            \n                The deadline for Fall transfer applications to Brown is \n                Mar 1. \n                \n            \n          \n\n            \n            Check the school website \n            for more information about deadlines for specific programs or special admissions programs\n            \n          \n\n\n\n\n\n\nBrown ACT Scores\n\n\n\n\nic_reflect\n\n\n\n\n\n\n\n\nACT Range\n\n\n                  \n                    33 - 35\n                  \n                \n\n\n\nEstimated Chance of Acceptance by ACT Score\n\n\nACT Score\nEstimated Chance\n\n\n35 and Above\nGood\n\n\n33 to 35\nAvg\n\n\n33 and Less\nLow\n\n\n\n\n\n\nStand out on your college application\n\n‚Ä¢ Qualify for scholarships\n‚Ä¢ Most students who retest improve their score\n\nSponsored by ACT\n\n\n            Take the Next ACT Test\n        \n\n\n\n\n\nBrown SAT Scores\n\n\n\n\nic_reflect\n\n\n\n\n\n\n\n\nComposite SAT Range\n\n\n                    \n                        720 - 770\n                    \n                \n\n\n\nic_reflect\n\n\n\n\n\n\n\n\nMath SAT Range\n\n\n                    \n                        Not available\n                    \n                \n\n\n\nic_reflect\n\n\n\n\n\n\n\n\nReading SAT Range\n\n\n                    \n                        740 - 800\n                    \n                \n\n\n\n\n\n\n        Brown Tuition & Fees\n    \n\n\n\nTuition & Fees\n\n\n\n                        $82,286\n                    \nIn State\n\n\n\n\n                        $82,286\n                    \nOut-of-State\n\n\n\n\n\n\n\nCost Breakdown\n\n\nIn State\n\n\nOut-of-State\n\n\n\n\nState Tuition\n\n\n\n                            $62,680\n                        \n\n\n\n                            $62,680\n                        \n\n\n\n\nFees\n\n\n\n                            $2,466\n                        \n\n\n\n                            $2,466\n                        \n\n\n\n\nHousing\n\n\n\n                            $15,840\n                        \n\n\n\n                            $15,840\n                        \n\n\n\n\nBooks\n\n\n\n                            $1,300\n                        \n\n\n\n                            $1,300\n                        \n\n\n\n\n\n                            Total (Before Financial Aid):\n                        \n\n\n\n                            $82,286\n                        \n\n\n\n                            $82,286\n                        \n\n\n\n\n\n\n\n\n\n\n\nStudent Life\n\n        Wondering what life at Brown is like? There are approximately \n        10,696 students enrolled at \n        Brown, \n        including 7,349 undergraduate students and \n        3,347  graduate students.\n        96% percent of students attend school \n        full-time, \n        6% percent are from RI and \n            94% percent of students are from other states.\n    \n\n\n\n\n\n                        None\n                    \n\n\n\n\nUndergraduate Enrollment\n\n\n\n                        96%\n                    \nFull Time\n\n\n\n\n                        4%\n                    \nPart Time\n\n\n\n\n\n\n\n                        94%\n                    \n\n\n\n\nResidency\n\n\n\n                        6%\n                    \nIn State\n\n\n\n\n                        94%\n                    \nOut-of-State\n\n\n\n\n\n\n\n                Data Source: IPEDs and Peterson\'s Databases ¬© 2022 Peterson\'s LLC All rights reserved\n            \n', lookup_str='', metadata={'source': 'https://www.collegeconfidential.com/colleges/brown-university/'}, lookup_index=0)]",en,
https://python.langchain.com/docs/integrations/document_loaders/datadog_logs,Datadog Logs | ü¶úÔ∏èüîó Langchain,Datadog is a monitoring and analytics,"Datadog LogsDatadog is a monitoring and analytics\nplatform for cloud-scale applications.This loader fetches the logs from your applications in Datadog using the\ndatadog_api_client Python package. You must initialize the loader with\nyour Datadog API key and APP key, and you need to pass in the query\nto extract the desired logs.from langchain_community.document_loaders import DatadogLogsLoader%pip install --upgrade --quiet  datadog-api-clientDD_API_KEY = ""...""DD_APP_KEY = ""...""query = ""service:agent status:error""loader = DatadogLogsLoader(    query=query,    api_key=DD_API_KEY,    app_key=DD_APP_KEY,    from_time=1688732708951,  # Optional, timestamp in milliseconds    to_time=1688736308951,  # Optional, timestamp in milliseconds    limit=100,  # Optional, default is 100)documents = loader.load()documents[Document(page_content='message: grep: /etc/datadog-agent/system-probe.yaml: No such file or directory', metadata={'id': 'AgAAAYkwpLImvkjRpQAAAAAAAAAYAAAAAEFZa3dwTUFsQUFEWmZfLU5QdElnM3dBWQAAACQAAAAAMDE4OTMwYTQtYzk3OS00MmJjLTlhNDAtOTY4N2EwY2I5ZDdk', 'status': 'error', 'service': 'agent', 'tags': ['accessible-from-goog-gke-node', 'allow-external-ingress-high-ports', 'allow-external-ingress-http', 'allow-external-ingress-https', 'container_id:c7d8ecd27b5b3cfdf3b0df04b8965af6f233f56b7c3c2ffabfab5e3b6ccbd6a5', 'container_name:lab_datadog_1', 'datadog.pipelines:false', 'datadog.submission_auth:private_api_key', 'docker_image:datadog/agent:7.41.1', 'env:dd101-dev', 'hostname:lab-host', 'image_name:datadog/agent', 'image_tag:7.41.1', 'instance-id:7497601202021312403', 'instance-type:custom-1-4096', 'instruqt_aws_accounts:', 'instruqt_azure_subscriptions:', 'instruqt_gcp_projects:', 'internal-hostname:lab-host.d4rjybavkary.svc.cluster.local', 'numeric_project_id:3390740675', 'p-d4rjybavkary', 'project:instruqt-prod', 'service:agent', 'short_image:agent', 'source:agent', 'zone:europe-west1-b'], 'timestamp': datetime.datetime(2023, 7, 7, 13, 57, 27, 206000, tzinfo=tzutc())}), Document(page_content='message: grep: /etc/datadog-agent/system-probe.yaml: No such file or directory', metadata={'id': 'AgAAAYkwpLImvkjRpgAAAAAAAAAYAAAAAEFZa3dwTUFsQUFEWmZfLU5QdElnM3dBWgAAACQAAAAAMDE4OTMwYTQtYzk3OS00MmJjLTlhNDAtOTY4N2EwY2I5ZDdk', 'status': 'error', 'service': 'agent', 'tags': ['accessible-from-goog-gke-node', 'allow-external-ingress-high-ports', 'allow-external-ingress-http', 'allow-external-ingress-https', 'container_id:c7d8ecd27b5b3cfdf3b0df04b8965af6f233f56b7c3c2ffabfab5e3b6ccbd6a5', 'container_name:lab_datadog_1', 'datadog.pipelines:false', 'datadog.submission_auth:private_api_key', 'docker_image:datadog/agent:7.41.1', 'env:dd101-dev', 'hostname:lab-host', 'image_name:datadog/agent', 'image_tag:7.41.1', 'instance-id:7497601202021312403', 'instance-type:custom-1-4096', 'instruqt_aws_accounts:', 'instruqt_azure_subscriptions:', 'instruqt_gcp_projects:', 'internal-hostname:lab-host.d4rjybavkary.svc.cluster.local', 'numeric_project_id:3390740675', 'p-d4rjybavkary', 'project:instruqt-prod', 'service:agent', 'short_image:agent', 'source:agent', 'zone:europe-west1-b'], 'timestamp': datetime.datetime(2023, 7, 7, 13, 57, 27, 206000, tzinfo=tzutc())})]",en,
https://python.langchain.com/docs/integrations/document_loaders/microsoft_excel,Microsoft Excel | ü¶úÔ∏èüîó Langchain,The UnstructuredExcelLoader is used to load Microsoft Excel files.,"Microsoft ExcelThe UnstructuredExcelLoader is used to load Microsoft Excel files.\nThe loader works with both .xlsx and .xls files. The page content\nwill be the raw text of the Excel file. If you use the loader in\n""elements"" mode, an HTML representation of the Excel file will be\navailable in the document metadata under the text_as_html key.from langchain_community.document_loaders import UnstructuredExcelLoaderloader = UnstructuredExcelLoader(""example_data/stanley-cups.xlsx"", mode=""elements"")docs = loader.load()docs[0]Document(page_content='\n  \n    \n      Team\n      Location\n      Stanley Cups\n    \n    \n      Blues\n      STL\n      1\n    \n    \n      Flyers\n      PHI\n      2\n    \n    \n      Maple Leafs\n      TOR\n      13\n    \n  \n', metadata={'source': 'example_data/stanley-cups.xlsx', 'filename': 'stanley-cups.xlsx', 'file_directory': 'example_data', 'filetype': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', 'page_number': 1, 'page_name': 'Stanley Cups', 'text_as_html': '<table border=""1"" class=""dataframe"">\n  <tbody>\n    <tr>\n      <td>Team</td>\n      <td>Location</td>\n      <td>Stanley Cups</td>\n    </tr>\n    <tr>\n      <td>Blues</td>\n      <td>STL</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>Flyers</td>\n      <td>PHI</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>Maple Leafs</td>\n      <td>TOR</td>\n      <td>13</td>\n    </tr>\n  </tbody>\n</table>', 'category': 'Table'})",en,
