{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8350c07b-6f1c-4f8e-950f-e594f345b426",
   "metadata": {},
   "source": [
    "# Recursive URL Loader\n",
    "Web上のドキュメントをクローリングし、ローカルに保存します。  \n",
    "データセットの作成に使用します。\n",
    "\n",
    "### データセット例\n",
    "Langchain の公式ドキュメントからcsv形式のデータセットを出力した例を示します。\n",
    "\n",
    "```csv\n",
    "source,title,description,content,language,docs_updated_at\n",
    "https://python.langchain.com/docs/integrations/document_loaders/microsoft_powerpoint,Microsoft PowerPoint | 🦜️🔗 Langchain,[Microsoft,\"Microsoft PowerPointMicrosoft\\nPowerPoint is a\\npresentation program by Microsoft.This covers how to load Microsoft PowerPoint documents into a document\\nformat that we can use downstream.from langchain_community.document_loaders import UnstructuredPowerPointLoaderloader = UnstructuredPowerPointLoader(\"\"example_data/fake-power-point.pptx\"\")data = loader.load()data[Document(page_content='Adding a Bullet Slide\\n\\nFind the bullet slide layout\\n\\nUse _TextFrame.text for first bullet\\n\\nUse _TextFrame.add_paragraph() for subsequent bullets\\n\\nHere is a lot of text!\\n\\nHere is some text in a text box!', metadata={'source': 'example_data/fake-power-point.pptx'})]Retain Elements​Under the hood, Unstructured creates different “elements” for\\ndifferent chunks of text. By default we combine those together, but you\\ncan easily keep that separation by specifying mode=\"\"elements\"\".loader = UnstructuredPowerPointLoader(    \"\"example_data/fake-power-point.pptx\"\", mode=\"\"elements\"\")data = loader.load()data[0]Document(page_content='Adding a Bullet Slide', lookup_str='', metadata={'source': 'example_data/fake-power-point.pptx'}, lookup_index=0)\",en,\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f59465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# セットアップ\n",
    "%pip install langchain-community\n",
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e826a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kazuhisafukuda/dev/kairu-kun/src/notebook\n",
      "ページ数:158\n",
      "CSVファイルを出力しました\n"
     ]
    }
   ],
   "source": [
    "# 定数の値を変更することで、再帰的に取得するページ数や保存するCSVファイル名を変更できる\n",
    "URL_ROOT = \"https://python.langchain.com/docs/integrations/document_loaders/\"\n",
    "MAX_DEPTH = 2\n",
    "SUB_DIRECTORY = \"recursive_url_loader/\"\n",
    "FILENAME_PREFIX = \"langchain_docs\"\n",
    "ROOT_SELECTOR = \".theme-doc-markdown.markdown\" # ページのコンテンツを取得するためのセレクタ body など\n",
    "\n",
    "# CSVファイルに結果を保存するためのユーティリティ\n",
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "\n",
    "def save_docs_to_csv(\n",
    "    csv_body: list[dict], sub_directory: str = \"\", filename_prefix: str = \"\"\n",
    ") -> None:\n",
    "\n",
    "    # Create directory\n",
    "    directory = os.path.join(os.getcwd(), f\"datasets/{sub_directory}\")\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Create CSV file name by timestamp\n",
    "    csv_path = os.path.join(\n",
    "        directory,\n",
    "        f\"{filename_prefix + '_' if filename_prefix != '' else ''}{datetime.now().strftime('%Y%m%d%H%M%S')}.csv\",\n",
    "    )\n",
    "\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        # write header\n",
    "        fieldname = csv_body[0].keys()\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldname)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # write body\n",
    "        for csv_row in csv_body:\n",
    "            writer.writerow(csv_row)\n",
    "\n",
    "\n",
    "# URL配下のドキュメントを再起的に取得しCSVファイルに保存する\n",
    "from langchain_community.document_loaders.recursive_url_loader import (\n",
    "    RecursiveUrlLoader,\n",
    "    Document,\n",
    ")\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "\n",
    "def load_docs(url: str, max_depth: int, root_selector: str) -> list[Document]:\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url,\n",
    "        max_depth=max_depth,\n",
    "        extractor=lambda x: (lambda y: y.get_text() if y else None)(\n",
    "            Soup(x, \"html.parser\").select_one(root_selector)\n",
    "        ),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "\n",
    "def transform_docs_to_csv_body(docs: list[Document]) -> list[dict]:\n",
    "    csv_body = [\n",
    "        {\n",
    "            \"source\": doc.metadata[\"source\"],\n",
    "            \"title\": doc.metadata[\"title\"],\n",
    "            \"description\": doc.metadata.get(\"description\", \"\"),\n",
    "            \"content\": doc.page_content.replace(\"\\n\", \"\\\\n\"),\n",
    "            \"language\": doc.metadata[\"language\"],\n",
    "            \"docs_updated_at\": doc.metadata.get(\"docs_updated_at\", \"\"),\n",
    "        }\n",
    "        for doc in docs\n",
    "    ]\n",
    "    return csv_body\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    docs = load_docs(url=URL_ROOT, max_depth=MAX_DEPTH, root_selector=ROOT_SELECTOR)\n",
    "    print(f\"ページ数:{len(docs)}\")\n",
    "    csv_body = transform_docs_to_csv_body(docs)\n",
    "    save_docs_to_csv(\n",
    "        csv_body=csv_body, sub_directory=SUB_DIRECTORY, filename_prefix=FILENAME_PREFIX\n",
    "    )\n",
    "    print(\"CSVファイルを出力しました\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d621b",
   "metadata": {},
   "source": [
    "# recursive_url_loaderの動作確認用コード\n",
    "recursive_url_loader によってどんな値が取得できるかチェックするためのコードです。\n",
    "\n",
    "### 参考:メタデータの形式\n",
    "\n",
    "```python\n",
    "{\n",
    "    'source': 'https://python.langchain.com/docs/integrations/document_loaders/dropbox',\n",
    "    'title': 'Dropbox | 🦜️🔗 Langchain',\n",
    "    'description': 'Dropbox is a file hosting',\n",
    "    'language': 'en'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e747bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive_url_loader によって再起的にコンテンツを取得できているかチェックする\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "url = \"https://python.langchain.com/docs/integrations/document_loaders/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=3, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"ページ数:{len(docs)}\")\n",
    "print(f\"テキスト:\" + docs[-1].page_content[:50].replace(\"\\n\", \" \"))\n",
    "print(f\"メタデータ:{docs[-1].metadata}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
