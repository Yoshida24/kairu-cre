{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8350c07b-6f1c-4f8e-950f-e594f345b426",
   "metadata": {},
   "source": [
    "# Recursive URL Loader\n",
    "Webä¸Šã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚  \n",
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¾‹\n",
    "Langchain ã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰csvå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‡ºåŠ›ã—ãŸä¾‹ã‚’ç¤ºã—ã¾ã™ã€‚\n",
    "\n",
    "```csv\n",
    "source,title,description,content,language,docs_updated_at\n",
    "https://python.langchain.com/docs/integrations/document_loaders/microsoft_powerpoint,Microsoft PowerPoint | ğŸ¦œï¸ğŸ”— Langchain,[Microsoft,\"Microsoft PowerPointMicrosoft\\nPowerPoint is a\\npresentation program by Microsoft.This covers how to load Microsoft PowerPoint documents into a document\\nformat that we can use downstream.from langchain_community.document_loaders import UnstructuredPowerPointLoaderloader = UnstructuredPowerPointLoader(\"\"example_data/fake-power-point.pptx\"\")data = loader.load()data[Document(page_content='Adding a Bullet Slide\\n\\nFind the bullet slide layout\\n\\nUse _TextFrame.text for first bullet\\n\\nUse _TextFrame.add_paragraph() for subsequent bullets\\n\\nHere is a lot of text!\\n\\nHere is some text in a text box!', metadata={'source': 'example_data/fake-power-point.pptx'})]Retain Elementsâ€‹Under the hood, Unstructured creates different â€œelementsâ€ for\\ndifferent chunks of text. By default we combine those together, but you\\ncan easily keep that separation by specifying mode=\"\"elements\"\".loader = UnstructuredPowerPointLoader(    \"\"example_data/fake-power-point.pptx\"\", mode=\"\"elements\"\")data = loader.load()data[0]Document(page_content='Adding a Bullet Slide', lookup_str='', metadata={'source': 'example_data/fake-power-point.pptx'}, lookup_index=0)\",en,\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f59465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "%pip install langchain-community\n",
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e826a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kazuhisafukuda/dev/kairu-kun/src/notebook\n",
      "ãƒšãƒ¼ã‚¸æ•°:158\n",
      "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ã—ã¾ã—ãŸ\n"
     ]
    }
   ],
   "source": [
    "# å®šæ•°ã®å€¤ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§ã€å†å¸°çš„ã«å–å¾—ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ã‚„ä¿å­˜ã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰æ›´ã§ãã‚‹\n",
    "URL_ROOT = \"https://python.langchain.com/docs/integrations/document_loaders/\"\n",
    "MAX_DEPTH = 2\n",
    "SUB_DIRECTORY = \"recursive_url_loader/\"\n",
    "FILENAME_PREFIX = \"langchain_docs\"\n",
    "ROOT_SELECTOR = \".theme-doc-markdown.markdown\" # ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚»ãƒ¬ã‚¯ã‚¿ body ãªã©\n",
    "\n",
    "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã«çµæœã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "\n",
    "def save_docs_to_csv(\n",
    "    csv_body: list[dict], sub_directory: str = \"\", filename_prefix: str = \"\"\n",
    ") -> None:\n",
    "\n",
    "    # Create directory\n",
    "    directory = os.path.join(os.getcwd(), f\"datasets/{sub_directory}\")\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Create CSV file name by timestamp\n",
    "    csv_path = os.path.join(\n",
    "        directory,\n",
    "        f\"{filename_prefix + '_' if filename_prefix != '' else ''}{datetime.now().strftime('%Y%m%d%H%M%S')}.csv\",\n",
    "    )\n",
    "\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        # write header\n",
    "        fieldname = csv_body[0].keys()\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldname)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # write body\n",
    "        for csv_row in csv_body:\n",
    "            writer.writerow(csv_row)\n",
    "\n",
    "\n",
    "# URLé…ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å†èµ·çš„ã«å–å¾—ã—CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹\n",
    "from langchain_community.document_loaders.recursive_url_loader import (\n",
    "    RecursiveUrlLoader,\n",
    "    Document,\n",
    ")\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "\n",
    "def load_docs(url: str, max_depth: int, root_selector: str) -> list[Document]:\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url,\n",
    "        max_depth=max_depth,\n",
    "        extractor=lambda x: (lambda y: y.get_text() if y else None)(\n",
    "            Soup(x, \"html.parser\").select_one(root_selector)\n",
    "        ),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "\n",
    "def transform_docs_to_csv_body(docs: list[Document]) -> list[dict]:\n",
    "    csv_body = [\n",
    "        {\n",
    "            \"source\": doc.metadata[\"source\"],\n",
    "            \"title\": doc.metadata[\"title\"],\n",
    "            \"description\": doc.metadata.get(\"description\", \"\"),\n",
    "            \"content\": doc.page_content.replace(\"\\n\", \"\\\\n\"),\n",
    "            \"language\": doc.metadata[\"language\"],\n",
    "            \"docs_updated_at\": doc.metadata.get(\"docs_updated_at\", \"\"),\n",
    "        }\n",
    "        for doc in docs\n",
    "    ]\n",
    "    return csv_body\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    docs = load_docs(url=URL_ROOT, max_depth=MAX_DEPTH, root_selector=ROOT_SELECTOR)\n",
    "    print(f\"ãƒšãƒ¼ã‚¸æ•°:{len(docs)}\")\n",
    "    csv_body = transform_docs_to_csv_body(docs)\n",
    "    save_docs_to_csv(\n",
    "        csv_body=csv_body, sub_directory=SUB_DIRECTORY, filename_prefix=FILENAME_PREFIX\n",
    "    )\n",
    "    print(\"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ã—ã¾ã—ãŸ\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d621b",
   "metadata": {},
   "source": [
    "# recursive_url_loaderã®å‹•ä½œç¢ºèªç”¨ã‚³ãƒ¼ãƒ‰\n",
    "recursive_url_loader ã«ã‚ˆã£ã¦ã©ã‚“ãªå€¤ãŒå–å¾—ã§ãã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚\n",
    "\n",
    "### å‚è€ƒ:ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼\n",
    "\n",
    "```python\n",
    "{\n",
    "    'source': 'https://python.langchain.com/docs/integrations/document_loaders/dropbox',\n",
    "    'title': 'Dropbox | ğŸ¦œï¸ğŸ”— Langchain',\n",
    "    'description': 'Dropbox is a file hosting',\n",
    "    'language': 'en'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e747bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive_url_loader ã«ã‚ˆã£ã¦å†èµ·çš„ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã§ãã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "url = \"https://python.langchain.com/docs/integrations/document_loaders/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=3, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"ãƒšãƒ¼ã‚¸æ•°:{len(docs)}\")\n",
    "print(f\"ãƒ†ã‚­ã‚¹ãƒˆ:\" + docs[-1].page_content[:50].replace(\"\\n\", \" \"))\n",
    "print(f\"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:{docs[-1].metadata}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
